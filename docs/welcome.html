<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Version 3 is out! Notebooks have been updated! &mdash; SuperGradients 3.0.2 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/graphviz.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Common package" href="super_gradients.common.html" />
    <link rel="prev" title="Welcome to SuperGradients‚Äôs documentation!" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> SuperGradients
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Welcome To SuperGradients</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Version 3 is out! Notebooks have been updated!</a></li>
<li class="toctree-l1"><a class="reference internal" href="#build-with-supergradients">Build with SuperGradients</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#support-various-computer-vision-tasks">Support various computer vision tasks</a></li>
<li class="toctree-l2"><a class="reference internal" href="#ready-to-deploy-pre-trained-sota-models">Ready to deploy pre-trained SOTA models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#all-computer-vision-models-pretrained-checkpoints-can-be-found-in-the-model-zoo">All Computer Vision Models - Pretrained Checkpoints can be found in the Model Zoo</a></li>
<li class="toctree-l3"><a class="reference internal" href="#classification">Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="#semantic-segmentation">Semantic Segmentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#object-detection">Object Detection</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#easy-to-train-sota-models">Easy to train SOTA Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#plug-and-play-recipes">Plug and play recipes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#production-readiness">Production readiness</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#quick-installation">Quick Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="#what-s-new">What‚Äôs New</a></li>
<li class="toctree-l1"><a class="reference internal" href="#coming-soon">Coming soon</a></li>
<li class="toctree-l1"><a class="reference internal" href="#table-of-content">Table of Content</a></li>
<li class="toctree-l1"><a class="reference internal" href="#getting-started">Getting Started</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#start-training-with-just-1-command-line">Start Training with Just 1 Command Line</a></li>
<li class="toctree-l2"><a class="reference internal" href="#quickly-load-pre-trained-weights-for-your-desired-model-with-sota-performance">Quickly Load Pre-Trained Weights for Your Desired Model with SOTA Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id1">Classification</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#transfer-learning">Transfer Learning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id2">Semantic Segmentation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#quick-start">Quick Start</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id3">Transfer Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#how-to-connect-custom-dataset">How to Connect Custom Dataset</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id4">Object Detection</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id5">Transfer Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id6">How to Connect Custom Dataset</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#how-to-predict-using-pre-trained-model">How to Predict Using Pre-trained Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#segmentation-detection-and-classification-prediction">Segmentation, Detection and Classification Prediction</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#advanced-features">Advanced Features</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#knowledge-distillation-training">Knowledge Distillation Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="#recipes">Recipes</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#why-use-ddp">Why use DDP ?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#how-does-it-work">How does it work ?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#how-to-use-it">How to use it ?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#calling-functions-on-a-single-node">Calling functions on a single node</a></li>
<li class="toctree-l3"><a class="reference internal" href="#good-to-know">Good to know</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#installation-methods">Installation Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#prerequisites">Prerequisites</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id7">Quick Installation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#implemented-model-architectures">Implemented Model Architectures</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#image-classification">Image Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id8">Semantic Segmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id9">Object Detection</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#documentation">Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="#contributing">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="#citation">Citation</a></li>
<li class="toctree-l1"><a class="reference internal" href="#community">Community</a></li>
<li class="toctree-l1"><a class="reference internal" href="#license">License</a></li>
<li class="toctree-l1"><a class="reference internal" href="#deci-platform">Deci Platform</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Technical Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="super_gradients.common.html">Common package</a></li>
<li class="toctree-l1"><a class="reference internal" href="super_gradients.training.html">Training package</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">SuperGradients</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Version 3 is out! Notebooks have been updated!</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/welcome.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div align="center">
  <img src="assets/SG_img/SG - Horizontal Glow 2.png" width="600"/>
 <br/><br/>
<p><strong>Build, train, and fine-tune production-ready deep learning  SOTA vision models</strong>
<a class="reference external" href="https://twitter.com/intent/tweet?text=Easily%20train%20or%20fine-tune%20SOTA%20computer%20vision%20models%20from%20one%20training%20repository&amp;url=https://github.com/Deci-AI/super-gradients&amp;via=deci_ai&amp;hashtags=AI,deeplearning,computervision,training,opensource"><img alt="Tweet" src="https://img.shields.io/twitter/url/http/shields.io.svg?style=social" /></a></p>
<div class="section" id="version-3-is-out-notebooks-have-been-updated">
<h1>Version 3 is out! Notebooks have been updated!<a class="headerlink" href="#version-3-is-out-notebooks-have-been-updated" title="Permalink to this heading">ÔÉÅ</a></h1>
<hr class="docutils" />
  <p align="center">
  <a href="https://www.supergradients.com/">Website</a> ‚Ä¢
  <a href="#getting-started">Getting Started</a> ‚Ä¢
  <a href="#implemented-model-architectures">Pretrained Models</a> ‚Ä¢
  <a href="#community">Community</a> ‚Ä¢
  <a href="#license">License</a> ‚Ä¢
  <a href="#deci-platform">Deci Platform</a>
</p>
<p align="center">
  <a href="https://github.com/Deci-AI/super-gradients#prerequisites"><img src="https://img.shields.io/badge/python-3.7%20%7C%203.8%20%7C%203.9-blue" />
  <a href="https://github.com/Deci-AI/super-gradients#prerequisites"><img src="https://img.shields.io/badge/pytorch-1.9%20%7C%201.10-blue" />
  <a href="https://pypi.org/project/super-gradients/"><img src="https://img.shields.io/pypi/v/super-gradients" />
  <a href="https://github.com/Deci-AI/super-gradients#computer-vision-models-pretrained-checkpoints" ><img src="https://img.shields.io/badge/pre--trained%20models-34-brightgreen" />
  <a href="https://github.com/Deci-AI/super-gradients/releases"><img src="https://img.shields.io/github/v/release/Deci-AI/super-gradients" />
  <a href="https://join.slack.com/t/supergradients-comm52/shared_invite/zt-10vz6o1ia-b_0W5jEPEnuHXm087K~t8Q"><img src="https://img.shields.io/badge/slack-community-blueviolet" />
  <a href="https://github.com/Deci-AI/super-gradients/blob/master/LICENSE.md"><img src="https://img.shields.io/badge/license-Apache%202.0-blue" />
  <a href="https://deci-ai.github.io/super-gradients/welcome.html"><img src="https://img.shields.io/badge/docs-sphinx-brightgreen" />
</p>    
</div>
</div>
<div class="section" id="build-with-supergradients">
<h1>Build with SuperGradients<a class="headerlink" href="#build-with-supergradients" title="Permalink to this heading">ÔÉÅ</a></h1>
<hr class="docutils" />
<div class="section" id="support-various-computer-vision-tasks">
<h2>Support various computer vision tasks<a class="headerlink" href="#support-various-computer-vision-tasks" title="Permalink to this heading">ÔÉÅ</a></h2>
<div align="center">
<img src="./assets/SG_img/Segmentation 1500x900 .png" width="250px">
<img src="./assets/SG_img/Object detection 1500X900.png" width="250px">
<img src="./assets/SG_img/Classification 1500x900.png" width="250px">
</div>
</div>
<div class="section" id="ready-to-deploy-pre-trained-sota-models">
<h2>Ready to deploy pre-trained SOTA models<a class="headerlink" href="#ready-to-deploy-pre-trained-sota-models" title="Permalink to this heading">ÔÉÅ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load model with pretrained weights</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;yolox_s&quot;</span><span class="p">,</span> <span class="n">pretrained_weights</span><span class="o">=</span><span class="s2">&quot;coco&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="all-computer-vision-models-pretrained-checkpoints-can-be-found-in-the-model-zoo">
<h3>All Computer Vision Models - Pretrained Checkpoints can be found in the <a class="reference external" href="http://bit.ly/3EGfKD4">Model Zoo</a><a class="headerlink" href="#all-computer-vision-models-pretrained-checkpoints-can-be-found-in-the-model-zoo" title="Permalink to this heading">ÔÉÅ</a></h3>
</div>
<div class="section" id="classification">
<h3>Classification<a class="headerlink" href="#classification" title="Permalink to this heading">ÔÉÅ</a></h3>
<div align="center">
<img src="./assets/SG_img/Classification@2xDark.png" width="800px">
</div>
</div>
<div class="section" id="semantic-segmentation">
<h3>Semantic Segmentation<a class="headerlink" href="#semantic-segmentation" title="Permalink to this heading">ÔÉÅ</a></h3>
<div align="center">
<img src="./assets/SG_img/Semantic Segmentation@2xDark.png" width="800px">
</div>
</div>
<div class="section" id="object-detection">
<h3>Object Detection<a class="headerlink" href="#object-detection" title="Permalink to this heading">ÔÉÅ</a></h3>
<div align="center">
<img src="./assets/SG_img/Object Detection@2xDark.png" width="800px">
</div>
</div>
</div>
<div class="section" id="easy-to-train-sota-models">
<h2>Easy to train SOTA Models<a class="headerlink" href="#easy-to-train-sota-models" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>Easily load and fine-tune production-ready, pre-trained SOTA models that incorporate best practices and validated hyper-parameters for achieving best-in-class accuracy.
For more information on how to do it go to <span class="xref myst">Getting Started</span></p>
<div class="section" id="plug-and-play-recipes">
<h3>Plug and play recipes<a class="headerlink" href="#plug-and-play-recipes" title="Permalink to this heading">ÔÉÅ</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">super_gradients</span><span class="o">.</span><span class="n">examples</span><span class="o">.</span><span class="n">train_from_recipe_example</span><span class="o">.</span><span class="n">train_from_recipe</span> <span class="n">architecture</span><span class="o">=</span><span class="n">regnetY800</span> <span class="n">dataset_interface</span><span class="o">.</span><span class="n">data_dir</span><span class="o">=&lt;</span><span class="n">YOUR_Imagenet_LOCAL_PATH</span><span class="o">&gt;</span> <span class="n">ckpt_root_dir</span><span class="o">=&lt;</span><span class="n">CHEKPOINT_DIRECTORY</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>More example on how and why to use recipes can be found in <span class="xref myst">Recipes</span></p>
</div>
</div>
<div class="section" id="production-readiness">
<h2>Production readiness<a class="headerlink" href="#production-readiness" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>All SuperGradients models‚Äô are production ready in the sense that they are compatible with deployment tools such as TensorRT (Nvidia) and OpenVINO (Intel) and can be easily taken into production. With a few lines of code you can easily integrate the models into your codebase.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load model with pretrained weights</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;yolox_s&quot;</span><span class="p">,</span> <span class="n">pretrained_weights</span><span class="o">=</span><span class="s2">&quot;coco&quot;</span><span class="p">)</span>

<span class="c1"># Prepare model for conversion</span>
<span class="c1"># Input size is in format of [Batch x Channels x Width x Height] where 640 is the standart COCO dataset dimensions</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">prep_model_for_conversion</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">640</span><span class="p">,</span> <span class="mi">640</span><span class="p">])</span>
    
<span class="c1"># Create dummy_input</span>

<span class="c1"># Convert model to onnx</span>
<span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dummy_input</span><span class="p">,</span>  <span class="s2">&quot;yolox_s.onnx&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>More information on how to take your model to production can be found in <span class="xref myst">Getting Started</span> notebooks</p>
</div>
</div>
<div class="section" id="quick-installation">
<h1>Quick Installation<a class="headerlink" href="#quick-installation" title="Permalink to this heading">ÔÉÅ</a></h1>
<hr class="docutils" />
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install super-gradients
</pre></div>
</div>
</div>
<div class="section" id="what-s-new">
<h1>What‚Äôs New<a class="headerlink" href="#what-s-new" title="Permalink to this heading">ÔÉÅ</a></h1>
<hr class="docutils" />
<ul class="simple">
<li><p>„Äê17/11/2022„Äë Integration with ClearML</p></li>
<li><p>„Äê06/9/2022„Äë PP-LiteSeg - new pre-trained <a class="reference external" href="http://bit.ly/3EGfKD4">checkpoints</a> and <a class="reference external" href="http://bit.ly/3gfLw07">recipes</a> for Cityscapes with SOTA mIoU scores (~1.5% above paper)üéØ</p></li>
<li><p>„Äê07/08/2022„ÄëDDRNet23 -  new pre-trained <a class="reference external" href="http://bit.ly/3EGfKD4">checkpoints</a> and <a class="reference external" href="http://bit.ly/3gfLw07">recipes</a> for Cityscapes with SOTA mIoU scores (~1% above paper)üéØ</p></li>
<li><p>„Äê27/07/2022„ÄëYOLOX models (object detection) - recipes and pre-trained checkpoints.</p></li>
<li><p>„Äê07/07/2022„ÄëSSD Lite MobileNet V2,V1 - Training <a class="reference external" href="http://bit.ly/3gfLw07">recipes</a> and pre-trained <a class="reference external" href="http://bit.ly/3EGfKD4">checkpoints</a> on COCO - Tailored for edge devices! üì±</p></li>
<li><p>„Äê07/07/2022„Äë STDC  - new pre-trained <a class="reference external" href="http://bit.ly/3EGfKD4">checkpoints</a> and <a class="reference external" href="http://bit.ly/3gfLw07">recipes</a> for Cityscapes with super SOTA mIoU scores (~2.5% above paper)üéØ</p></li>
</ul>
<p>Check out SG full <a class="reference external" href="https://github.com/Deci-AI/super-gradients/releases">release notes</a>.</p>
</div>
<div class="section" id="coming-soon">
<h1>Coming soon<a class="headerlink" href="#coming-soon" title="Permalink to this heading">ÔÉÅ</a></h1>
<hr class="docutils" />
<ul class="simple">
<li><p>[ ] PP-Yolo-E implementation</p></li>
<li><p>[ ] Quantization aware training (QAT)</p></li>
<li><p>[ ] Tools for faster training</p></li>
<li><p>[ ] Integration with more professional tools.</p></li>
</ul>
</div>
<div class="section" id="table-of-content">
<h1>Table of Content<a class="headerlink" href="#table-of-content" title="Permalink to this heading">ÔÉÅ</a></h1>
<hr class="docutils" />
<!-- toc -->
<ul class="simple">
<li><p><span class="xref myst">Getting Started</span></p></li>
<li><p><span class="xref myst">Advanced Features</span></p></li>
<li><p><span class="xref myst">Installation Methods</span></p>
<ul>
<li><p><span class="xref myst">Prerequisites</span></p></li>
<li><p><span class="xref myst">Quick Installation</span></p></li>
</ul>
</li>
<li><p><span class="xref myst">Implemented Model Architectures</span></p></li>
<li><p><span class="xref myst">Contributing</span></p></li>
<li><p><span class="xref myst">Citation</span></p></li>
<li><p><span class="xref myst">Community</span></p></li>
<li><p><span class="xref myst">License</span></p></li>
<li><p><span class="xref myst">Deci Platform</span></p></li>
</ul>
<!-- tocstop -->
</div>
<div class="section" id="getting-started">
<h1>Getting Started<a class="headerlink" href="#getting-started" title="Permalink to this heading">ÔÉÅ</a></h1>
<hr class="docutils" />
<div class="section" id="start-training-with-just-1-command-line">
<h2>Start Training with Just 1 Command Line<a class="headerlink" href="#start-training-with-just-1-command-line" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>The most simple and straightforward way to start training SOTA performance models with SuperGradients reproducible recipes. Just define your dataset path and where you want your checkpoints to be saved and you are good to go from your terminal!</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python -m super_gradients.train_from_recipe --config-name<span class="o">=</span>imagenet_regnetY <span class="nv">architecture</span><span class="o">=</span>regnetY800 dataset_interface.data_dir<span class="o">=</span>&lt;YOUR_Imagenet_LOCAL_PATH&gt; <span class="nv">ckpt_root_dir</span><span class="o">=</span>&lt;CHEKPOINT_DIRECTORY&gt;
</pre></div>
</div>
</div>
<div class="section" id="quickly-load-pre-trained-weights-for-your-desired-model-with-sota-performance">
<h2>Quickly Load Pre-Trained Weights for Your Desired Model with SOTA Performance<a class="headerlink" href="#quickly-load-pre-trained-weights-for-your-desired-model-with-sota-performance" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>Want to try our pre-trained models on your machine? Import SuperGradients, initialize your Trainer, and load your desired architecture and pre-trained weights from our <a class="reference external" href="http://bit.ly/3EGfKD4">SOTA model zoo</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># The pretrained_weights argument will load a pre-trained architecture on the provided dataset</span>
    
<span class="kn">import</span> <span class="nn">super_gradients</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;model-name&quot;</span><span class="p">,</span> <span class="n">pretrained_weights</span><span class="o">=</span><span class="s2">&quot;pretrained-model-name&quot;</span><span class="p">)</span>

</pre></div>
</div>
</div>
<div class="section" id="id1">
<h2>Classification<a class="headerlink" href="#id1" title="Permalink to this heading">ÔÉÅ</a></h2>
<div class="section" id="transfer-learning">
<h3>Transfer Learning<a class="headerlink" href="#transfer-learning" title="Permalink to this heading">ÔÉÅ</a></h3>
  <table class="tfo-notebook-buttons" align="left">
 <td width="500">  
  <a target="_blank" href="https://bit.ly/3xzIutb"><img src="./assets/SG_img/colab_logo.png" /> Classification Transfer Learning</a>
  </td>
 <td width="200">    
 <a target="_blank" href="https://bit.ly/3xwYEn1"><img src="./assets/SG_img/GitHub_logo.png" /> GitHub source</a>
 </td>
</table>
 </br></br>
</div>
</div>
<div class="section" id="id2">
<h2>Semantic Segmentation<a class="headerlink" href="#id2" title="Permalink to this heading">ÔÉÅ</a></h2>
<div class="section" id="quick-start">
<h3>Quick Start<a class="headerlink" href="#quick-start" title="Permalink to this heading">ÔÉÅ</a></h3>
<table class="tfo-notebook-buttons" align="left">
 <td width="500">
<a target="_blank" href="https://bit.ly/3qKx9m8"><img src="./assets/SG_img/colab_logo.png" /> Segmentation Quick Start</a>
 </td>
 <td width="200">
<a target="_blank" href="https://bit.ly/3qJjxYq"><img src="./assets/SG_img/GitHub_logo.png" /> GitHub source </a>
 </td>
</table>
 </br></br>
</div>
<div class="section" id="id3">
<h3>Transfer Learning<a class="headerlink" href="#id3" title="Permalink to this heading">ÔÉÅ</a></h3>
<table class="tfo-notebook-buttons" align="left">
 <td width="500">
<a target="_blank" href="https://bit.ly/3qKwMbe"><img src="./assets/SG_img/colab_logo.png" /> Segmentation Transfer Learning</a>
 </td>
 <td width="200">
<a target="_blank" href="https://bit.ly/3ShJlXn"><img src="./assets/SG_img/GitHub_logo.png" /> GitHub source</a>
 </td>
</table>
 </br></br>
</div>
<div class="section" id="how-to-connect-custom-dataset">
<h3>How to Connect Custom Dataset<a class="headerlink" href="#how-to-connect-custom-dataset" title="Permalink to this heading">ÔÉÅ</a></h3>
  <table class="tfo-notebook-buttons" align="left">
 <td width="500"> 
<a target="_blank" href="https://bit.ly/3QQBVJp"><img src="./assets/SG_img/colab_logo.png" /> Segmentation How to Connect Custom Dataset</a>
   </td>
 <td width="200">
 <a target="_blank" href="https://bit.ly/3Us2WGi"><img src="./assets/SG_img/GitHub_logo.png" /> GitHub source</a>
 </td>
</table>
 </br></br>
</div>
</div>
<div class="section" id="id4">
<h2>Object Detection<a class="headerlink" href="#id4" title="Permalink to this heading">ÔÉÅ</a></h2>
<div class="section" id="id5">
<h3>Transfer Learning<a class="headerlink" href="#id5" title="Permalink to this heading">ÔÉÅ</a></h3>
  <table class="tfo-notebook-buttons" align="left">
 <td width="500">   
<a target="_blank" href="https://bit.ly/3SkMohx"><img src="./assets/SG_img/colab_logo.png" /> Detection Transfer Learning</a>
   </td>
 <td width="200">   
<a target="_blank" href="https://bit.ly/3DF8siG"><img src="./assets/SG_img/GitHub_logo.png" /> GitHub source</a>
 </td>
</table>
 </br></br>
</div>
<div class="section" id="id6">
<h3>How to Connect Custom Dataset<a class="headerlink" href="#id6" title="Permalink to this heading">ÔÉÅ</a></h3>
  <table class="tfo-notebook-buttons" align="left">
 <td width="500">  
  <a target="_blank" href="https://bit.ly/3dqDlg3"><img src="./assets/SG_img/colab_logo.png" /> Detection How to Connect Custom Dataset</a>
  </td>
 <td width="200">      
<a target="_blank" href="https://bit.ly/3xBlcmq"><img src="./assets/SG_img/GitHub_logo.png" /> GitHub source</a>
 </td>
</table>
 </br></br>
</div>
</div>
<div class="section" id="how-to-predict-using-pre-trained-model">
<h2>How to Predict Using Pre-trained Model<a class="headerlink" href="#how-to-predict-using-pre-trained-model" title="Permalink to this heading">ÔÉÅ</a></h2>
<div class="section" id="segmentation-detection-and-classification-prediction">
<h3>Segmentation, Detection and Classification Prediction<a class="headerlink" href="#segmentation-detection-and-classification-prediction" title="Permalink to this heading">ÔÉÅ</a></h3>
  <table class="tfo-notebook-buttons" align="left">
 <td width="500">    
<a target="_blank" href="https://bit.ly/3f4mssd"><img src="./assets/SG_img/colab_logo.png" /> How to Predict Using Pre-trained Model</a>
  </td>
 <td width="200">   
<a target="_blank" href="https://bit.ly/3Sf59Tr"><img src="./assets/SG_img/GitHub_logo.png" /> GitHub source</a>
 </td>
</table>
 </br></br>
</div>
</div>
</div>
<div class="section" id="advanced-features">
<h1>Advanced Features<a class="headerlink" href="#advanced-features" title="Permalink to this heading">ÔÉÅ</a></h1>
<hr class="docutils" />
<div class="section" id="knowledge-distillation-training">
<h2>Knowledge Distillation Training<a class="headerlink" href="#knowledge-distillation-training" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>Knowledge Distillation is a training technique that uses a large model, teacher model, to improve the performance of a smaller model, the student model.
Learn more about SuperGradients knowledge distillation training with our pre-trained BEiT base teacher model and Resnet18 student model on CIFAR10 example notebook on Google Colab for an easy to use tutorial using free GPU hardware</p>
  <table class="tfo-notebook-buttons" align="left">
 <td width="500">   
   <a target="_blank" href="https://bit.ly/3BLA5oR"><img src="./assets/SG_img/colab_logo.png" /> Knowledge Distillation Training</a>
  </td>
 <td width="200">   
<a target="_blank" href="https://bit.ly/3S9UlG4"><img src="./assets/SG_img/GitHub_logo.png" /> GitHub source</a>
 </td>
</table>
 </br></br>
</div>
<div class="section" id="recipes">
<h2>Recipes<a class="headerlink" href="#recipes" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>To train a model, it is necessary to configure 4 main components. These components are aggregated into a single ‚Äúmain‚Äù recipe <code class="docutils literal notranslate"><span class="pre">.yaml</span></code> file that inherits the aforementioned dataset, architecture, raining and checkpoint params. It is also possible (and recomended for flexibility) to override default settings with custom ones.
All recipes can be found <a class="reference external" href="http://bit.ly/3gfLw07">here</a>
</br>
Recipes support out of the box every model, metric or loss that is implemented in SuperGradients, but you can easily extend this to any custom object that you need by ‚Äúregistering it‚Äù. Check out <a class="reference external" href="http://bit.ly/3TQ4iZB">this</a> tutorial for more information.</p>
  <table class="tfo-notebook-buttons" align="left">
 <td width="500">   
   <a target="_blank" href="https://bit.ly/3UiY5ab"><img src="./assets/SG_img/colab_logo.png" /> How to Use Recipes</a>
  </td>
 <td width="200">  
<a target="_blank" href="https://bit.ly/3QSrHbm"><img src="./assets/SG_img/GitHub_logo.png" /> GitHub source</a>
 </td>
</table>
 </br></br>
 </br>
<details>
  <summary><h3>Using Distributed Data Parallel (DDP) </h3></summary>
<div class="section" id="why-use-ddp">
<h3>Why use DDP ?<a class="headerlink" href="#why-use-ddp" title="Permalink to this heading">ÔÉÅ</a></h3>
<p>Recent Deep Learning models are growing larger and larger to an extent that training on a single GPU can take weeks.
In order to train models in a timely fashion, it is necessary to train them with multiple GPUs.
Using 100s GPUs can reduce training time of a model from a week to less than an hour.</p>
</div>
<div class="section" id="how-does-it-work">
<h3>How does it work ?<a class="headerlink" href="#how-does-it-work" title="Permalink to this heading">ÔÉÅ</a></h3>
<p>Each GPU has its own process, which controls a copy of the model and which loads its own mini-batch from disk and sends
it to its GPU during training. After the forward pass is completed on every GPU, the gradient is reduced across all
GPUs, yielding to all the GPUs having the same gradient locally. This leads to the model weights to stay synchronized
across all GPUs after the backward pass.</p>
</div>
<div class="section" id="how-to-use-it">
<h3>How to use it ?<a class="headerlink" href="#how-to-use-it" title="Permalink to this heading">ÔÉÅ</a></h3>
<p>You can use SuperGradients to train your model with DDP in just a few lines.</p>
<p><em>main.py</em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">super_gradients</span> <span class="kn">import</span> <span class="n">init_trainer</span><span class="p">,</span> <span class="n">Trainer</span>
<span class="kn">from</span> <span class="nn">super_gradients.common</span> <span class="kn">import</span> <span class="n">MultiGPUMode</span>
<span class="kn">from</span> <span class="nn">super_gradients.training.utils.distributed_training_utils</span> <span class="kn">import</span> <span class="n">setup_device</span>

<span class="c1"># Initialize the environment</span>
<span class="n">init_trainer</span><span class="p">()</span>

<span class="c1"># Launch DDP on 4 GPUs&#39;</span>
<span class="n">setup_device</span><span class="p">(</span><span class="n">multi_gpu</span><span class="o">=</span><span class="n">MultiGPUMode</span><span class="o">.</span><span class="n">DISTRIBUTED_DATA_PARALLEL</span><span class="p">,</span> <span class="n">num_gpus</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># Call the trainer</span>
<span class="n">Trainer</span><span class="p">(</span><span class="n">multi_gpu</span><span class="o">=</span><span class="n">MultiGPUMode</span><span class="o">.</span><span class="n">DISTRIBUTED_DATA_PARALLEL</span><span class="p">,</span> <span class="n">expriment_name</span><span class="o">=...</span><span class="p">)</span>

<span class="c1"># Everything you do below will run on 4 gpus</span>

<span class="o">...</span>

<span class="n">Trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

</pre></div>
</div>
<p>Finally, you can launch your distributed training with a simple python call.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python main.py
</pre></div>
</div>
<p>Please note that if you work with <code class="docutils literal notranslate"><span class="pre">torch&lt;1.9.0</span></code> (deprecated), you will have to launch your training with either
<code class="docutils literal notranslate"><span class="pre">torch.distributed.launch</span></code> or <code class="docutils literal notranslate"><span class="pre">torchrun</span></code>, in which case <code class="docutils literal notranslate"><span class="pre">nproc_per_node</span></code> will overwrite the value  set with <code class="docutils literal notranslate"><span class="pre">gpu_mode</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python -m torch.distributed.launch --nproc_per_node<span class="o">=</span><span class="m">4</span> main.py
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>torchrun --nproc_per_node<span class="o">=</span><span class="m">4</span> main.py
</pre></div>
</div>
</div>
<div class="section" id="calling-functions-on-a-single-node">
<h3>Calling functions on a single node<a class="headerlink" href="#calling-functions-on-a-single-node" title="Permalink to this heading">ÔÉÅ</a></h3>
<p>It is often in DDP training that we want to execute code on the master rank (i.e rank 0).
In SG, users usually execute their own code by triggering ‚ÄúPhase Callbacks‚Äù (see ‚ÄúUsing phase callbacks‚Äù section below).
One can make sure the desired code will only be ran on rank 0, using ddp_silent_mode or the multi_process_safe decorator.
For example, consider the simple phase callback below, that uploads the first 3 images of every batch during training to
the Tensorboard:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">super_gradients.training.utils.callbacks</span> <span class="kn">import</span> <span class="n">PhaseCallback</span><span class="p">,</span> <span class="n">PhaseContext</span><span class="p">,</span> <span class="n">Phase</span>
<span class="kn">from</span> <span class="nn">super_gradients.common.environment.env_helpers</span> <span class="kn">import</span> <span class="n">multi_process_safe</span>

<span class="k">class</span> <span class="nc">Upload3TrainImagesCalbback</span><span class="p">(</span><span class="n">PhaseCallback</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">phase</span><span class="o">=</span><span class="n">Phase</span><span class="o">.</span><span class="n">TRAIN_BATCH_END</span><span class="p">)</span>
    
    <span class="nd">@multi_process_safe</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">:</span> <span class="n">PhaseContext</span><span class="p">):</span>
        <span class="n">batch_imgs</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">inputs</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">tag</span> <span class="o">=</span> <span class="s2">&quot;batch_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">batch_idx</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;_images&quot;</span>
        <span class="n">context</span><span class="o">.</span><span class="n">sg_logger</span><span class="o">.</span><span class="n">add_images</span><span class="p">(</span><span class="n">tag</span><span class="o">=</span><span class="n">tag</span><span class="p">,</span> <span class="n">images</span><span class="o">=</span><span class="n">batch_imgs</span><span class="p">[:</span> <span class="mi">3</span><span class="p">],</span> <span class="n">global_step</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">epoch</span><span class="p">)</span>

</pre></div>
</div>
<p>The &#64;multi_process_safe decorator ensures that the callback will only be triggered by rank 0. Alternatively, this can also
be done by the SG trainer boolean attribute (which the phase context has access to), ddp_silent_mode, which is set to False
iff the current process rank is zero (even after the process group has been killed):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">super_gradients.training.utils.callbacks</span> <span class="kn">import</span> <span class="n">PhaseCallback</span><span class="p">,</span> <span class="n">PhaseContext</span><span class="p">,</span> <span class="n">Phase</span>

<span class="k">class</span> <span class="nc">Upload3TrainImagesCalbback</span><span class="p">(</span><span class="n">PhaseCallback</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">phase</span><span class="o">=</span><span class="n">Phase</span><span class="o">.</span><span class="n">TRAIN_BATCH_END</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">:</span> <span class="n">PhaseContext</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">ddp_silent_mode</span><span class="p">:</span>
            <span class="n">batch_imgs</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">inputs</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="n">tag</span> <span class="o">=</span> <span class="s2">&quot;batch_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">batch_idx</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;_images&quot;</span>
            <span class="n">context</span><span class="o">.</span><span class="n">sg_logger</span><span class="o">.</span><span class="n">add_images</span><span class="p">(</span><span class="n">tag</span><span class="o">=</span><span class="n">tag</span><span class="p">,</span> <span class="n">images</span><span class="o">=</span><span class="n">batch_imgs</span><span class="p">[:</span> <span class="mi">3</span><span class="p">],</span> <span class="n">global_step</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">epoch</span><span class="p">)</span>

</pre></div>
</div>
<p>Note that ddp_silent_mode can be accessed through SgTrainer.ddp_silent_mode. Hence, it can be used in scripts after calling
SgTrainer.train() when some part of it should be ran on rank 0 only.</p>
</div>
<div class="section" id="good-to-know">
<h3>Good to know<a class="headerlink" href="#good-to-know" title="Permalink to this heading">ÔÉÅ</a></h3>
<p>Your total batch size will be (number of gpus x batch size), so you might want to increase your learning rate.
There is no clear rule, but a rule of thumb seems to be to <a class="reference external" href="https://arxiv.org/pdf/1706.02677.pdf">linearly increase the learning rate with the number of gpus</a></p>
</details>
<details>
<summary><h3> Easily change architectures parameters </h3></summary>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">super_gradients.training</span> <span class="kn">import</span> <span class="n">models</span>

<span class="c1"># instantiate default pretrained resnet18</span>
<span class="n">default_resnet18</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;resnet18&quot;</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">pretrained_weights</span><span class="o">=</span><span class="s2">&quot;imagenet&quot;</span><span class="p">)</span>

<span class="c1"># instantiate pretrained resnet18, turning DropPath on with probability 0.5</span>
<span class="n">droppath_resnet18</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;resnet18&quot;</span><span class="p">,</span> <span class="n">arch_params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;droppath_prob&quot;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">},</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">pretrained_weights</span><span class="o">=</span><span class="s2">&quot;imagenet&quot;</span><span class="p">)</span>

<span class="c1"># instantiate pretrained resnet18, without classifier head. Output will be from the last stage before global pooling</span>
<span class="n">backbone_resnet18</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;resnet18&quot;</span><span class="p">,</span> <span class="n">arch_params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;backbone_mode&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">},</span> <span class="n">pretrained_weights</span><span class="o">=</span><span class="s2">&quot;imagenet&quot;</span><span class="p">)</span>
</pre></div>
</div>
</details>
<details>
<summary><h3> Using phase callbacks </h3></summary>  
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">super_gradients</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="kn">from</span> <span class="nn">torch.optim.lr_scheduler</span> <span class="kn">import</span> <span class="n">ReduceLROnPlateau</span>
<span class="kn">from</span> <span class="nn">super_gradients.training.utils.callbacks</span> <span class="kn">import</span> <span class="n">Phase</span><span class="p">,</span> <span class="n">LRSchedulerCallback</span>
<span class="kn">from</span> <span class="nn">super_gradients.training.metrics.classification_metrics</span> <span class="kn">import</span> <span class="n">Accuracy</span>

<span class="c1"># define PyTorch train and validation loaders and optimizer</span>

<span class="c1"># define what to be called in the callback</span>
<span class="n">rop_lr_scheduler</span> <span class="o">=</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># define phase callbacks, they will fire as defined in Phase</span>
<span class="n">phase_callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">LRSchedulerCallback</span><span class="p">(</span><span class="n">scheduler</span><span class="o">=</span><span class="n">rop_lr_scheduler</span><span class="p">,</span>
                                       <span class="n">phase</span><span class="o">=</span><span class="n">Phase</span><span class="o">.</span><span class="n">VALIDATION_EPOCH_END</span><span class="p">,</span>
                                       <span class="n">metric_name</span><span class="o">=</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">)]</span>

<span class="c1"># create a trainer object, look the declaration for more parameters</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="s2">&quot;experiment_name&quot;</span><span class="p">)</span>

<span class="c1"># define phase_callbacks as part of the training parameters</span>
<span class="n">train_params</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;phase_callbacks&quot;</span><span class="p">:</span> <span class="n">phase_callbacks</span><span class="p">}</span>
</pre></div>
</div>
</details>
<details>
<summary><h3> Integration to Weights and Biases </h3></summary>    
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">super_gradients</span> <span class="kn">import</span> <span class="n">Trainer</span>

<span class="c1"># create a trainer object, look the declaration for more parameters</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="s2">&quot;experiment_name&quot;</span><span class="p">)</span>

<span class="n">train_params</span> <span class="o">=</span> <span class="p">{</span> <span class="o">...</span> <span class="c1"># training parameters</span>
                <span class="s2">&quot;sg_logger&quot;</span><span class="p">:</span> <span class="s2">&quot;wandb_sg_logger&quot;</span><span class="p">,</span> <span class="c1"># Weights&amp;Biases Logger, see class WandBSGLogger for details</span>
                <span class="s2">&quot;sg_logger_params&quot;</span><span class="p">:</span> <span class="c1"># parameters that will be passes to __init__ of the logger </span>
                  <span class="p">{</span>
                    <span class="s2">&quot;project_name&quot;</span><span class="p">:</span> <span class="s2">&quot;project_name&quot;</span><span class="p">,</span> <span class="c1"># W&amp;B project name</span>
                    <span class="s2">&quot;save_checkpoints_remote&quot;</span><span class="p">:</span> <span class="kc">True</span>
                    <span class="s2">&quot;save_tensorboard_remote&quot;</span><span class="p">:</span> <span class="kc">True</span>
                    <span class="s2">&quot;save_logs_remote&quot;</span><span class="p">:</span> <span class="kc">True</span>
                  <span class="p">}</span> 
               <span class="p">}</span>
</pre></div>
</div>
</details>
<details>
<summary><h3> Integration to ClearML </h3></summary>    
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">super_gradients</span> <span class="kn">import</span> <span class="n">Trainer</span>

<span class="c1"># create a trainer object, look the declaration for more parameters</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="s2">&quot;experiment_name&quot;</span><span class="p">)</span>

<span class="n">train_params</span> <span class="o">=</span> <span class="p">{</span> <span class="o">...</span> <span class="c1"># training parameters</span>
                <span class="s2">&quot;sg_logger&quot;</span><span class="p">:</span> <span class="s2">&quot;clearml_sg_logger&quot;</span><span class="p">,</span> <span class="c1"># ClearML Logger, see class ClearMLSGLogger for details</span>
                <span class="s2">&quot;sg_logger_params&quot;</span><span class="p">:</span> <span class="c1"># parameters that will be passes to __init__ of the logger </span>
                  <span class="p">{</span>
                    <span class="s2">&quot;project_name&quot;</span><span class="p">:</span> <span class="s2">&quot;project_name&quot;</span><span class="p">,</span> <span class="c1"># ClearML project name</span>
                    <span class="s2">&quot;save_checkpoints_remote&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
                    <span class="s2">&quot;save_tensorboard_remote&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
                    <span class="s2">&quot;save_logs_remote&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
                  <span class="p">}</span> 
               <span class="p">}</span>
</pre></div>
</div>
  </details>
</div>
</div>
</div>
<div class="section" id="installation-methods">
<h1>Installation Methods<a class="headerlink" href="#installation-methods" title="Permalink to this heading">ÔÉÅ</a></h1>
<hr class="docutils" />
<div class="section" id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this heading">ÔÉÅ</a></h2>
<details>
<summary>General requirements</summary>
<ul class="simple">
<li><p>Python 3.7, 3.8 or 3.9 installed.</p></li>
<li><p>torch&gt;=1.9.0</p>
<ul>
<li><p>https://pytorch.org/get-started/locally/</p></li>
</ul>
</li>
<li><p>The python packages that are specified in requirements.txt;</p></li>
</ul>
</details>
<details>
<summary>To train on nvidia GPUs</summary>
<ul class="simple">
<li><p><a class="reference external" href="https://developer.nvidia.com/cuda-11.2.0-download-archive?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=Ubuntu">Nvidia CUDA Toolkit &gt;= 11.2</a></p></li>
<li><p>CuDNN &gt;= 8.1.x</p></li>
<li><p>Nvidia Driver with CUDA &gt;= 11.2 support (‚â•460.x)</p></li>
</ul>
</details>
</div>
<div class="section" id="id7">
<h2>Quick Installation<a class="headerlink" href="#id7" title="Permalink to this heading">ÔÉÅ</a></h2>
<details>
<summary>Install stable version using PyPi</summary>
<p>See in <a class="reference external" href="https://pypi.org/project/super-gradients/">PyPi</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install super-gradients
</pre></div>
</div>
<p>That‚Äôs it !</p>
</details>
<details>
<summary>Install using GitHub</summary>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install git+https://github.com/Deci-AI/super-gradients.git@stable
</pre></div>
</div>
</details> 
</div>
</div>
<div class="section" id="implemented-model-architectures">
<h1>Implemented Model Architectures<a class="headerlink" href="#implemented-model-architectures" title="Permalink to this heading">ÔÉÅ</a></h1>
<hr class="docutils" />
<p>All Computer Vision Models - Pretrained Checkpoints can be found in the <a class="reference external" href="http://bit.ly/3EGfKD4">Model Zoo</a></p>
<p>Detailed list can be found <a class="reference external" href="http://bit.ly/3GnJwgZ">here</a></p>
<div class="section" id="image-classification">
<h2>Image Classification<a class="headerlink" href="#image-classification" title="Permalink to this heading">ÔÉÅ</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/models/classification_models/densenet.py">DensNet (Densely Connected Convolutional Networks)</a></p></li>
<li><p><a class="reference external" href="https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/models/classification_models/dpn.py">DPN</a></p></li>
<li><p><a class="reference external" href="https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/models/classification_models/efficientnet.py">EfficientNet</a></p></li>
<li><p><a class="reference external" href="https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/models/classification_models/lenet.py">LeNet</a></p></li>
<li><p><a class="reference external" href="https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/models/classification_models/mobilenet.py">MobileNet</a></p></li>
<li><p><a class="reference external" href="https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/models/classification_models/mobilenetv2.py">MobileNet v2</a></p></li>
<li><p><a class="reference external" href="https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/models/classification_models/mobilenetv3.py">MobileNet v3</a></p></li>
<li><p><a class="reference external" href="https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/models/classification_models/pnasnet.py">PNASNet</a></p></li>
<li><p><a class="reference external" href="https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/models/classification_models/preact_resnet.py">Pre-activation ResNet</a></p></li>
<li><p><a class="reference external" href="https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/models/classification_models/regnet.py">RegNet</a></p></li>
<li><p><a class="reference external" href="https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/models/classification_models/repvgg.py">RepVGG</a></p></li>
<li><p><a class="reference external" href="https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/models/classification_models/resnet.py">ResNet</a></p></li>
<li><p><a class="reference external" href="https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/models/classification_models/resnext.py">ResNeXt</a></p></li>
<li><p><a class="reference external" href="https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/models/classification_models/senet.py">SENet </a></p></li>
<li><p><a class="reference external" href="https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/models/classification_models/shufflenet.py">ShuffleNet</a></p></li>
<li><p><a class="reference external" href="https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/models/classification_models/shufflenetv2.py">ShuffleNet v2</a></p></li>
<li><p><a class="reference external" href="https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/models/classification_models/vgg.py">VGG</a></p></li>
</ul>
</div>
<div class="section" id="id8">
<h2>Semantic Segmentation<a class="headerlink" href="#id8" title="Permalink to this heading">ÔÉÅ</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://bit.ly/3RrtMMO">PP-LiteSeg</a></p></li>
<li><p><a class="reference external" href="https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/models/segmentation_models/ddrnet.py">DDRNet (Deep Dual-resolution Networks)</a></p></li>
<li><p><a class="reference external" href="https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/models/segmentation_models/laddernet.py">LadderNet</a></p></li>
<li><p><a class="reference external" href="https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/models/segmentation_models/regseg.py">RegSeg</a></p></li>
<li><p><a class="reference external" href="https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/models/segmentation_models/shelfnet.py">ShelfNet</a></p></li>
<li><p><a class="reference external" href="https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/models/segmentation_models/stdc.py">STDC</a></p></li>
</ul>
</div>
<div class="section" id="id9">
<h2>Object Detection<a class="headerlink" href="#id9" title="Permalink to this heading">ÔÉÅ</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/models/detection_models/csp_darknet53.py">CSP DarkNet</a></p></li>
<li><p><a class="reference external" href="https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/models/detection_models/darknet53.py">DarkNet-53</a></p></li>
<li><p><a class="reference external" href="https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/models/detection_models/ssd.py">SSD (Single Shot Detector)</a></p></li>
<li><p><a class="reference external" href="https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/models/detection_models/yolox.py">YOLOX</a></p></li>
</ul>
<hr class="docutils" />
</div>
</div>
<div class="section" id="documentation">
<h1>Documentation<a class="headerlink" href="#documentation" title="Permalink to this heading">ÔÉÅ</a></h1>
<p>Check SuperGradients <a class="reference external" href="https://deci-ai.github.io/super-gradients/welcome.html">Docs</a> for full documentation, user guide, and examples.</p>
</div>
<div class="section" id="contributing">
<h1>Contributing<a class="headerlink" href="#contributing" title="Permalink to this heading">ÔÉÅ</a></h1>
<p>To learn about making a contribution to SuperGradients, please see our <a class="reference internal" href="CONTRIBUTING.html"><span class="doc std std-doc">Contribution page</span></a>.</p>
<p>Our awesome contributors:</p>
<a href="https://github.com/Deci-AI/super-gradients/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=Deci-AI/super-gradients" />
</a>
<p><br/>Made with <a class="reference external" href="https://contrib.rocks">contrib.rocks</a>.</p>
</div>
<div class="section" id="citation">
<h1>Citation<a class="headerlink" href="#citation" title="Permalink to this heading">ÔÉÅ</a></h1>
<p>If you are using SuperGradients library or benchmarks in your research, please cite SuperGradients deep learning training library.</p>
</div>
<div class="section" id="community">
<h1>Community<a class="headerlink" href="#community" title="Permalink to this heading">ÔÉÅ</a></h1>
<p>If you want to be a part of SuperGradients growing community, hear about all the exciting news and updates, need help, request for advanced features,
or want to file a bug or issue report, we would love to welcome you aboard!</p>
<ul class="simple">
<li><p>Slack is the place to be and ask questions about SuperGradients and get support. <a class="reference external" href="https://join.slack.com/t/supergradients-comm52/shared_invite/zt-10vz6o1ia-b_0W5jEPEnuHXm087K~t8Q">Click here to join our Slack</a></p></li>
<li><p>To report a bug, <a class="reference external" href="https://github.com/Deci-AI/super-gradients/issues">file an issue</a> on GitHub.</p></li>
<li><p>Join the <a class="reference external" href="https://www.supergradients.com/#Newsletter">SG Newsletter</a>
for staying up to date with new features and models, important announcements, and upcoming events.</p></li>
<li><p>For a short meeting with us, use this <a class="reference external" href="https://calendly.com/ofer-baratz-deci/15min">link</a> and choose your preferred time.</p></li>
</ul>
</div>
<div class="section" id="license">
<h1>License<a class="headerlink" href="#license" title="Permalink to this heading">ÔÉÅ</a></h1>
<p>This project is released under the <a class="reference internal" href="LICENSE.html"><span class="doc std std-doc">Apache 2.0 license</span></a>.</p>
<hr class="docutils" />
</div>
<div class="section" id="deci-platform">
<h1>Deci Platform<a class="headerlink" href="#deci-platform" title="Permalink to this heading">ÔÉÅ</a></h1>
<p>Deci Platform is our end to end platform for building, optimizing and deploying deep learning models to production.</p>
<p><a class="reference external" href="https://bit.ly/3qO3icq">Request free trial</a> to enjoy immediate improvement in throughput, latency, memory footprint and model size.</p>
<p>Features:</p>
<ul class="simple">
<li><p>Automatically compile and quantize your models with just a few clicks (TensorRT, OpenVINO).</p></li>
<li><p>Gain up to 10X improvement in throughput, latency, memory and model size.</p></li>
<li><p>Easily benchmark your models‚Äô performance on different hardware and batch sizes.</p></li>
<li><p>Invite co-workers to collaborate on models and communicate your progress.</p></li>
<li><p>Deci supports all common frameworks and Hardware, from Intel CPUs to Nvidia‚Äôs GPUs and Jetsons.
÷ø</p></li>
</ul>
<p>Request free trial <a class="reference external" href="https://bit.ly/3qO3icq">here</a></p>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="Welcome to SuperGradients‚Äôs documentation!" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="super_gradients.common.html" class="btn btn-neutral float-right" title="Common package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, SuperGradients team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>
