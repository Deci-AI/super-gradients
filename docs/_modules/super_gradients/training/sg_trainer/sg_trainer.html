<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>super_gradients.training.sg_trainer.sg_trainer &mdash; SuperGradients 3.0.3 documentation</title>
      <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/graphviz.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/sphinx_highlight.js"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../index.html" class="icon icon-home"> SuperGradients
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Welcome To SuperGradients</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../welcome.html">Version 3 is out! Notebooks have been updated!</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../welcome.html#build-with-supergradients">Build with SuperGradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../welcome.html#quick-installation">Quick Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../welcome.html#what-s-new">Whatâ€™s New</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../welcome.html#coming-soon">Coming soon</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../welcome.html#table-of-content">Table of Content</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../welcome.html#getting-started">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../welcome.html#advanced-features">Advanced Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../welcome.html#installation-methods">Installation Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../welcome.html#implemented-model-architectures">Implemented Model Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../welcome.html#documentation">Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../welcome.html#contributing">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../welcome.html#citation">Citation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../welcome.html#community">Community</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../welcome.html#license">License</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../welcome.html#deci-platform">Deci Platform</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Technical Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../super_gradients.common.html">Common package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../super_gradients.training.html">Training package</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">SuperGradients</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
      <li>super_gradients.training.sg_trainer.sg_trainer</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for super_gradients.training.sg_trainer.sg_trainer</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">inspect</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">deepcopy</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Mapping</span><span class="p">,</span> <span class="n">Dict</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">hydra</span>
<span class="kn">from</span> <span class="nn">omegaconf</span> <span class="kn">import</span> <span class="n">DictConfig</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">SequentialSampler</span>
<span class="kn">from</span> <span class="nn">torch.cuda.amp</span> <span class="kn">import</span> <span class="n">GradScaler</span><span class="p">,</span> <span class="n">autocast</span>
<span class="kn">from</span> <span class="nn">torchmetrics</span> <span class="kn">import</span> <span class="n">MetricCollection</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="nn">piptools.scripts.sync</span> <span class="kn">import</span> <span class="n">_get_installed_distributions</span>

<span class="kn">from</span> <span class="nn">torch.utils.data.distributed</span> <span class="kn">import</span> <span class="n">DistributedSampler</span>

<span class="kn">from</span> <span class="nn">super_gradients.common.factories.type_factory</span> <span class="kn">import</span> <span class="n">TypeFactory</span>
<span class="kn">from</span> <span class="nn">super_gradients.training.datasets.samplers</span> <span class="kn">import</span> <span class="n">InfiniteSampler</span><span class="p">,</span> <span class="n">RepeatAugSampler</span>

<span class="kn">from</span> <span class="nn">super_gradients.common.factories.callbacks_factory</span> <span class="kn">import</span> <span class="n">CallbacksFactory</span>
<span class="kn">from</span> <span class="nn">super_gradients.common.data_types.enum</span> <span class="kn">import</span> <span class="n">MultiGPUMode</span><span class="p">,</span> <span class="n">StrictLoad</span><span class="p">,</span> <span class="n">EvaluationType</span>
<span class="kn">from</span> <span class="nn">super_gradients.training.models.all_architectures</span> <span class="kn">import</span> <span class="n">ARCHITECTURES</span>
<span class="kn">from</span> <span class="nn">super_gradients.common.decorators.factory_decorator</span> <span class="kn">import</span> <span class="n">resolve_param</span>
<span class="kn">from</span> <span class="nn">super_gradients.common.environment</span> <span class="kn">import</span> <span class="n">env_helpers</span>
<span class="kn">from</span> <span class="nn">super_gradients.common.abstractions.abstract_logger</span> <span class="kn">import</span> <span class="n">get_logger</span><span class="p">,</span> <span class="n">mute_current_process</span>
<span class="kn">from</span> <span class="nn">super_gradients.common.factories.list_factory</span> <span class="kn">import</span> <span class="n">ListFactory</span>
<span class="kn">from</span> <span class="nn">super_gradients.common.factories.losses_factory</span> <span class="kn">import</span> <span class="n">LossesFactory</span>
<span class="kn">from</span> <span class="nn">super_gradients.common.factories.metrics_factory</span> <span class="kn">import</span> <span class="n">MetricsFactory</span>
<span class="kn">from</span> <span class="nn">super_gradients.common.sg_loggers</span> <span class="kn">import</span> <span class="n">SG_LOGGERS</span>
<span class="kn">from</span> <span class="nn">super_gradients.common.sg_loggers.abstract_sg_logger</span> <span class="kn">import</span> <span class="n">AbstractSGLogger</span>
<span class="kn">from</span> <span class="nn">super_gradients.common.sg_loggers.base_sg_logger</span> <span class="kn">import</span> <span class="n">BaseSGLogger</span>
<span class="kn">from</span> <span class="nn">super_gradients.training</span> <span class="kn">import</span> <span class="n">utils</span> <span class="k">as</span> <span class="n">core_utils</span><span class="p">,</span> <span class="n">models</span><span class="p">,</span> <span class="n">dataloaders</span>
<span class="kn">from</span> <span class="nn">super_gradients.training.models</span> <span class="kn">import</span> <span class="n">SgModule</span>
<span class="kn">from</span> <span class="nn">super_gradients.training.pretrained_models</span> <span class="kn">import</span> <span class="n">PRETRAINED_NUM_CLASSES</span>
<span class="kn">from</span> <span class="nn">super_gradients.training.utils</span> <span class="kn">import</span> <span class="n">sg_trainer_utils</span>
<span class="kn">from</span> <span class="nn">super_gradients.training.utils.sg_trainer_utils</span> <span class="kn">import</span> <span class="n">MonitoredValue</span><span class="p">,</span> <span class="n">parse_args</span><span class="p">,</span> <span class="n">log_main_training_params</span>
<span class="kn">from</span> <span class="nn">super_gradients.training.exceptions.sg_trainer_exceptions</span> <span class="kn">import</span> <span class="n">UnsupportedOptimizerFormat</span><span class="p">,</span> <span class="n">GPUModeNotSetupError</span>
<span class="kn">from</span> <span class="nn">super_gradients.training.losses</span> <span class="kn">import</span> <span class="n">LOSSES</span>
<span class="kn">from</span> <span class="nn">super_gradients.training.metrics.metric_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">get_metrics_titles</span><span class="p">,</span>
    <span class="n">get_metrics_results_tuple</span><span class="p">,</span>
    <span class="n">get_logging_values</span><span class="p">,</span>
    <span class="n">get_metrics_dict</span><span class="p">,</span>
    <span class="n">get_train_loop_description_dict</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">super_gradients.training.params</span> <span class="kn">import</span> <span class="n">TrainingParams</span>
<span class="kn">from</span> <span class="nn">super_gradients.training.utils.distributed_training_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">MultiGPUModeAutocastWrapper</span><span class="p">,</span>
    <span class="n">reduce_results_tuple_for_ddp</span><span class="p">,</span>
    <span class="n">compute_precise_bn_stats</span><span class="p">,</span>
    <span class="n">setup_device</span><span class="p">,</span>
    <span class="n">require_gpu_setup</span><span class="p">,</span>
    <span class="n">get_gpu_mem_utilization</span><span class="p">,</span>
    <span class="n">get_world_size</span><span class="p">,</span>
    <span class="n">get_local_rank</span><span class="p">,</span>
    <span class="n">wait_for_the_master</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">super_gradients.training.utils.ema</span> <span class="kn">import</span> <span class="n">ModelEMA</span>
<span class="kn">from</span> <span class="nn">super_gradients.training.utils.optimizer_utils</span> <span class="kn">import</span> <span class="n">build_optimizer</span>
<span class="kn">from</span> <span class="nn">super_gradients.training.utils.weight_averaging_utils</span> <span class="kn">import</span> <span class="n">ModelWeightAveraging</span>
<span class="kn">from</span> <span class="nn">super_gradients.training.metrics</span> <span class="kn">import</span> <span class="n">Accuracy</span><span class="p">,</span> <span class="n">Top5</span>
<span class="kn">from</span> <span class="nn">super_gradients.training.utils</span> <span class="kn">import</span> <span class="n">random_seed</span>
<span class="kn">from</span> <span class="nn">super_gradients.training.utils.checkpoint_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">get_ckpt_local_path</span><span class="p">,</span>
    <span class="n">read_ckpt_state_dict</span><span class="p">,</span>
    <span class="n">load_checkpoint_to_model</span><span class="p">,</span>
    <span class="n">load_pretrained_weights</span><span class="p">,</span>
    <span class="n">get_checkpoints_dir_path</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">super_gradients.training.datasets.datasets_utils</span> <span class="kn">import</span> <span class="n">DatasetStatisticsTensorboardLogger</span>
<span class="kn">from</span> <span class="nn">super_gradients.training.utils.callbacks</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">CallbackHandler</span><span class="p">,</span>
    <span class="n">Phase</span><span class="p">,</span>
    <span class="n">LR_SCHEDULERS_CLS_DICT</span><span class="p">,</span>
    <span class="n">PhaseContext</span><span class="p">,</span>
    <span class="n">MetricsUpdateCallback</span><span class="p">,</span>
    <span class="n">LR_WARMUP_CLS_DICT</span><span class="p">,</span>
    <span class="n">ContextSgMethods</span><span class="p">,</span>
    <span class="n">LRCallbackBase</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">super_gradients.common.environment</span> <span class="kn">import</span> <span class="n">environment_config</span>
<span class="kn">from</span> <span class="nn">super_gradients.training.utils</span> <span class="kn">import</span> <span class="n">HpmStruct</span>
<span class="kn">from</span> <span class="nn">super_gradients.training.utils.hydra_utils</span> <span class="kn">import</span> <span class="n">load_experiment_cfg</span><span class="p">,</span> <span class="n">add_params_to_cfg</span>
<span class="kn">from</span> <span class="nn">omegaconf</span> <span class="kn">import</span> <span class="n">OmegaConf</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">get_logger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<div class="viewcode-block" id="Trainer"><a class="viewcode-back" href="../../../../super_gradients.training.html#super_gradients.training.Trainer">[docs]</a><span class="k">class</span> <span class="nc">Trainer</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    SuperGradient Model - Base Class for Sg Models</span>

<span class="sd">    Methods</span>
<span class="sd">    -------</span>
<span class="sd">    train(max_epochs : int, initial_epoch : int, save_model : bool)</span>
<span class="sd">        the main function used for the training, h.p. updating, logging etc.</span>

<span class="sd">    predict(idx : int)</span>
<span class="sd">        returns the predictions and label of the current inputs</span>

<span class="sd">    test(epoch : int, idx : int, save : bool):</span>
<span class="sd">        returns the test loss, accuracy and runtime</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">experiment_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">multi_gpu</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">MultiGPUMode</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">MultiGPUMode</span><span class="o">.</span><span class="n">OFF</span><span class="p">,</span> <span class="n">ckpt_root_dir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        :param experiment_name:                      Used for logging and loading purposes</span>
<span class="sd">        :param device:                          If equal to &#39;cpu&#39; runs on the CPU otherwise on GPU</span>
<span class="sd">        :param multi_gpu:                       If True, runs on all available devices</span>
<span class="sd">                                                otherwise saves the Checkpoints Locally</span>
<span class="sd">                                                checkpoint from cloud service, otherwise overwrites the local checkpoints file</span>
<span class="sd">        :param ckpt_root_dir:                   Local root directory path where all experiment logging directories will</span>
<span class="sd">                                                reside. When none is give, it is assumed that</span>
<span class="sd">                                                pkg_resources.resource_filename(&#39;checkpoints&#39;, &quot;&quot;) exists and will be used.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># SET THE EMPTY PROPERTIES</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">architecture</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">arch_params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset_interface</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_gpu</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ema</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ema_model</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sg_logger</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_param_groups</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scaler</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">phase_callbacks</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_params</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pre_prediction_callback</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># SET THE DEFAULT PROPERTIES</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">half_precision</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_checkpoint</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_backbone</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_weights_only</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ddp_silent_mode</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">source_ckpt_folder_name</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_weight_averaging</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">average_model_checkpoint_filename</span> <span class="o">=</span> <span class="s2">&quot;average_model.pth&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">start_epoch</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_metric</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">external_checkpoint_path</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strict_load</span> <span class="o">=</span> <span class="n">StrictLoad</span><span class="o">.</span><span class="n">ON</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_ema_as_net</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ckpt_best_name</span> <span class="o">=</span> <span class="s2">&quot;ckpt_best.pth&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enable_qat</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">qat_params</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_infinite_train_loader</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_first_backward</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># METRICS</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_logging_items_names</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_metrics</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">valid_metrics</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">greater_metric_to_watch_is_better</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metric_to_watch</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">greater_train_metrics_is_better</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># For each metric, indicates if greater is better</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">greater_valid_metrics_is_better</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># SETTING THE PROPERTIES FROM THE CONSTRUCTOR</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">experiment_name</span> <span class="o">=</span> <span class="n">experiment_name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ckpt_name</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">checkpoints_dir_path</span> <span class="o">=</span> <span class="n">get_checkpoints_dir_path</span><span class="p">(</span><span class="n">experiment_name</span><span class="p">,</span> <span class="n">ckpt_root_dir</span><span class="p">)</span>

        <span class="c1"># INITIALIZE THE DEVICE FOR THE MODEL</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_device</span><span class="p">(</span><span class="n">requested_device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">requested_multi_gpu</span><span class="o">=</span><span class="n">multi_gpu</span><span class="p">)</span>

        <span class="c1"># SET THE DEFAULTS</span>
        <span class="c1"># TODO: SET DEFAULT TRAINING PARAMS FOR EACH TASK</span>

        <span class="n">default_results_titles</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Train Loss&quot;</span><span class="p">,</span> <span class="s2">&quot;Train Acc&quot;</span><span class="p">,</span> <span class="s2">&quot;Train Top5&quot;</span><span class="p">,</span> <span class="s2">&quot;Valid Loss&quot;</span><span class="p">,</span> <span class="s2">&quot;Valid Acc&quot;</span><span class="p">,</span> <span class="s2">&quot;Valid Top5&quot;</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">results_titles</span> <span class="o">=</span> <span class="n">default_results_titles</span>

        <span class="n">default_train_metrics</span><span class="p">,</span> <span class="n">default_valid_metrics</span> <span class="o">=</span> <span class="n">MetricCollection</span><span class="p">([</span><span class="n">Accuracy</span><span class="p">(),</span> <span class="n">Top5</span><span class="p">()]),</span> <span class="n">MetricCollection</span><span class="p">([</span><span class="n">Accuracy</span><span class="p">(),</span> <span class="n">Top5</span><span class="p">()])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">train_metrics</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">valid_metrics</span> <span class="o">=</span> <span class="n">default_train_metrics</span><span class="p">,</span> <span class="n">default_valid_metrics</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">train_monitored_values</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">valid_monitored_values</span> <span class="o">=</span> <span class="p">{}</span>

<div class="viewcode-block" id="Trainer.train_from_config"><a class="viewcode-back" href="../../../../super_gradients.training.html#super_gradients.training.Trainer.train_from_config">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">train_from_config</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">DictConfig</span><span class="p">,</span> <span class="nb">dict</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Trains according to cfg recipe configuration.</span>

<span class="sd">        @param cfg: The parsed DictConfig from yaml recipe files or a dictionary</span>
<span class="sd">        @return: the model and the output of trainer.train(...) (i.e results tuple)</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">setup_device</span><span class="p">(</span><span class="n">multi_gpu</span><span class="o">=</span><span class="n">core_utils</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="s2">&quot;multi_gpu&quot;</span><span class="p">,</span> <span class="n">MultiGPUMode</span><span class="o">.</span><span class="n">OFF</span><span class="p">),</span> <span class="n">num_gpus</span><span class="o">=</span><span class="n">core_utils</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="s2">&quot;num_gpus&quot;</span><span class="p">))</span>

        <span class="c1"># INSTANTIATE ALL OBJECTS IN CFG</span>
        <span class="n">cfg</span> <span class="o">=</span> <span class="n">hydra</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">instantiate</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>

        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">parse_args</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="fm">__init__</span><span class="p">)</span>

        <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># INSTANTIATE DATA LOADERS</span>
        <span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">dataloaders</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">train_dataloader</span><span class="p">,</span> <span class="n">dataset_params</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">dataset_params</span><span class="o">.</span><span class="n">train_dataset_params</span><span class="p">,</span> <span class="n">dataloader_params</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">dataset_params</span><span class="o">.</span><span class="n">train_dataloader_params</span>
        <span class="p">)</span>

        <span class="n">val_dataloader</span> <span class="o">=</span> <span class="n">dataloaders</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">val_dataloader</span><span class="p">,</span> <span class="n">dataset_params</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">dataset_params</span><span class="o">.</span><span class="n">val_dataset_params</span><span class="p">,</span> <span class="n">dataloader_params</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">dataset_params</span><span class="o">.</span><span class="n">val_dataloader_params</span>
        <span class="p">)</span>

        <span class="c1"># BUILD NETWORK</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="n">model_name</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">architecture</span><span class="p">,</span>
            <span class="n">num_classes</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">arch_params</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span>
            <span class="n">arch_params</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">arch_params</span><span class="p">,</span>
            <span class="n">strict_load</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">checkpoint_params</span><span class="o">.</span><span class="n">strict_load</span><span class="p">,</span>
            <span class="n">pretrained_weights</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">checkpoint_params</span><span class="o">.</span><span class="n">pretrained_weights</span><span class="p">,</span>
            <span class="n">checkpoint_path</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">checkpoint_params</span><span class="o">.</span><span class="n">checkpoint_path</span><span class="p">,</span>
            <span class="n">load_backbone</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">checkpoint_params</span><span class="o">.</span><span class="n">load_backbone</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">recipe_logged_cfg</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;recipe_config&quot;</span><span class="p">:</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">to_container</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">resolve</span><span class="o">=</span><span class="kc">True</span><span class="p">)}</span>
        <span class="c1"># TRAIN</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">train_loader</span><span class="o">=</span><span class="n">train_dataloader</span><span class="p">,</span>
            <span class="n">valid_loader</span><span class="o">=</span><span class="n">val_dataloader</span><span class="p">,</span>
            <span class="n">training_params</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">training_hyperparams</span><span class="p">,</span>
            <span class="n">additional_configs_to_log</span><span class="o">=</span><span class="n">recipe_logged_cfg</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">res</span></div>

<div class="viewcode-block" id="Trainer.resume_experiment"><a class="viewcode-back" href="../../../../super_gradients.training.html#super_gradients.training.Trainer.resume_experiment">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">resume_experiment</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">experiment_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">ckpt_root_dir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Resume a training that was run using our recipes.</span>

<span class="sd">        :param experiment_name:     Name of the experiment to resume</span>
<span class="sd">        :param ckpt_root_dir:       Directory including the checkpoints</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Resume training using the checkpoint recipe, ignoring the current recipe&quot;</span><span class="p">)</span>
        <span class="n">cfg</span> <span class="o">=</span> <span class="n">load_experiment_cfg</span><span class="p">(</span><span class="n">experiment_name</span><span class="p">,</span> <span class="n">ckpt_root_dir</span><span class="p">)</span>
        <span class="n">add_params_to_cfg</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;training_hyperparams.resume=True&quot;</span><span class="p">])</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">train_from_config</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span></div>

<div class="viewcode-block" id="Trainer.evaluate_from_recipe"><a class="viewcode-back" href="../../../../super_gradients.training.html#super_gradients.training.Trainer.evaluate_from_recipe">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">evaluate_from_recipe</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">DictConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluate according to a cfg recipe configuration.</span>

<span class="sd">        Note:   This script does NOT run training, only validation.</span>
<span class="sd">                Please make sure that the config refers to a PRETRAINED MODEL either from one of your checkpoint or from pretrained weights from model zoo.</span>
<span class="sd">        :param cfg: The parsed DictConfig from yaml recipe files or a dictionary</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">setup_device</span><span class="p">(</span><span class="n">multi_gpu</span><span class="o">=</span><span class="n">core_utils</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="s2">&quot;multi_gpu&quot;</span><span class="p">,</span> <span class="n">MultiGPUMode</span><span class="o">.</span><span class="n">OFF</span><span class="p">),</span> <span class="n">num_gpus</span><span class="o">=</span><span class="n">core_utils</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="s2">&quot;num_gpus&quot;</span><span class="p">))</span>

        <span class="c1"># INSTANTIATE ALL OBJECTS IN CFG</span>
        <span class="n">cfg</span> <span class="o">=</span> <span class="n">hydra</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">instantiate</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>

        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">parse_args</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="fm">__init__</span><span class="p">)</span>

        <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># INSTANTIATE DATA LOADERS</span>
        <span class="n">val_dataloader</span> <span class="o">=</span> <span class="n">dataloaders</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">val_dataloader</span><span class="p">,</span> <span class="n">dataset_params</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">dataset_params</span><span class="o">.</span><span class="n">val_dataset_params</span><span class="p">,</span> <span class="n">dataloader_params</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">dataset_params</span><span class="o">.</span><span class="n">val_dataloader_params</span>
        <span class="p">)</span>

        <span class="n">checkpoints_dir</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">get_checkpoints_dir_path</span><span class="p">(</span><span class="n">experiment_name</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">experiment_name</span><span class="p">,</span> <span class="n">ckpt_root_dir</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">ckpt_root_dir</span><span class="p">))</span>
        <span class="n">checkpoint_path</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">checkpoints_dir</span> <span class="o">/</span> <span class="n">cfg</span><span class="o">.</span><span class="n">training_hyperparams</span><span class="o">.</span><span class="n">ckpt_name</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Evaluating checkpoint: </span><span class="si">{</span><span class="n">checkpoint_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># BUILD NETWORK</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="n">model_name</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">architecture</span><span class="p">,</span>
            <span class="n">num_classes</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">arch_params</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span>
            <span class="n">arch_params</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">arch_params</span><span class="p">,</span>
            <span class="n">pretrained_weights</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">checkpoint_params</span><span class="o">.</span><span class="n">pretrained_weights</span><span class="p">,</span>
            <span class="n">checkpoint_path</span><span class="o">=</span><span class="n">checkpoint_path</span><span class="p">,</span>
            <span class="n">load_backbone</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">checkpoint_params</span><span class="o">.</span><span class="n">load_backbone</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># TEST</span>
        <span class="n">val_results_tuple</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">test_loader</span><span class="o">=</span><span class="n">val_dataloader</span><span class="p">,</span> <span class="n">test_metrics_list</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">training_hyperparams</span><span class="o">.</span><span class="n">valid_metrics_list</span><span class="p">)</span>

        <span class="n">valid_metrics_dict</span> <span class="o">=</span> <span class="n">get_metrics_dict</span><span class="p">(</span><span class="n">val_results_tuple</span><span class="p">,</span> <span class="n">trainer</span><span class="o">.</span><span class="n">test_metrics</span><span class="p">,</span> <span class="n">trainer</span><span class="o">.</span><span class="n">loss_logging_items_names</span><span class="p">)</span>

        <span class="n">results</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Validate Results&quot;</span><span class="p">]</span>
        <span class="n">results</span> <span class="o">+=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;   - </span><span class="si">{</span><span class="n">metric</span><span class="si">:</span><span class="s2">10</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">metric</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">valid_metrics_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()]</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">results</span><span class="p">))</span></div>

<div class="viewcode-block" id="Trainer.evaluate_checkpoint"><a class="viewcode-back" href="../../../../super_gradients.training.html#super_gradients.training.Trainer.evaluate_checkpoint">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">evaluate_checkpoint</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">experiment_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">ckpt_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;ckpt_latest.pth&quot;</span><span class="p">,</span> <span class="n">ckpt_root_dir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluate a checkpoint resulting from one of your previous experiment, using the same parameters (dataset, valid_metrics,...)</span>
<span class="sd">        as used during the training of the experiment</span>

<span class="sd">        Note:</span>
<span class="sd">            The parameters will be unchanged even if the recipe used for that experiment was changed since then.</span>
<span class="sd">            This is to ensure that validation of the experiment will remain exactly the same as during training.</span>

<span class="sd">        Example, evaluate the checkpoint &quot;average_model.pth&quot; from experiment &quot;my_experiment_name&quot;:</span>
<span class="sd">            &gt;&gt; evaluate_checkpoint(experiment_name=&quot;my_experiment_name&quot;, ckpt_name=&quot;average_model.pth&quot;)</span>

<span class="sd">        :param experiment_name:     Name of the experiment to validate</span>
<span class="sd">        :param ckpt_name:           Name of the checkpoint to test (&quot;ckpt_latest.pth&quot;, &quot;average_model.pth&quot; or &quot;ckpt_best.pth&quot; for instance)</span>
<span class="sd">        :param ckpt_root_dir:       Directory including the checkpoints</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Evaluate checkpoint&quot;</span><span class="p">)</span>
        <span class="n">cfg</span> <span class="o">=</span> <span class="n">load_experiment_cfg</span><span class="p">(</span><span class="n">experiment_name</span><span class="p">,</span> <span class="n">ckpt_root_dir</span><span class="p">)</span>
        <span class="n">add_params_to_cfg</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;training_hyperparams.resume=True&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;ckpt_name=</span><span class="si">{</span><span class="n">ckpt_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">])</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">evaluate_from_recipe</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_set_dataset_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dataset_params</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;train_dataset_params&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">dataset_params</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">,</span> <span class="s2">&quot;dataset_params&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s2">&quot;train_dataloader_params&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span><span class="o">.</span><span class="n">dataloader_params</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span><span class="p">,</span> <span class="s2">&quot;dataloader_params&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s2">&quot;valid_dataset_params&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">valid_loader</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">dataset_params</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">valid_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">,</span> <span class="s2">&quot;dataset_params&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s2">&quot;valid_dataloader_params&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">valid_loader</span><span class="o">.</span><span class="n">dataloader_params</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">valid_loader</span><span class="p">,</span> <span class="s2">&quot;dataloader_params&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dataset_params</span> <span class="o">=</span> <span class="n">HpmStruct</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset_params</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_net_to_device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Manipulates self.net according to self.multi_gpu</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># FOR MULTI-GPU TRAINING (not distributed)</span>
        <span class="n">sync_bn</span> <span class="o">=</span> <span class="n">core_utils</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="p">,</span> <span class="s2">&quot;sync_bn&quot;</span><span class="p">,</span> <span class="n">default_val</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_gpu</span> <span class="o">==</span> <span class="n">MultiGPUMode</span><span class="o">.</span><span class="n">DATA_PARALLEL</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_gpu</span> <span class="o">==</span> <span class="n">MultiGPUMode</span><span class="o">.</span><span class="n">DISTRIBUTED_DATA_PARALLEL</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">sync_bn</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">ddp_silent_mode</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;DDP - Using Sync Batch Norm... Training time will be affected accordingly&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">SyncBatchNorm</span><span class="o">.</span><span class="n">convert_sync_batchnorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

            <span class="n">local_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;:&quot;</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">local_rank</span><span class="p">],</span> <span class="n">output_device</span><span class="o">=</span><span class="n">local_rank</span><span class="p">,</span> <span class="n">find_unused_parameters</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">core_utils</span><span class="o">.</span><span class="n">WrappedModel</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_train_epoch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">silent_mode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        train_epoch - A single epoch training procedure</span>
<span class="sd">            :param optimizer:   The optimizer for the network</span>
<span class="sd">            :param epoch:       The current epoch</span>
<span class="sd">            :param silent_mode: No verbosity</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># SET THE MODEL IN training STATE</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="c1"># THE DISABLE FLAG CONTROLS WHETHER THE PROGRESS BAR IS SILENT OR PRINTS THE LOGS</span>
        <span class="n">progress_bar_train_loader</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">bar_format</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">{l_bar}{bar:10}{r_bar}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">dynamic_ncols</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">disable</span><span class="o">=</span><span class="n">silent_mode</span><span class="p">)</span>
        <span class="n">progress_bar_train_loader</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Train epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># RESET/INIT THE METRIC LOGGERS</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reset_metrics</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">train_metrics</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">loss_avg_meter</span> <span class="o">=</span> <span class="n">core_utils</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">AverageMeter</span><span class="p">()</span>

        <span class="n">context</span> <span class="o">=</span> <span class="n">PhaseContext</span><span class="p">(</span>
            <span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span><span class="p">,</span>
            <span class="n">optimizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span>
            <span class="n">metrics_compute_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_metrics</span><span class="p">,</span>
            <span class="n">loss_avg_meter</span><span class="o">=</span><span class="n">loss_avg_meter</span><span class="p">,</span>
            <span class="n">criterion</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="n">lr_warmup_epochs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">lr_warmup_epochs</span><span class="p">,</span>
            <span class="n">sg_logger</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sg_logger</span><span class="p">,</span>
            <span class="n">train_loader</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span><span class="p">,</span>
            <span class="n">context_methods</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_context_methods</span><span class="p">(</span><span class="n">Phase</span><span class="o">.</span><span class="n">TRAIN_BATCH_END</span><span class="p">),</span>
            <span class="n">ddp_silent_mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ddp_silent_mode</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">batch_items</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">progress_bar_train_loader</span><span class="p">):</span>
            <span class="n">batch_items</span> <span class="o">=</span> <span class="n">core_utils</span><span class="o">.</span><span class="n">tensor_container_to_device</span><span class="p">(</span><span class="n">batch_items</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">additional_batch_items</span> <span class="o">=</span> <span class="n">sg_trainer_utils</span><span class="o">.</span><span class="n">unpack_batch_items</span><span class="p">(</span><span class="n">batch_items</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_prediction_callback</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_prediction_callback</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>
            <span class="c1"># AUTOCAST IS ENABLED ONLY IF self.training_params.mixed_precision - IF enabled=False AUTOCAST HAS NO EFFECT</span>
            <span class="k">with</span> <span class="n">autocast</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">mixed_precision</span><span class="p">):</span>
                <span class="c1"># FORWARD PASS TO GET NETWORK&#39;S PREDICTIONS</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

                <span class="c1"># COMPUTE THE LOSS FOR BACK PROP + EXTRA METRICS COMPUTED DURING THE LOSS FORWARD PASS</span>
                <span class="n">loss</span><span class="p">,</span> <span class="n">loss_log_items</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_losses</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

            <span class="n">context</span><span class="o">.</span><span class="n">update_context</span><span class="p">(</span><span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">preds</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">targets</span><span class="p">,</span> <span class="n">loss_log_items</span><span class="o">=</span><span class="n">loss_log_items</span><span class="p">,</span> <span class="o">**</span><span class="n">additional_batch_items</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">phase_callback_handler</span><span class="p">(</span><span class="n">Phase</span><span class="o">.</span><span class="n">TRAIN_BATCH_END</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>

            <span class="c1"># LOG LR THAT WILL BE USED IN CURRENT EPOCH AND AFTER FIRST WARMUP/LR_SCHEDULER UPDATE BEFORE WEIGHT UPDATE</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">ddp_silent_mode</span> <span class="ow">and</span> <span class="n">batch_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_write_lrs</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_backward_step</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>

            <span class="c1"># COMPUTE THE RUNNING USER METRICS AND LOSS RUNNING ITEMS. RESULT TUPLE IS THEIR CONCATENATION.</span>
            <span class="n">logging_values</span> <span class="o">=</span> <span class="n">loss_avg_meter</span><span class="o">.</span><span class="n">average</span> <span class="o">+</span> <span class="n">get_metrics_results_tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_metrics</span><span class="p">)</span>
            <span class="n">gpu_memory_utilization</span> <span class="o">=</span> <span class="n">get_gpu_mem_utilization</span><span class="p">()</span> <span class="o">/</span> <span class="mf">1e9</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="mi">0</span>

            <span class="c1"># RENDER METRICS PROGRESS</span>
            <span class="n">pbar_message_dict</span> <span class="o">=</span> <span class="n">get_train_loop_description_dict</span><span class="p">(</span>
                <span class="n">logging_values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_metrics</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_logging_items_names</span><span class="p">,</span> <span class="n">gpu_mem</span><span class="o">=</span><span class="n">gpu_memory_utilization</span>
            <span class="p">)</span>

            <span class="n">progress_bar_train_loader</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">(</span><span class="o">**</span><span class="n">pbar_message_dict</span><span class="p">)</span>

            <span class="c1"># TODO: ITERATE BY MAX ITERS</span>
            <span class="c1"># FOR INFINITE SAMPLERS WE MUST BREAK WHEN REACHING LEN ITERATIONS.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_infinite_train_loader</span> <span class="ow">and</span> <span class="n">batch_idx</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">break</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">ddp_silent_mode</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sg_logger</span><span class="o">.</span><span class="n">upload</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">train_monitored_values</span> <span class="o">=</span> <span class="n">sg_trainer_utils</span><span class="o">.</span><span class="n">update_monitored_values_dict</span><span class="p">(</span>
            <span class="n">monitored_values_dict</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_monitored_values</span><span class="p">,</span> <span class="n">new_values_dict</span><span class="o">=</span><span class="n">pbar_message_dict</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">logging_values</span>

    <span class="k">def</span> <span class="nf">_get_losses</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">targets</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">]:</span>
        <span class="c1"># GET THE OUTPUT OF THE LOSS FUNCTION</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="n">loss</span><span class="p">,</span> <span class="n">loss_logging_items</span> <span class="o">=</span> <span class="n">loss</span>
            <span class="c1"># IF ITS NOT A TUPLE THE LOGGING ITEMS CONTAIN ONLY THE LOSS FOR BACKPROP (USER DEFINED LOSS RETURNS SCALAR)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">loss_logging_items</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

        <span class="c1"># ON FIRST BACKWARD, DERRIVE THE LOGGING TITLES.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_logging_items_names</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_first_backward</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init_loss_logging_names</span><span class="p">(</span><span class="n">loss_logging_items</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">metric_to_watch</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_init_monitored_items</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_first_backward</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">loss_logging_items</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_logging_items_names</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Loss output length must match loss_logging_items_names. Got &quot;</span>
                <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">loss_logging_items</span><span class="p">))</span>
                <span class="o">+</span> <span class="s2">&quot;, and &quot;</span>
                <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_logging_items_names</span><span class="p">))</span>
            <span class="p">)</span>
        <span class="c1"># RETURN AND THE LOSS LOGGING ITEMS COMPUTED DURING LOSS FORWARD PASS</span>
        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">loss_logging_items</span>

    <span class="k">def</span> <span class="nf">_init_monitored_items</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metric_idx_in_results_tuple</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_logging_items_names</span> <span class="o">+</span> <span class="n">get_metrics_titles</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">valid_metrics</span><span class="p">))</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metric_to_watch</span><span class="p">)</span>
        <span class="c1"># Instantiate the values to monitor (loss/metric)</span>
        <span class="k">for</span> <span class="n">loss_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_logging_items_names</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_monitored_values</span><span class="p">[</span><span class="n">loss_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">MonitoredValue</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">loss_name</span><span class="p">,</span> <span class="n">greater_is_better</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">valid_monitored_values</span><span class="p">[</span><span class="n">loss_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">MonitoredValue</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">loss_name</span><span class="p">,</span> <span class="n">greater_is_better</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">metric_name</span> <span class="ow">in</span> <span class="n">get_metrics_titles</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_metrics</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_monitored_values</span><span class="p">[</span><span class="n">metric_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">MonitoredValue</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">metric_name</span><span class="p">,</span> <span class="n">greater_is_better</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">greater_train_metrics_is_better</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">metric_name</span><span class="p">))</span>

        <span class="k">for</span> <span class="n">metric_name</span> <span class="ow">in</span> <span class="n">get_metrics_titles</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">valid_metrics</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">valid_monitored_values</span><span class="p">[</span><span class="n">metric_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">MonitoredValue</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">metric_name</span><span class="p">,</span> <span class="n">greater_is_better</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">greater_valid_metrics_is_better</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">metric_name</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">results_titles</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Train_&quot;</span> <span class="o">+</span> <span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_logging_items_names</span> <span class="o">+</span> <span class="n">get_metrics_titles</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_metrics</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span>
            <span class="s2">&quot;Valid_&quot;</span> <span class="o">+</span> <span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_logging_items_names</span> <span class="o">+</span> <span class="n">get_metrics_titles</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">valid_metrics</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">average_best_models</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model_weight_averaging</span> <span class="o">=</span> <span class="n">ModelWeightAveraging</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">checkpoints_dir_path</span><span class="p">,</span>
                <span class="n">greater_is_better</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">greater_metric_to_watch_is_better</span><span class="p">,</span>
                <span class="n">source_ckpt_folder_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">source_ckpt_folder_name</span><span class="p">,</span>
                <span class="n">metric_to_watch</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">metric_to_watch</span><span class="p">,</span>
                <span class="n">metric_idx</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">metric_idx_in_results_tuple</span><span class="p">,</span>
                <span class="n">load_checkpoint</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_backward_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">epoch</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">context</span><span class="p">:</span> <span class="n">PhaseContext</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Run backprop on the loss and perform a step</span>
<span class="sd">        :param loss: The value computed by the loss function</span>
<span class="sd">        :param optimizer: An object that can perform a gradient step and zeroize model gradient</span>
<span class="sd">        :param epoch: number of epoch the training is on</span>
<span class="sd">        :param batch_idx: number of iteration inside the current epoch</span>
<span class="sd">        :param context: current phase context</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># SCALER IS ENABLED ONLY IF self.training_params.mixed_precision=True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c1"># APPLY GRADIENT CLIPPING IF REQUIRED</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">clip_grad_norm</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">clip_grad_norm</span><span class="p">)</span>

        <span class="c1"># ACCUMULATE GRADIENT FOR X BATCHES BEFORE OPTIMIZING</span>
        <span class="n">integrated_batches_num</span> <span class="o">=</span> <span class="n">batch_idx</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span><span class="p">)</span> <span class="o">*</span> <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="n">integrated_batches_num</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_accumulate</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># SCALER IS ENABLED ONLY IF self.training_params.mixed_precision=True</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scaler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scaler</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ema</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">ema_model</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">,</span> <span class="n">integrated_batches_num</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_epochs</span><span class="p">))</span>

            <span class="c1"># RUN PHASE CALLBACKS</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">phase_callback_handler</span><span class="p">(</span><span class="n">Phase</span><span class="o">.</span><span class="n">TRAIN_BATCH_STEP</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">epoch</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">validation_results_tuple</span><span class="p">:</span> <span class="nb">tuple</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">context</span><span class="p">:</span> <span class="n">PhaseContext</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save the current state dict as latest (always), best (if metric was improved), epoch# (if determined in training</span>
<span class="sd">        params)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># WHEN THE validation_results_tuple IS NONE WE SIMPLY SAVE THE state_dict AS LATEST AND Return</span>
        <span class="k">if</span> <span class="n">validation_results_tuple</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sg_logger</span><span class="o">.</span><span class="n">add_checkpoint</span><span class="p">(</span><span class="n">tag</span><span class="o">=</span><span class="s2">&quot;ckpt_latest_weights_only.pth&quot;</span><span class="p">,</span> <span class="n">state_dict</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;net&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()},</span> <span class="n">global_step</span><span class="o">=</span><span class="n">epoch</span><span class="p">)</span>
            <span class="k">return</span>

        <span class="c1"># COMPUTE THE CURRENT metric</span>
        <span class="c1"># IF idx IS A LIST - SUM ALL THE VALUES STORED IN THE LIST&#39;S INDICES</span>
        <span class="n">metric</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">validation_results_tuple</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">metric_idx_in_results_tuple</span><span class="p">]</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metric_idx_in_results_tuple</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>
            <span class="k">else</span> <span class="nb">sum</span><span class="p">([</span><span class="n">validation_results_tuple</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">metric_idx_in_results_tuple</span><span class="p">])</span>
        <span class="p">)</span>

        <span class="c1"># BUILD THE state_dict</span>
        <span class="n">state</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;net&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s2">&quot;acc&quot;</span><span class="p">:</span> <span class="n">metric</span><span class="p">,</span> <span class="s2">&quot;epoch&quot;</span><span class="p">:</span> <span class="n">epoch</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">optimizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;optimizer_state_dict&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;scaler_state_dict&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaler</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ema</span><span class="p">:</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;ema_net&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ema_model</span><span class="o">.</span><span class="n">ema</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
        <span class="c1"># SAVES CURRENT MODEL AS ckpt_latest</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sg_logger</span><span class="o">.</span><span class="n">add_checkpoint</span><span class="p">(</span><span class="n">tag</span><span class="o">=</span><span class="s2">&quot;ckpt_latest.pth&quot;</span><span class="p">,</span> <span class="n">state_dict</span><span class="o">=</span><span class="n">state</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="n">epoch</span><span class="p">)</span>

        <span class="c1"># SAVE MODEL AT SPECIFIC EPOCHS DETERMINED BY save_ckpt_epoch_list</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">save_ckpt_epoch_list</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sg_logger</span><span class="o">.</span><span class="n">add_checkpoint</span><span class="p">(</span><span class="n">tag</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;ckpt_epoch_</span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">.pth&quot;</span><span class="p">,</span> <span class="n">state_dict</span><span class="o">=</span><span class="n">state</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="n">epoch</span><span class="p">)</span>

        <span class="c1"># OVERRIDE THE BEST CHECKPOINT AND best_metric IF metric GOT BETTER THAN THE PREVIOUS BEST</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">metric</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_metric</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">greater_metric_to_watch_is_better</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">metric</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_metric</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">greater_metric_to_watch_is_better</span><span class="p">):</span>
            <span class="c1"># STORE THE CURRENT metric AS BEST</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">best_metric</span> <span class="o">=</span> <span class="n">metric</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_save_best_checkpoint</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>

            <span class="c1"># RUN PHASE CALLBACKS</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">phase_callback_handler</span><span class="p">(</span><span class="n">Phase</span><span class="o">.</span><span class="n">VALIDATION_END_BEST_EPOCH</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">metric</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="n">metric</span> <span class="o">=</span> <span class="n">metric</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Best checkpoint overriden: validation &quot;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">metric_to_watch</span> <span class="o">+</span> <span class="s2">&quot;: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">metric</span><span class="p">))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">average_best_models</span><span class="p">:</span>
            <span class="n">net_for_averaging</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ema_model</span><span class="o">.</span><span class="n">ema</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ema</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span>
            <span class="n">averaged_model_sd</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_weight_averaging</span><span class="o">.</span><span class="n">get_average_model</span><span class="p">(</span><span class="n">net_for_averaging</span><span class="p">,</span> <span class="n">validation_results_tuple</span><span class="o">=</span><span class="n">validation_results_tuple</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sg_logger</span><span class="o">.</span><span class="n">add_checkpoint</span><span class="p">(</span><span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">average_model_checkpoint_filename</span><span class="p">,</span> <span class="n">state_dict</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;net&quot;</span><span class="p">:</span> <span class="n">averaged_model_sd</span><span class="p">},</span> <span class="n">global_step</span><span class="o">=</span><span class="n">epoch</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_save_best_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sg_logger</span><span class="o">.</span><span class="n">add_checkpoint</span><span class="p">(</span><span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ckpt_best_name</span><span class="p">,</span> <span class="n">state_dict</span><span class="o">=</span><span class="n">state</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="n">epoch</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_prep_net_for_train</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">arch_params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init_arch_params</span><span class="p">()</span>

        <span class="c1"># TODO: REMOVE THE BELOW LINE (FOR BACKWARD COMPATIBILITY)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_params</span> <span class="o">=</span> <span class="n">HpmStruct</span><span class="p">(</span><span class="n">load_checkpoint</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">resume</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_net_to_device</span><span class="p">()</span>

        <span class="c1"># SET THE FLAG FOR DIFFERENT PARAMETER GROUP OPTIMIZER UPDATE</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_param_groups</span> <span class="o">=</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;update_param_groups&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strict_load</span> <span class="o">=</span> <span class="n">core_utils</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="p">,</span> <span class="s2">&quot;resume_strict_load&quot;</span><span class="p">,</span> <span class="n">StrictLoad</span><span class="o">.</span><span class="n">ON</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_ema_as_net</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_checkpoint</span> <span class="o">=</span> <span class="n">core_utils</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="p">,</span> <span class="s2">&quot;resume&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">external_checkpoint_path</span> <span class="o">=</span> <span class="n">core_utils</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="p">,</span> <span class="s2">&quot;resume_path&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_checkpoint</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">load_checkpoint</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">external_checkpoint_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ckpt_name</span> <span class="o">=</span> <span class="n">core_utils</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="p">,</span> <span class="s2">&quot;ckpt_name&quot;</span><span class="p">,</span> <span class="s2">&quot;ckpt_latest.pth&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_load_checkpoint_to_model</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_init_arch_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">default_arch_params</span> <span class="o">=</span> <span class="n">HpmStruct</span><span class="p">()</span>
        <span class="n">arch_params</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">,</span> <span class="s2">&quot;arch_params&quot;</span><span class="p">,</span> <span class="n">default_arch_params</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">arch_params</span> <span class="o">=</span> <span class="n">default_arch_params</span>
        <span class="k">if</span> <span class="n">arch_params</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">arch_params</span><span class="o">.</span><span class="n">override</span><span class="p">(</span><span class="o">**</span><span class="n">arch_params</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>

    <span class="c1"># FIXME - we need to resolve flake8&#39;s &#39;function is too complex&#39; for this function</span>
<div class="viewcode-block" id="Trainer.train"><a class="viewcode-back" href="../../../../super_gradients.training.html#super_gradients.training.Trainer.train">[docs]</a>    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">training_params</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">train_loader</span><span class="p">:</span> <span class="n">DataLoader</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">valid_loader</span><span class="p">:</span> <span class="n">DataLoader</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">additional_configs_to_log</span><span class="p">:</span> <span class="n">Dict</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>  <span class="c1"># noqa: C901</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        train - Trains the Model</span>

<span class="sd">        IMPORTANT NOTE: Additional batch parameters can be added as a third item (optional) if a tuple is returned by</span>
<span class="sd">          the data loaders, as dictionary. The phase context will hold the additional items, under an attribute with</span>
<span class="sd">          the same name as the key in this dictionary. Then such items can be accessed through phase callbacks.</span>

<span class="sd">            :param additional_configs_to_log: Dict, dictionary containing configs that will be added to the training&#39;s</span>
<span class="sd">                sg_logger. Format should be {&quot;Config_title_1&quot;: {...}, &quot;Config_title_2&quot;:{..}}.</span>
<span class="sd">            :param model: torch.nn.Module, model to train.</span>

<span class="sd">            :param train_loader: Dataloader for train set.</span>
<span class="sd">            :param valid_loader: Dataloader for validation.</span>
<span class="sd">            :param training_params:</span>

<span class="sd">                - `resume` : bool (default=False)</span>

<span class="sd">                    Whether to continue training from ckpt with the same experiment name</span>
<span class="sd">                     (i.e resume from CKPT_ROOT_DIR/EXPERIMENT_NAME/CKPT_NAME)</span>

<span class="sd">                - `ckpt_name` : str (default=ckpt_latest.pth)</span>

<span class="sd">                    The checkpoint (.pth file) filename in CKPT_ROOT_DIR/EXPERIMENT_NAME/ to use when resume=True and</span>
<span class="sd">                     resume_path=None</span>

<span class="sd">                - `resume_path`: str (default=None)</span>

<span class="sd">                    Explicit checkpoint path (.pth file) to use to resume training.</span>

<span class="sd">                - `max_epochs` : int</span>

<span class="sd">                    Number of epochs to run training.</span>

<span class="sd">                - `lr_updates` : list(int)</span>

<span class="sd">                    List of fixed epoch numbers to perform learning rate updates when `lr_mode=&#39;step&#39;`.</span>

<span class="sd">                - `lr_decay_factor` : float</span>

<span class="sd">                    Decay factor to apply to the learning rate at each update when `lr_mode=&#39;step&#39;`.</span>


<span class="sd">                -  `lr_mode` : str</span>

<span class="sd">                    Learning rate scheduling policy, one of [&#39;step&#39;,&#39;poly&#39;,&#39;cosine&#39;,&#39;function&#39;]. &#39;step&#39; refers to</span>
<span class="sd">                    constant updates at epoch numbers passed through `lr_updates`. &#39;cosine&#39; refers to Cosine Anealing</span>
<span class="sd">                    policy as mentioned in https://arxiv.org/abs/1608.03983. &#39;poly&#39; refers to polynomial decrease i.e</span>
<span class="sd">                    in each epoch iteration `self.lr = self.initial_lr * pow((1.0 - (current_iter / max_iter)),</span>
<span class="sd">                    0.9)` &#39;function&#39; refers to user defined learning rate scheduling function, that is passed through</span>
<span class="sd">                    `lr_schedule_function`.</span>

<span class="sd">                - `lr_schedule_function` : Union[callable,None]</span>

<span class="sd">                    Learning rate scheduling function to be used when `lr_mode` is &#39;function&#39;.</span>

<span class="sd">                - `lr_warmup_epochs` : int (default=0)</span>

<span class="sd">                    Number of epochs for learning rate warm up - see https://arxiv.org/pdf/1706.02677.pdf (Section 2.2).</span>

<span class="sd">                - `cosine_final_lr_ratio` : float (default=0.01)</span>
<span class="sd">                    Final learning rate ratio (only relevant when `lr_mode`=&#39;cosine&#39;). The cosine starts from initial_lr and reaches</span>
<span class="sd">                     initial_lr * cosine_final_lr_ratio in last epoch</span>

<span class="sd">                - `inital_lr` : float</span>

<span class="sd">                    Initial learning rate.</span>

<span class="sd">                - `loss` : Union[nn.module, str]</span>

<span class="sd">                    Loss function for training.</span>
<span class="sd">                    One of SuperGradient&#39;s built in options:</span>

<span class="sd">                              &quot;cross_entropy&quot;: LabelSmoothingCrossEntropyLoss,</span>
<span class="sd">                              &quot;mse&quot;: MSELoss,</span>
<span class="sd">                              &quot;r_squared_loss&quot;: RSquaredLoss,</span>
<span class="sd">                              &quot;detection_loss&quot;: YoLoV3DetectionLoss,</span>
<span class="sd">                              &quot;shelfnet_ohem_loss&quot;: ShelfNetOHEMLoss,</span>
<span class="sd">                              &quot;shelfnet_se_loss&quot;: ShelfNetSemanticEncodingLoss,</span>
<span class="sd">                              &quot;ssd_loss&quot;: SSDLoss,</span>


<span class="sd">                    or user defined nn.module loss function.</span>

<span class="sd">                    IMPORTANT: forward(...) should return a (loss, loss_items) tuple where loss is the tensor used</span>
<span class="sd">                    for backprop (i.e what your original loss function returns), and loss_items should be a tensor of</span>
<span class="sd">                    shape (n_items), of values computed during the forward pass which we desire to log over the</span>
<span class="sd">                    entire epoch. For example- the loss itself should always be logged. Another example is a scenario</span>
<span class="sd">                    where the computed loss is the sum of a few components we would like to log- these entries in</span>
<span class="sd">                    loss_items).</span>

<span class="sd">                    IMPORTANT:When dealing with external loss classes, to logg/monitor the loss_items as described</span>
<span class="sd">                    above by specific string name:</span>

<span class="sd">                    Set a &quot;component_names&quot; property in the loss class, whos instance is passed through train_params,</span>
<span class="sd">                     to be a list of strings, of length n_items who&#39;s ith element is the name of the ith entry in loss_items.</span>
<span class="sd">                     Then each item will be logged, rendered on tensorboard and &quot;watched&quot; (i.e saving model checkpoints</span>
<span class="sd">                     according to it) under &lt;LOSS_CLASS.__name__&gt;&quot;/&quot;&lt;COMPONENT_NAME&gt;. If a single item is returned rather then a</span>
<span class="sd">                     tuple, it would be logged under &lt;LOSS_CLASS.__name__&gt;. When there is no such attributed, the items</span>
<span class="sd">                     will be named &lt;LOSS_CLASS.__name__&gt;&quot;/&quot;Loss_&quot;&lt;IDX&gt; according to the length of loss_items</span>

<span class="sd">                    For example:</span>
<span class="sd">                        class MyLoss(_Loss):</span>
<span class="sd">                            ...</span>
<span class="sd">                            def forward(self, inputs, targets):</span>
<span class="sd">                                ...</span>
<span class="sd">                                total_loss = comp1 + comp2</span>
<span class="sd">                                loss_items = torch.cat((total_loss.unsqueeze(0),comp1.unsqueeze(0), comp2.unsqueeze(0)).detach()</span>
<span class="sd">                                return total_loss, loss_items</span>
<span class="sd">                            ...</span>
<span class="sd">                            @property</span>
<span class="sd">                            def component_names(self):</span>
<span class="sd">                                return [&quot;total_loss&quot;, &quot;my_1st_component&quot;, &quot;my_2nd_component&quot;]</span>

<span class="sd">                    Trainer.train(...</span>
<span class="sd">                                    train_params={&quot;loss&quot;:MyLoss(),</span>
<span class="sd">                                                    ...</span>
<span class="sd">                                                    &quot;metric_to_watch&quot;: &quot;MyLoss/my_1st_component&quot;}</span>

<span class="sd">                        This will write to log and monitor MyLoss/total_loss, MyLoss/my_1st_component,</span>
<span class="sd">                         MyLoss/my_2nd_component.</span>

<span class="sd">                   For example:</span>
<span class="sd">                        class MyLoss2(_Loss):</span>
<span class="sd">                            ...</span>
<span class="sd">                            def forward(self, inputs, targets):</span>
<span class="sd">                                ...</span>
<span class="sd">                                total_loss = comp1 + comp2</span>
<span class="sd">                                loss_items = torch.cat((total_loss.unsqueeze(0),comp1.unsqueeze(0), comp2.unsqueeze(0)).detach()</span>
<span class="sd">                                return total_loss, loss_items</span>
<span class="sd">                            ...</span>

<span class="sd">                    Trainer.train(...</span>
<span class="sd">                                    train_params={&quot;loss&quot;:MyLoss(),</span>
<span class="sd">                                                    ...</span>
<span class="sd">                                                    &quot;metric_to_watch&quot;: &quot;MyLoss2/loss_0&quot;}</span>

<span class="sd">                        This will write to log and monitor MyLoss2/loss_0, MyLoss2/loss_1, MyLoss2/loss_2</span>
<span class="sd">                        as they have been named by their positional index in loss_items.</span>

<span class="sd">                    Since running logs will save the loss_items in some internal state, it is recommended that</span>
<span class="sd">                    loss_items are detached from their computational graph for memory efficiency.</span>

<span class="sd">                - `optimizer` : Union[str, torch.optim.Optimizer]</span>

<span class="sd">                    Optimization algorithm. One of [&#39;Adam&#39;,&#39;SGD&#39;,&#39;RMSProp&#39;] corresponding to the torch.optim</span>
<span class="sd">                    optimzers implementations, or any object that implements torch.optim.Optimizer.</span>

<span class="sd">                - `criterion_params` : dict</span>

<span class="sd">                    Loss function parameters.</span>

<span class="sd">                - `optimizer_params` : dict</span>
<span class="sd">                    When `optimizer` is one of [&#39;Adam&#39;,&#39;SGD&#39;,&#39;RMSProp&#39;], it will be initialized with optimizer_params.</span>

<span class="sd">                    (see https://pytorch.org/docs/stable/optim.html for the full list of</span>
<span class="sd">                    parameters for each optimizer).</span>

<span class="sd">                - `train_metrics_list` : list(torchmetrics.Metric)</span>

<span class="sd">                    Metrics to log during training. For more information on torchmetrics see</span>
<span class="sd">                    https://torchmetrics.rtfd.io/en/latest/.</span>


<span class="sd">                - `valid_metrics_list` : list(torchmetrics.Metric)</span>

<span class="sd">                    Metrics to log during validation/testing. For more information on torchmetrics see</span>
<span class="sd">                    https://torchmetrics.rtfd.io/en/latest/.</span>


<span class="sd">                - `loss_logging_items_names` : list(str)</span>

<span class="sd">                    The list of names/titles for the outputs returned from the loss functions forward pass (reminder-</span>
<span class="sd">                    the loss function should return the tuple (loss, loss_items)). These names will be used for</span>
<span class="sd">                    logging their values.</span>

<span class="sd">                - `metric_to_watch` : str (default=&quot;Accuracy&quot;)</span>

<span class="sd">                    will be the metric which the model checkpoint will be saved according to, and can be set to any</span>
<span class="sd">                    of the following:</span>

<span class="sd">                        a metric name (str) of one of the metric objects from the valid_metrics_list</span>

<span class="sd">                        a &quot;metric_name&quot; if some metric in valid_metrics_list has an attribute component_names which</span>
<span class="sd">                        is a list referring to the names of each entry in the output metric (torch tensor of size n)</span>

<span class="sd">                        one of &quot;loss_logging_items_names&quot; i.e which will correspond to an item returned during the</span>
<span class="sd">                        loss function&#39;s forward pass (see loss docs abov).</span>

<span class="sd">                    At the end of each epoch, if a new best metric_to_watch value is achieved, the models checkpoint</span>
<span class="sd">                    is saved in YOUR_PYTHON_PATH/checkpoints/ckpt_best.pth</span>

<span class="sd">                - `greater_metric_to_watch_is_better` : bool</span>

<span class="sd">                    When choosing a model&#39;s checkpoint to be saved, the best achieved model is the one that maximizes the</span>
<span class="sd">                     metric_to_watch when this parameter is set to True, and a one that minimizes it otherwise.</span>

<span class="sd">                - `ema` : bool (default=False)</span>

<span class="sd">                    Whether to use Model Exponential Moving Average (see</span>
<span class="sd">                    https://github.com/rwightman/pytorch-image-models ema implementation)</span>

<span class="sd">                - `batch_accumulate` : int (default=1)</span>

<span class="sd">                    Number of batches to accumulate before every backward pass.</span>

<span class="sd">                - `ema_params` : dict</span>

<span class="sd">                    Parameters for the ema model.</span>

<span class="sd">                - `zero_weight_decay_on_bias_and_bn` : bool (default=False)</span>

<span class="sd">                    Whether to apply weight decay on batch normalization parameters or not (ignored when the passed</span>
<span class="sd">                    optimizer has already been initialized).</span>


<span class="sd">                - `load_opt_params` : bool (default=True)</span>

<span class="sd">                    Whether to load the optimizers parameters as well when loading a model&#39;s checkpoint.</span>

<span class="sd">                - `run_validation_freq` : int (default=1)</span>

<span class="sd">                    The frequency in which validation is performed during training (i.e the validation is ran every</span>
<span class="sd">                     `run_validation_freq` epochs.</span>

<span class="sd">                - `save_model` : bool (default=True)</span>

<span class="sd">                    Whether to save the model checkpoints.</span>

<span class="sd">                - `silent_mode` : bool</span>

<span class="sd">                    Silents the print outs.</span>

<span class="sd">                - `mixed_precision` : bool</span>

<span class="sd">                    Whether to use mixed precision or not.</span>

<span class="sd">                - `save_ckpt_epoch_list` : list(int) (default=[])</span>

<span class="sd">                    List of fixed epoch indices the user wishes to save checkpoints in.</span>

<span class="sd">                - `average_best_models` : bool (default=False)</span>

<span class="sd">                    If set, a snapshot dictionary file and the average model will be saved / updated at every epoch</span>
<span class="sd">                    and evaluated only when training is completed. The snapshot file will only be deleted upon</span>
<span class="sd">                    completing the training. The snapshot dict will be managed on cpu.</span>

<span class="sd">                - `precise_bn` : bool (default=False)</span>

<span class="sd">                    Whether to use precise_bn calculation during the training.</span>

<span class="sd">                - `precise_bn_batch_size` : int (default=None)</span>

<span class="sd">                    The effective batch size we want to calculate the batchnorm on. For example, if we are training a model</span>
<span class="sd">                    on 8 gpus, with a batch of 128 on each gpu, a good rule of thumb would be to give it 8192</span>
<span class="sd">                    (ie: effective_batch_size * num_gpus = batch_per_gpu * num_gpus * num_gpus).</span>
<span class="sd">                    If precise_bn_batch_size is not provided in the training_params, the latter heuristic will be taken.</span>

<span class="sd">                - `seed` : int (default=42)</span>

<span class="sd">                    Random seed to be set for torch, numpy, and random. When using DDP each process will have it&#39;s seed</span>
<span class="sd">                    set to seed + rank.</span>


<span class="sd">                - `log_installed_packages` : bool (default=False)</span>

<span class="sd">                    When set, the list of all installed packages (and their versions) will be written to the tensorboard</span>
<span class="sd">                     and logfile (useful when trying to reproduce results).</span>

<span class="sd">                - `dataset_statistics` : bool (default=False)</span>

<span class="sd">                    Enable a statistic analysis of the dataset. If set to True the dataset will be analyzed and a report</span>
<span class="sd">                    will be added to the tensorboard along with some sample images from the dataset. Currently only</span>
<span class="sd">                    detection datasets are supported for analysis.</span>

<span class="sd">                -  `sg_logger` : Union[AbstractSGLogger, str] (defauls=base_sg_logger)</span>

<span class="sd">                    Define the SGLogger object for this training process. The SGLogger handles all disk writes, logs, TensorBoard, remote logging</span>
<span class="sd">                    and remote storage. By overriding the default base_sg_logger, you can change the storage location, support external monitoring and logging</span>
<span class="sd">                    or support remote storage.</span>

<span class="sd">                -   `sg_logger_params` : dict</span>

<span class="sd">                    SGLogger parameters</span>

<span class="sd">                -   `clip_grad_norm` : float</span>

<span class="sd">                    Defines a maximal L2 norm of the gradients. Values which exceed the given value will be clipped</span>

<span class="sd">                -   `lr_cooldown_epochs` : int (default=0)</span>

<span class="sd">                    Number of epochs to cooldown LR (i.e the last epoch from scheduling view point=max_epochs-cooldown).</span>

<span class="sd">                -   `pre_prediction_callback` : Callable (default=None)</span>

<span class="sd">                     When not None, this callback will be applied to images and targets, and returning them to be used</span>
<span class="sd">                      for the forward pass, and further computations. Args for this callable should be in the order</span>
<span class="sd">                      (inputs, targets, batch_idx) returning modified_inputs, modified_targets</span>

<span class="sd">                -   `ckpt_best_name` : str (default=&#39;ckpt_best.pth&#39;)</span>

<span class="sd">                    The best checkpoint (according to metric_to_watch) will be saved under this filename in the checkpoints directory.</span>

<span class="sd">                -   `enable_qat`: bool (default=False)</span>

<span class="sd">                    Adds a QATCallback to the phase callbacks, that triggers quantization aware training starting from</span>
<span class="sd">                     qat_params[&quot;start_epoch&quot;]</span>

<span class="sd">                -   `qat_params`: dict-like object with the following key/values:</span>

<span class="sd">                        start_epoch: int, first epoch to start QAT.</span>

<span class="sd">                        quant_modules_calib_method: str, One of [percentile, mse, entropy, max]. Statistics method for amax</span>
<span class="sd">                         computation of the quantized modules (default=percentile).</span>

<span class="sd">                        per_channel_quant_modules: bool, whether quant modules should be per channel (default=False).</span>

<span class="sd">                        calibrate: bool, whether to perfrom calibration (default=False).</span>

<span class="sd">                        calibrated_model_path: str, path to a calibrated checkpoint (default=None).</span>

<span class="sd">                        calib_data_loader: torch.utils.data.DataLoader, data loader of the calibration dataset. When None,</span>
<span class="sd">                         context.train_loader will be used (default=None).</span>

<span class="sd">                        num_calib_batches: int, number of batches to collect the statistics from.</span>

<span class="sd">                        percentile: float, percentile value to use when Trainer,quant_modules_calib_method=&#39;percentile&#39;.</span>
<span class="sd">                         Discarded when other methods are used (Default=99.99).</span>


<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">global</span> <span class="n">logger</span>
        <span class="k">if</span> <span class="n">training_params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">training_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span> <span class="o">=</span> <span class="n">train_loader</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">valid_loader</span> <span class="o">=</span> <span class="n">valid_loader</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">valid_loader</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_dataset_params</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_gpu</span> <span class="o">==</span> <span class="n">MultiGPUMode</span><span class="o">.</span><span class="n">DISTRIBUTED_DATA_PARALLEL</span><span class="p">:</span>
            <span class="c1"># Note: the dataloader uses sampler of the batch_sampler when it is not None.</span>
            <span class="n">train_sampler</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span><span class="o">.</span><span class="n">batch_sampler</span><span class="o">.</span><span class="n">sampler</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span><span class="o">.</span><span class="n">batch_sampler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span><span class="o">.</span><span class="n">sampler</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">train_sampler</span><span class="p">,</span> <span class="n">SequentialSampler</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;You are using a SequentialSampler on you training dataloader, while working on DDP. &quot;</span>
                    <span class="s2">&quot;This cancels the DDP benefits since it makes each process iterate through the entire dataset&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">train_sampler</span><span class="p">,</span> <span class="p">(</span><span class="n">DistributedSampler</span><span class="p">,</span> <span class="n">InfiniteSampler</span><span class="p">,</span> <span class="n">RepeatAugSampler</span><span class="p">)):</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;The training sampler you are using might not support DDP. &quot;</span>
                    <span class="s2">&quot;If it doesnt, please use one of the following sampler: DistributedSampler, InfiniteSampler, RepeatAugSampler&quot;</span>
                <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span> <span class="o">=</span> <span class="n">TrainingParams</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">override</span><span class="p">(</span><span class="o">**</span><span class="n">training_params</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prep_net_for_train</span><span class="p">()</span>

        <span class="c1"># SET RANDOM SEED</span>
        <span class="n">random_seed</span><span class="p">(</span><span class="n">is_ddp</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">multi_gpu</span> <span class="o">==</span> <span class="n">MultiGPUMode</span><span class="o">.</span><span class="n">DISTRIBUTED_DATA_PARALLEL</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>

        <span class="n">silent_mode</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">silent_mode</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">ddp_silent_mode</span>
        <span class="c1"># METRICS</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_train_metrics</span><span class="p">(</span><span class="n">train_metrics_list</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">train_metrics_list</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_valid_metrics</span><span class="p">(</span><span class="n">valid_metrics_list</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">valid_metrics_list</span><span class="p">)</span>

        <span class="c1"># Store the metric to follow (loss\accuracy) and initialize as the worst value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metric_to_watch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">metric_to_watch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">greater_metric_to_watch_is_better</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">greater_metric_to_watch_is_better</span>

        <span class="c1"># Allowing loading instantiated loss or string</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">criterion_cls</span> <span class="o">=</span> <span class="n">LOSSES</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">loss</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">criterion_cls</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">criterion_params</span><span class="p">)</span>

        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span> <span class="n">Mapping</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">LossesFactory</span><span class="p">()</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>

        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">loss</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">max_epochs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">max_epochs</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ema</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">ema</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">precise_bn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">precise_bn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">precise_bn_batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">precise_bn_batch_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">batch_accumulate</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">batch_accumulate</span>
        <span class="n">num_batches</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ema</span><span class="p">:</span>
            <span class="n">ema_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">ema_params</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Using EMA with params </span><span class="si">{</span><span class="n">ema_params</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ema_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_instantiate_ema_model</span><span class="p">(</span><span class="o">**</span><span class="n">ema_params</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ema_model</span><span class="o">.</span><span class="n">updates</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_epoch</span> <span class="o">*</span> <span class="n">num_batches</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_accumulate</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">:</span>
                <span class="k">if</span> <span class="s2">&quot;ema_net&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">ema_model</span><span class="o">.</span><span class="n">ema</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">checkpoint</span><span class="p">[</span><span class="s2">&quot;ema_net&quot;</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">ema</span> <span class="o">=</span> <span class="kc">False</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;[Warning] Checkpoint does not include EMA weights, continuing training without EMA.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">run_validation_freq</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">run_validation_freq</span>
        <span class="n">validation_results_tuple</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">inf_time</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">timer</span> <span class="o">=</span> <span class="n">core_utils</span><span class="o">.</span><span class="n">Timer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># IF THE LR MODE IS NOT DEFAULT TAKE IT FROM THE TRAINING PARAMS</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr_mode</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">lr_mode</span>
        <span class="n">load_opt_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">load_opt_params</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">phase_callbacks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">phase_callbacks</span> <span class="ow">or</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">phase_callbacks</span> <span class="o">=</span> <span class="n">ListFactory</span><span class="p">(</span><span class="n">CallbacksFactory</span><span class="p">())</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">phase_callbacks</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr_mode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sg_lr_callback_cls</span> <span class="o">=</span> <span class="n">LR_SCHEDULERS_CLS_DICT</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">lr_mode</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">phase_callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">sg_lr_callback_cls</span><span class="p">(</span>
                    <span class="n">train_loader_len</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span><span class="p">),</span>
                    <span class="n">net</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">,</span>
                    <span class="n">training_params</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="p">,</span>
                    <span class="n">update_param_groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">update_param_groups</span><span class="p">,</span>
                    <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(),</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">lr_warmup_epochs</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">warmup_mode</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">warmup_mode</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">warmup_mode</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="n">warmup_callback_cls</span> <span class="o">=</span> <span class="n">LR_WARMUP_CLS_DICT</span><span class="p">[</span><span class="n">warmup_mode</span><span class="p">]</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">warmup_mode</span><span class="p">,</span> <span class="nb">type</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">warmup_mode</span><span class="p">,</span> <span class="n">LRCallbackBase</span><span class="p">):</span>
                <span class="n">warmup_callback_cls</span> <span class="o">=</span> <span class="n">warmup_mode</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;warmup_mode has to be either a name of a mode (str) or a subclass of PhaseCallback&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">phase_callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">warmup_callback_cls</span><span class="p">(</span>
                    <span class="n">train_loader_len</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span><span class="p">),</span>
                    <span class="n">net</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">,</span>
                    <span class="n">training_params</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="p">,</span>
                    <span class="n">update_param_groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">update_param_groups</span><span class="p">,</span>
                    <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(),</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_add_metrics_update_callback</span><span class="p">(</span><span class="n">Phase</span><span class="o">.</span><span class="n">TRAIN_BATCH_END</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_add_metrics_update_callback</span><span class="p">(</span><span class="n">Phase</span><span class="o">.</span><span class="n">VALIDATION_BATCH_END</span><span class="p">)</span>

        <span class="c1"># ADD CALLBACK FOR QAT</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enable_qat</span> <span class="o">=</span> <span class="n">core_utils</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="p">,</span> <span class="s2">&quot;enable_qat&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_qat</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="s2">&quot;QAT is not implemented as a plug-and-play feature yet. Please refer to examples/resnet_qat to learn how to do it manually.&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">phase_callback_handler</span> <span class="o">=</span> <span class="n">CallbackHandler</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">phase_callbacks</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">ddp_silent_mode</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_sg_logger_objects</span><span class="p">(</span><span class="n">additional_configs_to_log</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">dataset_statistics</span><span class="p">:</span>
                <span class="n">dataset_statistics_logger</span> <span class="o">=</span> <span class="n">DatasetStatisticsTensorboardLogger</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sg_logger</span><span class="p">)</span>
                <span class="n">dataset_statistics_logger</span><span class="o">.</span><span class="n">analyze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">all_classes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">classes</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Train-set&quot;</span><span class="p">,</span> <span class="n">anchors</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">arch_params</span><span class="o">.</span><span class="n">anchors</span><span class="p">)</span>
                <span class="n">dataset_statistics_logger</span><span class="o">.</span><span class="n">analyze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">valid_loader</span><span class="p">,</span> <span class="n">all_classes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">classes</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;val-set&quot;</span><span class="p">)</span>

        <span class="n">sg_trainer_utils</span><span class="o">.</span><span class="n">log_uncaught_exceptions</span><span class="p">(</span><span class="n">logger</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">load_checkpoint</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">load_weights_only</span><span class="p">:</span>
            <span class="c1"># WHEN STARTING TRAINING FROM SCRATCH, DO NOT LOAD OPTIMIZER PARAMS (EVEN IF LOADING BACKBONE)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">start_epoch</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_reset_best_metric</span><span class="p">()</span>
            <span class="n">load_opt_params</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="n">inspect</span><span class="o">.</span><span class="n">isclass</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">optimizer</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">issubclass</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">build_optimizer</span><span class="p">(</span><span class="n">net</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">initial_lr</span><span class="p">,</span> <span class="n">training_params</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">optimizer</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">UnsupportedOptimizerFormat</span><span class="p">()</span>

        <span class="c1"># VERIFY GRADIENT CLIPPING VALUE</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">clip_grad_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">clip_grad_norm</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Params&quot;</span><span class="p">,</span> <span class="s2">&quot;Invalid clip_grad_norm&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">load_checkpoint</span> <span class="ow">and</span> <span class="n">load_opt_params</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">checkpoint</span><span class="p">[</span><span class="s2">&quot;optimizer_state_dict&quot;</span><span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pre_prediction_callback</span> <span class="o">=</span> <span class="n">CallbacksFactory</span><span class="p">()</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">pre_prediction_callback</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_mixed_precision</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">mixed_precision</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_infinite_train_loader</span> <span class="o">=</span> <span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span><span class="p">,</span> <span class="s2">&quot;sampler&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span><span class="o">.</span><span class="n">sampler</span><span class="p">,</span> <span class="n">InfiniteSampler</span><span class="p">))</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span><span class="p">,</span> <span class="s2">&quot;batch_sampler&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span><span class="o">.</span><span class="n">batch_sampler</span><span class="o">.</span><span class="n">sampler</span><span class="p">,</span> <span class="n">InfiniteSampler</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ckpt_best_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">ckpt_best_name</span>

        <span class="c1"># STATE ATTRIBUTE SET HERE FOR SUBSEQUENT TRAIN() CALLS</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_first_backward</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">context</span> <span class="o">=</span> <span class="n">PhaseContext</span><span class="p">(</span>
            <span class="n">optimizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span>
            <span class="n">net</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">,</span>
            <span class="n">experiment_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">experiment_name</span><span class="p">,</span>
            <span class="n">ckpt_dir</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">checkpoints_dir_path</span><span class="p">,</span>
            <span class="n">criterion</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">,</span>
            <span class="n">lr_warmup_epochs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">lr_warmup_epochs</span><span class="p">,</span>
            <span class="n">sg_logger</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sg_logger</span><span class="p">,</span>
            <span class="n">train_loader</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span><span class="p">,</span>
            <span class="n">valid_loader</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">valid_loader</span><span class="p">,</span>
            <span class="n">training_params</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="p">,</span>
            <span class="n">ddp_silent_mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ddp_silent_mode</span><span class="p">,</span>
            <span class="n">checkpoint_params</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_params</span><span class="p">,</span>
            <span class="n">architecture</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">architecture</span><span class="p">,</span>
            <span class="n">arch_params</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">arch_params</span><span class="p">,</span>
            <span class="n">metric_to_watch</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">metric_to_watch</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="n">context_methods</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_context_methods</span><span class="p">(</span><span class="n">Phase</span><span class="o">.</span><span class="n">PRE_TRAINING</span><span class="p">),</span>
            <span class="n">ema_model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ema_model</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">phase_callback_handler</span><span class="p">(</span><span class="n">Phase</span><span class="o">.</span><span class="n">PRE_TRAINING</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>

        <span class="n">first_batch</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span><span class="p">))</span>
        <span class="n">log_main_training_params</span><span class="p">(</span>
            <span class="n">multi_gpu</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">multi_gpu</span><span class="p">,</span>
            <span class="n">num_gpus</span><span class="o">=</span><span class="n">get_world_size</span><span class="p">(),</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">first_batch</span><span class="p">),</span>
            <span class="n">batch_accumulate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_accumulate</span><span class="p">,</span>
            <span class="n">len_train_set</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># HEADERS OF THE TRAINING PROGRESS</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">silent_mode</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Started training for </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">max_epochs</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_epoch</span><span class="si">}</span><span class="s2"> epochs (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">start_epoch</span><span class="si">}</span><span class="s2">/&quot;</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">max_epochs</span> <span class="o">-</span> <span class="mi">1</span><span class="si">}</span><span class="s2">)</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">start_epoch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_epochs</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">stop_training</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Request to stop training has been received, stopping training&quot;</span><span class="p">)</span>
                    <span class="k">break</span>

                <span class="c1"># Phase.TRAIN_EPOCH_START</span>
                <span class="c1"># RUN PHASE CALLBACKS</span>
                <span class="n">context</span><span class="o">.</span><span class="n">update_context</span><span class="p">(</span><span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">phase_callback_handler</span><span class="p">(</span><span class="n">Phase</span><span class="o">.</span><span class="n">TRAIN_EPOCH_START</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>

                <span class="c1"># IN DDP- SET_EPOCH WILL CAUSE EVERY PROCESS TO BE EXPOSED TO THE ENTIRE DATASET BY SHUFFLING WITH A</span>
                <span class="c1"># DIFFERENT SEED EACH EPOCH START</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">multi_gpu</span> <span class="o">==</span> <span class="n">MultiGPUMode</span><span class="o">.</span><span class="n">DISTRIBUTED_DATA_PARALLEL</span>
                    <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span><span class="p">,</span> <span class="s2">&quot;sampler&quot;</span><span class="p">)</span>
                    <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span><span class="o">.</span><span class="n">sampler</span><span class="p">,</span> <span class="s2">&quot;set_epoch&quot;</span><span class="p">)</span>
                <span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">set_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>

                <span class="n">train_metrics_tuple</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span><span class="p">,</span> <span class="n">silent_mode</span><span class="o">=</span><span class="n">silent_mode</span><span class="p">)</span>

                <span class="c1"># Phase.TRAIN_EPOCH_END</span>
                <span class="c1"># RUN PHASE CALLBACKS</span>
                <span class="n">train_metrics_dict</span> <span class="o">=</span> <span class="n">get_metrics_dict</span><span class="p">(</span><span class="n">train_metrics_tuple</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_metrics</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_logging_items_names</span><span class="p">)</span>

                <span class="n">context</span><span class="o">.</span><span class="n">update_context</span><span class="p">(</span><span class="n">metrics_dict</span><span class="o">=</span><span class="n">train_metrics_dict</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">phase_callback_handler</span><span class="p">(</span><span class="n">Phase</span><span class="o">.</span><span class="n">TRAIN_EPOCH_END</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>

                <span class="c1"># CALCULATE PRECISE BATCHNORM STATS</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">precise_bn</span><span class="p">:</span>
                    <span class="n">compute_precise_bn_stats</span><span class="p">(</span>
                        <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">,</span> <span class="n">loader</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">precise_bn_batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">precise_bn_batch_size</span><span class="p">,</span> <span class="n">num_gpus</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_devices</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ema</span><span class="p">:</span>
                        <span class="n">compute_precise_bn_stats</span><span class="p">(</span>
                            <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ema_model</span><span class="o">.</span><span class="n">ema</span><span class="p">,</span> <span class="n">loader</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">precise_bn_batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">precise_bn_batch_size</span><span class="p">,</span> <span class="n">num_gpus</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_devices</span>
                        <span class="p">)</span>

                <span class="c1"># model switch - we replace self.net.module with the ema model for the testing and saving part</span>
                <span class="c1"># and then switch it back before the next training epoch</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ema</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">ema_model</span><span class="o">.</span><span class="n">update_attr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">)</span>
                    <span class="n">keep_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ema_model</span><span class="o">.</span><span class="n">ema</span>

                <span class="c1"># RUN TEST ON VALIDATION SET EVERY self.run_validation_freq EPOCHS</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_validation_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">timer</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
                    <span class="n">validation_results_tuple</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span><span class="p">,</span> <span class="n">silent_mode</span><span class="o">=</span><span class="n">silent_mode</span><span class="p">)</span>
                    <span class="n">inf_time</span> <span class="o">=</span> <span class="n">timer</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>

                    <span class="c1"># Phase.VALIDATION_EPOCH_END</span>
                    <span class="c1"># RUN PHASE CALLBACKS</span>
                    <span class="n">valid_metrics_dict</span> <span class="o">=</span> <span class="n">get_metrics_dict</span><span class="p">(</span><span class="n">validation_results_tuple</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">valid_metrics</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_logging_items_names</span><span class="p">)</span>

                    <span class="n">context</span><span class="o">.</span><span class="n">update_context</span><span class="p">(</span><span class="n">metrics_dict</span><span class="o">=</span><span class="n">valid_metrics_dict</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">phase_callback_handler</span><span class="p">(</span><span class="n">Phase</span><span class="o">.</span><span class="n">VALIDATION_EPOCH_END</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ema</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">keep_model</span>

                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">ddp_silent_mode</span><span class="p">:</span>
                    <span class="c1"># SAVING AND LOGGING OCCURS ONLY IN THE MAIN PROCESS (IN CASES THERE ARE SEVERAL PROCESSES - DDP)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_write_to_disk_operations</span><span class="p">(</span><span class="n">train_metrics_tuple</span><span class="p">,</span> <span class="n">validation_results_tuple</span><span class="p">,</span> <span class="n">inf_time</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>

            <span class="c1"># Evaluating the average model and removing snapshot averaging file if training is completed</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">average_best_models</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_validate_final_average_model</span><span class="p">(</span><span class="n">cleanup_snapshots_pkl_file</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">except</span> <span class="ne">KeyboardInterrupt</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">[MODEL TRAINING EXECUTION HAS BEEN INTERRUPTED]... Please wait until SOFT-TERMINATION process &quot;</span>
                <span class="s2">&quot;finishes and saves all of the Model Checkpoints and log files before terminating...&quot;</span>
            <span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;For HARD Termination - Stop the process again&quot;</span><span class="p">)</span>

        <span class="k">finally</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_gpu</span> <span class="o">==</span> <span class="n">MultiGPUMode</span><span class="o">.</span><span class="n">DISTRIBUTED_DATA_PARALLEL</span><span class="p">:</span>
                <span class="c1"># CLEAN UP THE MULTI-GPU PROCESS GROUP WHEN DONE</span>
                <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">destroy_process_group</span><span class="p">()</span>

            <span class="c1"># PHASE.TRAIN_END</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">phase_callback_handler</span><span class="p">(</span><span class="n">Phase</span><span class="o">.</span><span class="n">POST_TRAINING</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">ddp_silent_mode</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">sg_logger</span><span class="o">.</span><span class="n">close</span><span class="p">()</span></div>

    <span class="k">def</span> <span class="nf">_reset_best_metric</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_metric</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">greater_metric_to_watch_is_better</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>

    <span class="k">def</span> <span class="nf">_reset_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;train_metrics&quot;</span><span class="p">,</span> <span class="s2">&quot;valid_metrics&quot;</span><span class="p">,</span> <span class="s2">&quot;test_metrics&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">metric</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">metric</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">metric</span><span class="p">)</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

    <span class="nd">@resolve_param</span><span class="p">(</span><span class="s2">&quot;train_metrics_list&quot;</span><span class="p">,</span> <span class="n">ListFactory</span><span class="p">(</span><span class="n">MetricsFactory</span><span class="p">()))</span>
    <span class="k">def</span> <span class="nf">_set_train_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_metrics_list</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_metrics</span> <span class="o">=</span> <span class="n">MetricCollection</span><span class="p">(</span><span class="n">train_metrics_list</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">metric_name</span><span class="p">,</span> <span class="n">metric</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_metrics</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">metric</span><span class="p">,</span> <span class="s2">&quot;greater_component_is_better&quot;</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">greater_train_metrics_is_better</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">metric</span><span class="o">.</span><span class="n">greater_component_is_better</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">metric</span><span class="p">,</span> <span class="s2">&quot;greater_is_better&quot;</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">greater_train_metrics_is_better</span><span class="p">[</span><span class="n">metric_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">metric</span><span class="o">.</span><span class="n">greater_is_better</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">greater_train_metrics_is_better</span><span class="p">[</span><span class="n">metric_name</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="nd">@resolve_param</span><span class="p">(</span><span class="s2">&quot;valid_metrics_list&quot;</span><span class="p">,</span> <span class="n">ListFactory</span><span class="p">(</span><span class="n">MetricsFactory</span><span class="p">()))</span>
    <span class="k">def</span> <span class="nf">_set_valid_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">valid_metrics_list</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">valid_metrics</span> <span class="o">=</span> <span class="n">MetricCollection</span><span class="p">(</span><span class="n">valid_metrics_list</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">metric_name</span><span class="p">,</span> <span class="n">metric</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">valid_metrics</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">metric</span><span class="p">,</span> <span class="s2">&quot;greater_component_is_better&quot;</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">greater_valid_metrics_is_better</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">metric</span><span class="o">.</span><span class="n">greater_component_is_better</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">metric</span><span class="p">,</span> <span class="s2">&quot;greater_is_better&quot;</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">greater_valid_metrics_is_better</span><span class="p">[</span><span class="n">metric_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">metric</span><span class="o">.</span><span class="n">greater_is_better</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">greater_valid_metrics_is_better</span><span class="p">[</span><span class="n">metric_name</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="nd">@resolve_param</span><span class="p">(</span><span class="s2">&quot;test_metrics_list&quot;</span><span class="p">,</span> <span class="n">ListFactory</span><span class="p">(</span><span class="n">MetricsFactory</span><span class="p">()))</span>
    <span class="k">def</span> <span class="nf">_set_test_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">test_metrics_list</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">test_metrics</span> <span class="o">=</span> <span class="n">MetricCollection</span><span class="p">(</span><span class="n">test_metrics_list</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_initialize_mixed_precision</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mixed_precision_enabled</span><span class="p">:</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="c1"># SCALER IS ALWAYS INITIALIZED BUT IS DISABLED IF MIXED PRECISION WAS NOT SET</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scaler</span> <span class="o">=</span> <span class="n">GradScaler</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="n">mixed_precision_enabled</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">mixed_precision_enabled</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">),</span> <span class="s2">&quot;mixed precision is not available for CPU&quot;</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_gpu</span> <span class="o">==</span> <span class="n">MultiGPUMode</span><span class="o">.</span><span class="n">DATA_PARALLEL</span><span class="p">:</span>
                <span class="c1"># IN DATAPARALLEL MODE WE NEED TO WRAP THE FORWARD FUNCTION OF OUR MODEL SO IT WILL RUN WITH AUTOCAST.</span>
                <span class="c1"># BUT SINCE THE MODULE IS CLONED TO THE DEVICES ON EACH FORWARD CALL OF A DATAPARALLEL MODEL,</span>
                <span class="c1"># WE HAVE TO REGISTER THE WRAPPER BEFORE EVERY FORWARD CALL</span>
                <span class="k">def</span> <span class="nf">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">_</span><span class="p">):</span>
                    <span class="n">module</span><span class="o">.</span><span class="n">forward</span> <span class="o">=</span> <span class="n">MultiGPUModeAutocastWrapper</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">forward</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">register_forward_pre_hook</span><span class="p">(</span><span class="n">hook</span><span class="o">=</span><span class="n">hook</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">:</span>
                <span class="n">scaler_state_dict</span> <span class="o">=</span> <span class="n">core_utils</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">checkpoint</span><span class="p">,</span> <span class="s2">&quot;scaler_state_dict&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">scaler_state_dict</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Mixed Precision - scaler state_dict not found in loaded model. This may case issues &quot;</span> <span class="s2">&quot;with loss scaling&quot;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">scaler</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">scaler_state_dict</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_validate_final_average_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cleanup_snapshots_pkl_file</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Testing the averaged model by loading the last saved average checkpoint and running test.</span>
<span class="sd">        Will be loaded to each of DDP processes</span>
<span class="sd">        :param cleanup_pkl_file: a flag for deleting the 10 best snapshots dictionary</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;RUNNING ADDITIONAL TEST ON THE AVERAGED MODEL...&quot;</span><span class="p">)</span>

        <span class="n">keep_state_dict</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
        <span class="c1"># SETTING STATE DICT TO THE AVERAGE MODEL FOR EVALUATION</span>
        <span class="n">average_model_ckpt_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">checkpoints_dir_path</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">average_model_checkpoint_filename</span><span class="p">)</span>
        <span class="n">local_rank</span> <span class="o">=</span> <span class="n">get_local_rank</span><span class="p">()</span>

        <span class="c1"># WAIT FOR MASTER RANK TO SAVE THE CKPT BEFORE WE TRY TO READ IT.</span>
        <span class="k">with</span> <span class="n">wait_for_the_master</span><span class="p">(</span><span class="n">local_rank</span><span class="p">):</span>
            <span class="n">average_model_sd</span> <span class="o">=</span> <span class="n">read_ckpt_state_dict</span><span class="p">(</span><span class="n">average_model_ckpt_path</span><span class="p">)[</span><span class="s2">&quot;net&quot;</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">average_model_sd</span><span class="p">)</span>
        <span class="c1"># testing the averaged model and save instead of best model if needed</span>
        <span class="n">averaged_model_results_tuple</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_epochs</span><span class="p">)</span>

        <span class="c1"># Reverting the current model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">keep_state_dict</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">ddp_silent_mode</span><span class="p">:</span>
            <span class="c1"># Adding values to sg_logger</span>
            <span class="c1"># looping over last titles which corresponds to validation (and average model) metrics.</span>
            <span class="n">all_titles</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">results_titles</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">averaged_model_results_tuple</span><span class="p">)</span> <span class="p">:]</span>
            <span class="n">result_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">all_titles</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span> <span class="n">averaged_model_results_tuple</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">averaged_model_results_tuple</span><span class="p">))}</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">sg_logger</span><span class="o">.</span><span class="n">add_scalars</span><span class="p">(</span><span class="n">tag_scalar_dict</span><span class="o">=</span><span class="n">result_dict</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_epochs</span><span class="p">)</span>

            <span class="n">average_model_tb_titles</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Averaged Model &quot;</span> <span class="o">+</span> <span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">results_titles</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">averaged_model_results_tuple</span><span class="p">)</span> <span class="p">:]]</span>
            <span class="n">write_struct</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
            <span class="k">for</span> <span class="n">ind</span><span class="p">,</span> <span class="n">title</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">average_model_tb_titles</span><span class="p">):</span>
                <span class="n">write_struct</span> <span class="o">+=</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">: </span><span class="si">%.3f</span><span class="s2">  </span><span class="se">\n</span><span class="s2">  &quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">averaged_model_results_tuple</span><span class="p">[</span><span class="n">ind</span><span class="p">])</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">sg_logger</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">averaged_model_results_tuple</span><span class="p">[</span><span class="n">ind</span><span class="p">],</span> <span class="n">global_step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_epochs</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">sg_logger</span><span class="o">.</span><span class="n">add_text</span><span class="p">(</span><span class="s2">&quot;Averaged_Model_Performance&quot;</span><span class="p">,</span> <span class="n">write_struct</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_epochs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">cleanup_snapshots_pkl_file</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model_weight_averaging</span><span class="o">.</span><span class="n">cleanup</span><span class="p">()</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">get_arch_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">arch_params</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">get_structure</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">structure</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">get_architecture</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">architecture</span>

<div class="viewcode-block" id="Trainer.set_experiment_name"><a class="viewcode-back" href="../../../../super_gradients.training.html#super_gradients.training.Trainer.set_experiment_name">[docs]</a>    <span class="k">def</span> <span class="nf">set_experiment_name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">experiment_name</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">experiment_name</span> <span class="o">=</span> <span class="n">experiment_name</span></div>

    <span class="k">def</span> <span class="nf">_re_build_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arch_params</span><span class="o">=</span><span class="p">{}):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        arch_params : dict</span>
<span class="sd">            Architecture H.P. e.g.: block, num_blocks, num_classes, etc.</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="s2">&quot;num_classes&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">arch_params</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset_interface</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Error&quot;</span><span class="p">,</span> <span class="s2">&quot;Number of classes not defined in arch params and dataset is not defined&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">arch_params</span><span class="p">[</span><span class="s2">&quot;num_classes&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">arch_params</span> <span class="o">=</span> <span class="n">core_utils</span><span class="o">.</span><span class="n">HpmStruct</span><span class="p">(</span><span class="o">**</span><span class="n">arch_params</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">arch_params</span><span class="o">.</span><span class="n">num_classes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_instantiate_net</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">architecture</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">arch_params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_params</span><span class="p">)</span>
        <span class="c1"># save the architecture for neural architecture search</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">,</span> <span class="s2">&quot;structure&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">architecture</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">structure</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_gpu</span> <span class="o">==</span> <span class="n">MultiGPUMode</span><span class="o">.</span><span class="n">DISTRIBUTED_DATA_PARALLEL</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Warning: distributed training is not supported in re_build_model()&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_gpu</span> <span class="k">else</span> <span class="n">core_utils</span><span class="o">.</span><span class="n">WrappedModel</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">get_module</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span>

<div class="viewcode-block" id="Trainer.set_module"><a class="viewcode-back" href="../../../../super_gradients.training.html#super_gradients.training.Trainer.set_module">[docs]</a>    <span class="k">def</span> <span class="nf">set_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">module</span></div>

    <span class="nd">@resolve_param</span><span class="p">(</span><span class="s2">&quot;requested_multi_gpu&quot;</span><span class="p">,</span> <span class="n">TypeFactory</span><span class="p">(</span><span class="n">MultiGPUMode</span><span class="o">.</span><span class="n">dict</span><span class="p">()))</span>
    <span class="k">def</span> <span class="nf">_initialize_device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">requested_device</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">requested_multi_gpu</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">MultiGPUMode</span><span class="p">,</span> <span class="nb">str</span><span class="p">]):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        _initialize_device - Initializes the device for the model - Default is CUDA</span>
<span class="sd">            :param requested_device:        Device to initialize (&#39;cuda&#39; / &#39;cpu&#39;)</span>
<span class="sd">            :param requested_multi_gpu:     Get Multiple GPU</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># SELECT CUDA DEVICE</span>
        <span class="k">if</span> <span class="n">requested_device</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span>  <span class="c1"># TODO - we may want to set the device number as well i.e. &#39;cuda:1&#39;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;CUDA DEVICE NOT FOUND... EXITING&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">require_gpu_setup</span><span class="p">(</span><span class="n">requested_multi_gpu</span><span class="p">):</span>
            <span class="k">raise</span> <span class="n">GPUModeNotSetupError</span><span class="p">()</span>

        <span class="c1"># SELECT CPU DEVICE</span>
        <span class="k">elif</span> <span class="n">requested_device</span> <span class="o">==</span> <span class="s2">&quot;cpu&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">multi_gpu</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># SELECT CUDA DEVICE BY DEFAULT IF AVAILABLE</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>

        <span class="c1"># DEFUALT IS SET TO 1 - IT IS CHANGED IF MULTI-GPU IS USED</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_devices</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="c1"># IN CASE OF MULTIPLE GPUS UPDATE THE LEARNING AND DATA PARAMETERS</span>
        <span class="c1"># FIXME - CREATE A DISCUSSION ON THESE PARAMETERS - WE MIGHT WANT TO CHANGE THE WAY WE USE THE LR AND</span>
        <span class="k">if</span> <span class="n">requested_multi_gpu</span> <span class="o">!=</span> <span class="n">MultiGPUMode</span><span class="o">.</span><span class="n">OFF</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;cuda&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">:</span>
                <span class="c1"># COLLECT THE AVAILABLE GPU AND COUNT THE AVAILABLE GPUS AMOUNT</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">num_devices</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_devices</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">multi_gpu</span> <span class="o">=</span> <span class="n">MultiGPUMode</span><span class="o">.</span><span class="n">OFF</span>
                    <span class="k">if</span> <span class="n">requested_multi_gpu</span> <span class="o">!=</span> <span class="n">MultiGPUMode</span><span class="o">.</span><span class="n">AUTO</span><span class="p">:</span>
                        <span class="c1"># if AUTO mode was set - do not log a warning</span>
                        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">[WARNING] - Tried running on multiple GPU but only a single GPU is available</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">requested_multi_gpu</span> <span class="o">==</span> <span class="n">MultiGPUMode</span><span class="o">.</span><span class="n">AUTO</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">env_helpers</span><span class="o">.</span><span class="n">is_distributed</span><span class="p">():</span>
                            <span class="n">requested_multi_gpu</span> <span class="o">=</span> <span class="n">MultiGPUMode</span><span class="o">.</span><span class="n">DISTRIBUTED_DATA_PARALLEL</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="n">requested_multi_gpu</span> <span class="o">=</span> <span class="n">MultiGPUMode</span><span class="o">.</span><span class="n">DATA_PARALLEL</span>

                    <span class="bp">self</span><span class="o">.</span><span class="n">multi_gpu</span> <span class="o">=</span> <span class="n">requested_multi_gpu</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_gpu</span> <span class="o">==</span> <span class="n">MultiGPUMode</span><span class="o">.</span><span class="n">DISTRIBUTED_DATA_PARALLEL</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_ddp</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># MULTIPLE GPUS CAN BE ACTIVE ONLY IF A GPU IS AVAILABLE</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">multi_gpu</span> <span class="o">=</span> <span class="n">MultiGPUMode</span><span class="o">.</span><span class="n">OFF</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">[WARNING] - Tried running on multiple GPU but none are available =&gt; running on CPU</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_initialize_ddp</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize Distributed Data Parallel</span>

<span class="sd">        Important note: (1) in distributed training it is customary to specify learning rates and batch sizes per GPU.</span>
<span class="sd">        Whatever learning rate and schedule you specify will be applied to the each GPU individually.</span>
<span class="sd">        Since gradients are passed and summed (reduced) from all to all GPUs, the effective batch size is the</span>
<span class="sd">        batch you specify times the number of GPUs. In the literature there are several &quot;best practices&quot; to set</span>
<span class="sd">        learning rates and schedules for large batch sizes.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">local_rank</span> <span class="o">=</span> <span class="n">environment_config</span><span class="o">.</span><span class="n">DDP_LOCAL_RANK</span>
        <span class="k">if</span> <span class="n">local_rank</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">mute_current_process</span><span class="p">()</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Distributed training starting...&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
            <span class="n">backend</span> <span class="o">=</span> <span class="s2">&quot;gloo&quot;</span> <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="s2">&quot;nt&quot;</span> <span class="k">else</span> <span class="s2">&quot;nccl&quot;</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s2">&quot;env://&quot;</span><span class="p">)</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda:</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">local_rank</span>

        <span class="c1"># MAKE ALL HIGHER-RANK GPUS SILENT (DISTRIBUTED MODE)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ddp_silent_mode</span> <span class="o">=</span> <span class="n">local_rank</span> <span class="o">&gt;</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training in distributed mode... with </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">())</span><span class="si">}</span><span class="s2"> GPUs&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_switch_device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_device</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">new_device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># FIXME - we need to resolve flake8&#39;s &#39;function is too complex&#39; for this function</span>
    <span class="k">def</span> <span class="nf">_load_checkpoint_to_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  <span class="c1"># noqa: C901 - too complex</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Copies the source checkpoint to a local folder and loads the checkpoint&#39;s data to the model using the</span>
<span class="sd">         attributes:</span>

<span class="sd">         strict:           See StrictLoad class documentation for details.</span>
<span class="sd">         load_backbone:    loads the provided checkpoint to self.net.backbone instead of self.net</span>
<span class="sd">         source_ckpt_folder_name: The folder where the checkpoint is saved. By default uses the self.experiment_name</span>

<span class="sd">        NOTE: &#39;acc&#39;, &#39;epoch&#39;, &#39;optimizer_state_dict&#39; and the logs are NOT loaded if self.zeroize_prev_train_params</span>
<span class="sd">         is True</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">load_checkpoint</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">external_checkpoint_path</span><span class="p">:</span>
            <span class="c1"># GET LOCAL PATH TO THE CHECKPOINT FILE FIRST</span>
            <span class="n">ckpt_local_path</span> <span class="o">=</span> <span class="n">get_ckpt_local_path</span><span class="p">(</span>
                <span class="n">source_ckpt_folder_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">source_ckpt_folder_name</span><span class="p">,</span>
                <span class="n">experiment_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">experiment_name</span><span class="p">,</span>
                <span class="n">ckpt_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ckpt_name</span><span class="p">,</span>
                <span class="n">external_checkpoint_path</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">external_checkpoint_path</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># LOAD CHECKPOINT TO MODEL</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint</span> <span class="o">=</span> <span class="n">load_checkpoint_to_model</span><span class="p">(</span>
                <span class="n">ckpt_local_path</span><span class="o">=</span><span class="n">ckpt_local_path</span><span class="p">,</span>
                <span class="n">load_backbone</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">load_backbone</span><span class="p">,</span>
                <span class="n">net</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">,</span>
                <span class="n">strict</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">strict_load</span><span class="o">.</span><span class="n">value</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">strict_load</span><span class="p">,</span> <span class="n">StrictLoad</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">strict_load</span><span class="p">,</span>
                <span class="n">load_weights_only</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">load_weights_only</span><span class="p">,</span>
                <span class="n">load_ema_as_net</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">load_ema_as_net</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="s2">&quot;ema_net&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;[WARNING] Main network has been loaded from checkpoint but EMA network exists as &quot;</span>
                    <span class="s2">&quot;well. It &quot;</span>
                    <span class="s2">&quot; will only be loaded during validation when training with ema=True. &quot;</span>
                <span class="p">)</span>

        <span class="c1"># UPDATE TRAINING PARAMS IF THEY EXIST &amp; WE ARE NOT LOADING AN EXTERNAL MODEL&#39;s WEIGHTS</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_metric</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint</span><span class="p">[</span><span class="s2">&quot;acc&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="s2">&quot;acc&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">start_epoch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint</span><span class="p">[</span><span class="s2">&quot;epoch&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="s2">&quot;epoch&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">_prep_for_test</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">test_metrics_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">loss_logging_items_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">test_phase_callbacks</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Run commands that are common to all models&quot;&quot;&quot;</span>
        <span class="c1"># SET THE MODEL IN evaluation STATE</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

        <span class="c1"># IF SPECIFIED IN THE FUNCTION CALL - OVERRIDE THE self ARGUMENTS</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">test_loader</span> <span class="o">=</span> <span class="n">test_loader</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">test_loader</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">loss</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_logging_items_names</span> <span class="o">=</span> <span class="n">loss_logging_items_names</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_logging_items_names</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">phase_callbacks</span> <span class="o">=</span> <span class="n">test_phase_callbacks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">phase_callbacks</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">phase_callbacks</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">phase_callbacks</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">if</span> <span class="n">test_metrics_list</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_set_test_metrics</span><span class="p">(</span><span class="n">test_metrics_list</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_add_metrics_update_callback</span><span class="p">(</span><span class="n">Phase</span><span class="o">.</span><span class="n">TEST_BATCH_END</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">phase_callback_handler</span> <span class="o">=</span> <span class="n">CallbackHandler</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">phase_callbacks</span><span class="p">)</span>

        <span class="c1"># WHEN TESTING WITHOUT A LOSS FUNCTION- CREATE EPOCH HEADERS FOR PRINTS</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss_logging_items_names</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">test_metrics</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Metrics are required to perform test. Pass them through test_metrics_list arg when &quot;</span>
                <span class="s2">&quot;calling test or through training_params when calling train(...)&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">test_loader</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Test dataloader is required to perform test. Make sure to either pass it through &quot;</span> <span class="s2">&quot;test_loader arg.&quot;</span><span class="p">)</span>

        <span class="c1"># RESET METRIC RUNNERS</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reset_metrics</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">test_metrics</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">arch_params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init_arch_params</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_net_to_device</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_add_metrics_update_callback</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">phase</span><span class="p">:</span> <span class="n">Phase</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Adds MetricsUpdateCallback to be fired at phase</span>

<span class="sd">        :param phase: Phase for the metrics callback to be fired at</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">phase_callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">MetricsUpdateCallback</span><span class="p">(</span><span class="n">phase</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">_initialize_sg_logger_objects</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">additional_configs_to_log</span><span class="p">:</span> <span class="n">Dict</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize object that collect, write to disk, monitor and store remotely all training outputs&quot;&quot;&quot;</span>
        <span class="n">sg_logger</span> <span class="o">=</span> <span class="n">core_utils</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="p">,</span> <span class="s2">&quot;sg_logger&quot;</span><span class="p">)</span>

        <span class="c1"># OVERRIDE SOME PARAMETERS TO MAKE SURE THEY MATCH THE TRAINING PARAMETERS</span>
        <span class="n">general_sg_logger_params</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;experiment_name&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">experiment_name</span><span class="p">,</span>
            <span class="s2">&quot;storage_location&quot;</span><span class="p">:</span> <span class="s2">&quot;local&quot;</span><span class="p">,</span>
            <span class="s2">&quot;resumed&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">,</span>
            <span class="s2">&quot;training_params&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="p">,</span>
            <span class="s2">&quot;checkpoints_dir_path&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoints_dir_path</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="k">if</span> <span class="n">sg_logger</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;sg_logger must be defined in training params (see default_training_params)&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sg_logger</span><span class="p">,</span> <span class="n">AbstractSGLogger</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sg_logger</span> <span class="o">=</span> <span class="n">sg_logger</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sg_logger</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">sg_logger_params</span> <span class="o">=</span> <span class="n">core_utils</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="p">,</span> <span class="s2">&quot;sg_logger_params&quot;</span><span class="p">,</span> <span class="p">{})</span>
            <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">SG_LOGGERS</span><span class="p">[</span><span class="n">sg_logger</span><span class="p">],</span> <span class="n">BaseSGLogger</span><span class="p">):</span>
                <span class="n">sg_logger_params</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="n">sg_logger_params</span><span class="p">,</span> <span class="o">**</span><span class="n">general_sg_logger_params</span><span class="p">}</span>
            <span class="k">if</span> <span class="n">sg_logger</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">SG_LOGGERS</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;sg_logger not defined in SG_LOGGERS&quot;</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">sg_logger</span> <span class="o">=</span> <span class="n">SG_LOGGERS</span><span class="p">[</span><span class="n">sg_logger</span><span class="p">](</span><span class="o">**</span><span class="n">sg_logger_params</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;sg_logger can be either an sg_logger name (str) or an instance of AbstractSGLogger&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sg_logger</span><span class="p">,</span> <span class="n">BaseSGLogger</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;WARNING! Using a user-defined sg_logger: files will not be automatically written to disk!</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="s2">&quot;Please make sure the provided sg_logger writes to disk or compose your sg_logger to BaseSGLogger&quot;</span>
            <span class="p">)</span>

        <span class="c1"># IN CASE SG_LOGGER UPDATED THE DIR PATH</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">checkpoints_dir_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sg_logger</span><span class="o">.</span><span class="n">local_dir</span><span class="p">()</span>
        <span class="n">hyper_param_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_hyper_param_config</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">sg_logger</span><span class="o">.</span><span class="n">add_config</span><span class="p">(</span><span class="s2">&quot;hyper_params&quot;</span><span class="p">,</span> <span class="n">hyper_param_config</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">additional_configs_to_log</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">additional_logging_title</span> <span class="ow">in</span> <span class="n">additional_configs_to_log</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">sg_logger</span><span class="o">.</span><span class="n">add_config</span><span class="p">(</span><span class="n">additional_logging_title</span><span class="p">,</span> <span class="n">additional_configs_to_log</span><span class="p">[</span><span class="n">additional_logging_title</span><span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">sg_logger</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_get_hyper_param_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates a training hyper param config for logging.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">additional_log_items</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;initial_LR&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">initial_lr</span><span class="p">,</span>
            <span class="s2">&quot;num_devices&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_devices</span><span class="p">,</span>
            <span class="s2">&quot;multi_gpu&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">multi_gpu</span><span class="p">),</span>
            <span class="s2">&quot;device_type&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_name</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="c1"># ADD INSTALLED PACKAGE LIST + THEIR VERSIONS</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">log_installed_packages</span><span class="p">:</span>
            <span class="n">pkg_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">pkg</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">pkg</span><span class="p">),</span> <span class="n">_get_installed_distributions</span><span class="p">()))</span>
            <span class="n">additional_log_items</span><span class="p">[</span><span class="s2">&quot;installed_packages&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pkg_list</span>
        <span class="n">hyper_param_config</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;arch_params&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">arch_params</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">,</span>
            <span class="s2">&quot;checkpoint_params&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_params</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">,</span>
            <span class="s2">&quot;training_hyperparams&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">,</span>
            <span class="s2">&quot;dataset_params&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset_params</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">,</span>
            <span class="s2">&quot;additional_log_items&quot;</span><span class="p">:</span> <span class="n">additional_log_items</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">hyper_param_config</span>

    <span class="k">def</span> <span class="nf">_write_to_disk_operations</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_metrics</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">,</span> <span class="n">validation_results</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">,</span> <span class="n">inf_time</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">epoch</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">context</span><span class="p">:</span> <span class="n">PhaseContext</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Run the various logging operations, e.g.: log file, Tensorboard, save checkpoint etc.&quot;&quot;&quot;</span>
        <span class="c1"># STORE VALUES IN A TENSORBOARD FILE</span>
        <span class="n">train_results</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">train_metrics</span><span class="p">)</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">validation_results</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="n">inf_time</span><span class="p">]</span>
        <span class="n">all_titles</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">results_titles</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;Inference Time&quot;</span><span class="p">]</span>

        <span class="n">result_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">all_titles</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span> <span class="n">train_results</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_results</span><span class="p">))}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sg_logger</span><span class="o">.</span><span class="n">add_scalars</span><span class="p">(</span><span class="n">tag_scalar_dict</span><span class="o">=</span><span class="n">result_dict</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="n">epoch</span><span class="p">)</span>

        <span class="c1"># SAVE THE CHECKPOINT</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">save_model</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validation_results</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_write_lrs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>
        <span class="n">lrs</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">))]</span>
        <span class="n">lr_titles</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;LR/Param_group_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">))]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="p">[</span><span class="s2">&quot;LR&quot;</span><span class="p">]</span>
        <span class="n">lr_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">lr_titles</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span> <span class="n">lrs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">lrs</span><span class="p">))}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sg_logger</span><span class="o">.</span><span class="n">add_scalars</span><span class="p">(</span><span class="n">tag_scalar_dict</span><span class="o">=</span><span class="n">lr_dict</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="n">epoch</span><span class="p">)</span>

<div class="viewcode-block" id="Trainer.test"><a class="viewcode-back" href="../../../../super_gradients.training.html#super_gradients.training.Trainer.test">[docs]</a>    <span class="k">def</span> <span class="nf">test</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">test_loader</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">loss</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">_Loss</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">silent_mode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">test_metrics_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">loss_logging_items_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">metrics_progress_verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">test_phase_callbacks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">use_ema_net</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluates the model on given dataloader and metrics.</span>
<span class="sd">        :param model: model to perfrom test on. When none is given, will try to use self.net (defalut=None).</span>
<span class="sd">        :param test_loader: dataloader to perform test on.</span>
<span class="sd">        :param test_metrics_list: (list(torchmetrics.Metric)) metrics list for evaluation.</span>
<span class="sd">        :param silent_mode: (bool) controls verbosity</span>
<span class="sd">        :param metrics_progress_verbose: (bool) controls the verbosity of metrics progress (default=False). Slows down the program.</span>
<span class="sd">        :param use_ema_net (bool) whether to perform test on self.ema_model.ema (when self.ema_model.ema exists,</span>
<span class="sd">            otherwise self.net will be tested) (default=True)</span>
<span class="sd">        :return: results tuple (tuple) containing the loss items and metric values.</span>

<span class="sd">        All of the above args will override Trainer&#39;s corresponding attribute when not equal to None. Then evaluation</span>
<span class="sd">         is ran on self.test_loader with self.test_metrics.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">model</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span>

        <span class="c1"># IN CASE TRAINING WAS PERFROMED BEFORE TEST- MAKE SURE TO TEST THE EMA MODEL (UNLESS SPECIFIED OTHERWISE BY</span>
        <span class="c1"># use_ema_net)</span>

        <span class="k">if</span> <span class="n">use_ema_net</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">ema_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">keep_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ema_model</span><span class="o">.</span><span class="n">ema</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_prep_for_test</span><span class="p">(</span>
            <span class="n">test_loader</span><span class="o">=</span><span class="n">test_loader</span><span class="p">,</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
            <span class="n">test_metrics_list</span><span class="o">=</span><span class="n">test_metrics_list</span><span class="p">,</span>
            <span class="n">loss_logging_items_names</span><span class="o">=</span><span class="n">loss_logging_items_names</span><span class="p">,</span>
            <span class="n">test_phase_callbacks</span><span class="o">=</span><span class="n">test_phase_callbacks</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">test_results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
            <span class="n">data_loader</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">test_loader</span><span class="p">,</span>
            <span class="n">metrics</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">test_metrics</span><span class="p">,</span>
            <span class="n">evaluation_type</span><span class="o">=</span><span class="n">EvaluationType</span><span class="o">.</span><span class="n">TEST</span><span class="p">,</span>
            <span class="n">silent_mode</span><span class="o">=</span><span class="n">silent_mode</span><span class="p">,</span>
            <span class="n">metrics_progress_verbose</span><span class="o">=</span><span class="n">metrics_progress_verbose</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># SWITCH BACK BETWEEN NETS SO AN ADDITIONAL TRAINING CAN BE DONE AFTER TEST</span>
        <span class="k">if</span> <span class="n">use_ema_net</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">ema_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">keep_model</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_first_backward</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">return</span> <span class="n">test_results</span></div>

    <span class="k">def</span> <span class="nf">_validate_epoch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">silent_mode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Runs evaluation on self.valid_loader, with self.valid_metrics.</span>

<span class="sd">        :param epoch: (int) epoch idx</span>
<span class="sd">        :param silent_mode: (bool) controls verbosity</span>

<span class="sd">        :return: results tuple (tuple) containing the loss items and metric values.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reset_metrics</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">valid_metrics</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
            <span class="n">data_loader</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">valid_loader</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">valid_metrics</span><span class="p">,</span> <span class="n">evaluation_type</span><span class="o">=</span><span class="n">EvaluationType</span><span class="o">.</span><span class="n">VALIDATION</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span><span class="p">,</span> <span class="n">silent_mode</span><span class="o">=</span><span class="n">silent_mode</span>
        <span class="p">)</span>

<div class="viewcode-block" id="Trainer.evaluate"><a class="viewcode-back" href="../../../../super_gradients.training.html#super_gradients.training.Trainer.evaluate">[docs]</a>    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">data_loader</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">,</span>
        <span class="n">metrics</span><span class="p">:</span> <span class="n">MetricCollection</span><span class="p">,</span>
        <span class="n">evaluation_type</span><span class="p">:</span> <span class="n">EvaluationType</span><span class="p">,</span>
        <span class="n">epoch</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">silent_mode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">metrics_progress_verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluates the model on given dataloader and metrics.</span>

<span class="sd">        :param data_loader: dataloader to perform evaluataion on</span>
<span class="sd">        :param metrics: (MetricCollection) metrics for evaluation</span>
<span class="sd">        :param evaluation_type: (EvaluationType) controls which phase callbacks will be used (for example, on batch end,</span>
<span class="sd">            when evaluation_type=EvaluationType.VALIDATION the Phase.VALIDATION_BATCH_END callbacks will be triggered)</span>
<span class="sd">        :param epoch: (int) epoch idx</span>
<span class="sd">        :param silent_mode: (bool) controls verbosity</span>
<span class="sd">        :param metrics_progress_verbose: (bool) controls the verbosity of metrics progress (default=False).</span>
<span class="sd">            Slows down the program significantly.</span>

<span class="sd">        :return: results tuple (tuple) containing the loss items and metric values.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># THE DISABLE FLAG CONTROLS WHETHER THE PROGRESS BAR IS SILENT OR PRINTS THE LOGS</span>
        <span class="n">progress_bar_data_loader</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">data_loader</span><span class="p">,</span> <span class="n">bar_format</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">{l_bar}{bar:10}{r_bar}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">dynamic_ncols</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">disable</span><span class="o">=</span><span class="n">silent_mode</span><span class="p">)</span>
        <span class="n">loss_avg_meter</span> <span class="o">=</span> <span class="n">core_utils</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">AverageMeter</span><span class="p">()</span>
        <span class="n">logging_values</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">loss_tuple</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">lr_warmup_epochs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span><span class="o">.</span><span class="n">lr_warmup_epochs</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_params</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">PhaseContext</span><span class="p">(</span>
            <span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span><span class="p">,</span>
            <span class="n">metrics_compute_fn</span><span class="o">=</span><span class="n">metrics</span><span class="p">,</span>
            <span class="n">loss_avg_meter</span><span class="o">=</span><span class="n">loss_avg_meter</span><span class="p">,</span>
            <span class="n">criterion</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="n">lr_warmup_epochs</span><span class="o">=</span><span class="n">lr_warmup_epochs</span><span class="p">,</span>
            <span class="n">sg_logger</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sg_logger</span><span class="p">,</span>
            <span class="n">context_methods</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_context_methods</span><span class="p">(</span><span class="n">Phase</span><span class="o">.</span><span class="n">VALIDATION_BATCH_END</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">silent_mode</span><span class="p">:</span>
            <span class="c1"># PRINT TITLES</span>
            <span class="n">pbar_start_msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Validation epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="n">evaluation_type</span> <span class="o">==</span> <span class="n">EvaluationType</span><span class="o">.</span><span class="n">VALIDATION</span> <span class="k">else</span> <span class="s2">&quot;Test&quot;</span>
            <span class="n">progress_bar_data_loader</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="n">pbar_start_msg</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">batch_items</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">progress_bar_data_loader</span><span class="p">):</span>
                <span class="n">batch_items</span> <span class="o">=</span> <span class="n">core_utils</span><span class="o">.</span><span class="n">tensor_container_to_device</span><span class="p">(</span><span class="n">batch_items</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">additional_batch_items</span> <span class="o">=</span> <span class="n">sg_trainer_utils</span><span class="o">.</span><span class="n">unpack_batch_items</span><span class="p">(</span><span class="n">batch_items</span><span class="p">)</span>

                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="c1"># STORE THE loss_items ONLY, THE 1ST RETURNED VALUE IS THE loss FOR BACKPROP DURING TRAINING</span>
                    <span class="n">loss_tuple</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_losses</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">targets</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>

                <span class="n">context</span><span class="o">.</span><span class="n">update_context</span><span class="p">(</span><span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">preds</span><span class="o">=</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">targets</span><span class="p">,</span> <span class="n">loss_log_items</span><span class="o">=</span><span class="n">loss_tuple</span><span class="p">,</span> <span class="o">**</span><span class="n">additional_batch_items</span><span class="p">)</span>

                <span class="c1"># TRIGGER PHASE CALLBACKS CORRESPONDING TO THE EVALUATION TYPE</span>
                <span class="k">if</span> <span class="n">evaluation_type</span> <span class="o">==</span> <span class="n">EvaluationType</span><span class="o">.</span><span class="n">VALIDATION</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">phase_callback_handler</span><span class="p">(</span><span class="n">Phase</span><span class="o">.</span><span class="n">VALIDATION_BATCH_END</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">phase_callback_handler</span><span class="p">(</span><span class="n">Phase</span><span class="o">.</span><span class="n">TEST_BATCH_END</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>

                <span class="c1"># COMPUTE METRICS IF PROGRESS VERBOSITY IS SET</span>
                <span class="k">if</span> <span class="n">metrics_progress_verbose</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">silent_mode</span><span class="p">:</span>
                    <span class="c1"># COMPUTE THE RUNNING USER METRICS AND LOSS RUNNING ITEMS. RESULT TUPLE IS THEIR CONCATENATION.</span>
                    <span class="n">logging_values</span> <span class="o">=</span> <span class="n">get_logging_values</span><span class="p">(</span><span class="n">loss_avg_meter</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">)</span>
                    <span class="n">pbar_message_dict</span> <span class="o">=</span> <span class="n">get_train_loop_description_dict</span><span class="p">(</span><span class="n">logging_values</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_logging_items_names</span><span class="p">)</span>

                    <span class="n">progress_bar_data_loader</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">(</span><span class="o">**</span><span class="n">pbar_message_dict</span><span class="p">)</span>

        <span class="c1"># NEED TO COMPUTE METRICS FOR THE FIRST TIME IF PROGRESS VERBOSITY IS NOT SET</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">metrics_progress_verbose</span><span class="p">:</span>
            <span class="c1"># COMPUTE THE RUNNING USER METRICS AND LOSS RUNNING ITEMS. RESULT TUPLE IS THEIR CONCATENATION.</span>
            <span class="n">logging_values</span> <span class="o">=</span> <span class="n">get_logging_values</span><span class="p">(</span><span class="n">loss_avg_meter</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">)</span>
            <span class="n">pbar_message_dict</span> <span class="o">=</span> <span class="n">get_train_loop_description_dict</span><span class="p">(</span><span class="n">logging_values</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_logging_items_names</span><span class="p">)</span>

            <span class="n">progress_bar_data_loader</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">(</span><span class="o">**</span><span class="n">pbar_message_dict</span><span class="p">)</span>

        <span class="c1"># TODO: SUPPORT PRINTING AP PER CLASS- SINCE THE METRICS ARE NOT HARD CODED ANYMORE (as done in</span>
        <span class="c1">#  calc_batch_prediction_accuracy_per_class in metric_utils.py), THIS IS ONLY RELEVANT WHEN CHOOSING</span>
        <span class="c1">#  DETECTIONMETRICS, WHICH ALREADY RETURN THE METRICS VALUEST HEMSELVES AND NOT THE ITEMS REQUIRED FOR SUCH</span>
        <span class="c1">#  COMPUTATION. ALSO REMOVE THE BELOW LINES BY IMPLEMENTING CRITERION AS A TORCHMETRIC.</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_gpu</span> <span class="o">==</span> <span class="n">MultiGPUMode</span><span class="o">.</span><span class="n">DISTRIBUTED_DATA_PARALLEL</span><span class="p">:</span>
            <span class="n">logging_values</span> <span class="o">=</span> <span class="n">reduce_results_tuple_for_ddp</span><span class="p">(</span><span class="n">logging_values</span><span class="p">,</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="n">pbar_message_dict</span> <span class="o">=</span> <span class="n">get_train_loop_description_dict</span><span class="p">(</span><span class="n">logging_values</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_logging_items_names</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">valid_monitored_values</span> <span class="o">=</span> <span class="n">sg_trainer_utils</span><span class="o">.</span><span class="n">update_monitored_values_dict</span><span class="p">(</span>
            <span class="n">monitored_values_dict</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">valid_monitored_values</span><span class="p">,</span> <span class="n">new_values_dict</span><span class="o">=</span><span class="n">pbar_message_dict</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">silent_mode</span> <span class="ow">and</span> <span class="n">evaluation_type</span> <span class="o">==</span> <span class="n">EvaluationType</span><span class="o">.</span><span class="n">VALIDATION</span><span class="p">:</span>
            <span class="n">progress_bar_data_loader</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;===========================================================&quot;</span><span class="p">)</span>
            <span class="n">sg_trainer_utils</span><span class="o">.</span><span class="n">display_epoch_summary</span><span class="p">(</span>
                <span class="n">epoch</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">epoch</span><span class="p">,</span> <span class="n">n_digits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">train_monitored_values</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_monitored_values</span><span class="p">,</span> <span class="n">valid_monitored_values</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">valid_monitored_values</span>
            <span class="p">)</span>
            <span class="n">progress_bar_data_loader</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;===========================================================&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">logging_values</span></div>

    <span class="k">def</span> <span class="nf">_instantiate_net</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">architecture</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">SgModule</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="n">arch_params</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">checkpoint_params</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Instantiates nn.Module according to architecture and arch_params, and handles pretrained weights and the required</span>
<span class="sd">            module manipulation (i.e head replacement).</span>

<span class="sd">        :param architecture: String, torch.nn.Module or uninstantiated SgModule class describing the netowrks architecture.</span>
<span class="sd">        :param arch_params: Architecture&#39;s parameters passed to networks c&#39;tor.</span>
<span class="sd">        :param checkpoint_params: checkpoint loading related parameters dictionary with &#39;pretrained_weights&#39; key,</span>
<span class="sd">            s.t it&#39;s value is a string describing the dataset of the pretrained weights (for example &quot;imagenent&quot;).</span>

<span class="sd">        :return: instantiated netowrk i.e torch.nn.Module, architecture_class (will be none when architecture is not str)</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">pretrained_weights</span> <span class="o">=</span> <span class="n">core_utils</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="n">checkpoint_params</span><span class="p">,</span> <span class="s2">&quot;pretrained_weights&quot;</span><span class="p">,</span> <span class="n">default_val</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">pretrained_weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">num_classes_new_head</span> <span class="o">=</span> <span class="n">arch_params</span><span class="o">.</span><span class="n">num_classes</span>
            <span class="n">arch_params</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="n">PRETRAINED_NUM_CLASSES</span><span class="p">[</span><span class="n">pretrained_weights</span><span class="p">]</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">architecture</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">architecture_cls</span> <span class="o">=</span> <span class="n">ARCHITECTURES</span><span class="p">[</span><span class="n">architecture</span><span class="p">]</span>
            <span class="n">net</span> <span class="o">=</span> <span class="n">architecture_cls</span><span class="p">(</span><span class="n">arch_params</span><span class="o">=</span><span class="n">arch_params</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">architecture</span><span class="p">,</span> <span class="n">SgModule</span><span class="o">.</span><span class="vm">__class__</span><span class="p">):</span>
            <span class="n">net</span> <span class="o">=</span> <span class="n">architecture</span><span class="p">(</span><span class="n">arch_params</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">net</span> <span class="o">=</span> <span class="n">architecture</span>

        <span class="k">if</span> <span class="n">pretrained_weights</span><span class="p">:</span>
            <span class="n">load_pretrained_weights</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">architecture</span><span class="p">,</span> <span class="n">pretrained_weights</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">num_classes_new_head</span> <span class="o">!=</span> <span class="n">arch_params</span><span class="o">.</span><span class="n">num_classes</span><span class="p">:</span>
                <span class="n">net</span><span class="o">.</span><span class="n">replace_head</span><span class="p">(</span><span class="n">new_num_classes</span><span class="o">=</span><span class="n">num_classes_new_head</span><span class="p">)</span>
                <span class="n">arch_params</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="n">num_classes_new_head</span>

        <span class="k">return</span> <span class="n">net</span>

    <span class="k">def</span> <span class="nf">_instantiate_ema_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">decay</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.9999</span><span class="p">,</span> <span class="n">beta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">15</span><span class="p">,</span> <span class="n">exp_activation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ModelEMA</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Instantiate ema model for standard SgModule.</span>
<span class="sd">        :param decay: the maximum decay value. as the training process advances, the decay will climb towards this value</span>
<span class="sd">                      until the EMA_t+1 = EMA_t * decay + TRAINING_MODEL * (1- decay)</span>
<span class="sd">        :param beta: the exponent coefficient. The higher the beta, the sooner in the training the decay will saturate to</span>
<span class="sd">                     its final value. beta=15 is ~40% of the training process.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">ModelEMA</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">,</span> <span class="n">decay</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">exp_activation</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">get_net</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Getter for network.</span>
<span class="sd">        :return: torch.nn.Module, self.net</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span>

<div class="viewcode-block" id="Trainer.set_net"><a class="viewcode-back" href="../../../../super_gradients.training.html#super_gradients.training.Trainer.set_net">[docs]</a>    <span class="k">def</span> <span class="nf">set_net</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Setter for network.</span>

<span class="sd">        :param net: torch.nn.Module, value to set net</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">net</span></div>

<div class="viewcode-block" id="Trainer.set_ckpt_best_name"><a class="viewcode-back" href="../../../../super_gradients.training.html#super_gradients.training.Trainer.set_ckpt_best_name">[docs]</a>    <span class="k">def</span> <span class="nf">set_ckpt_best_name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ckpt_best_name</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Setter for best checkpoint filename.</span>

<span class="sd">        :param ckpt_best_name: str, value to set ckpt_best_name</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ckpt_best_name</span> <span class="o">=</span> <span class="n">ckpt_best_name</span></div>

<div class="viewcode-block" id="Trainer.set_ema"><a class="viewcode-back" href="../../../../super_gradients.training.html#super_gradients.training.Trainer.set_ema">[docs]</a>    <span class="k">def</span> <span class="nf">set_ema</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val</span><span class="p">:</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Setter for self.ema</span>

<span class="sd">        :param val: bool, value to set ema</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ema</span> <span class="o">=</span> <span class="n">val</span></div>

    <span class="k">def</span> <span class="nf">_get_context_methods</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">phase</span><span class="p">:</span> <span class="n">Phase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ContextSgMethods</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns ContextSgMethods holding the methods that should be accessible through phase callbacks to the user at</span>
<span class="sd">         the specific phase</span>

<span class="sd">        :param phase: Phase, controls what methods should be returned.</span>
<span class="sd">        :return: ContextSgMethods holding methods from self.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">phase</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="n">Phase</span><span class="o">.</span><span class="n">PRE_TRAINING</span><span class="p">,</span>
            <span class="n">Phase</span><span class="o">.</span><span class="n">TRAIN_EPOCH_START</span><span class="p">,</span>
            <span class="n">Phase</span><span class="o">.</span><span class="n">TRAIN_EPOCH_END</span><span class="p">,</span>
            <span class="n">Phase</span><span class="o">.</span><span class="n">VALIDATION_EPOCH_END</span><span class="p">,</span>
            <span class="n">Phase</span><span class="o">.</span><span class="n">VALIDATION_EPOCH_END</span><span class="p">,</span>
            <span class="n">Phase</span><span class="o">.</span><span class="n">POST_TRAINING</span><span class="p">,</span>
            <span class="n">Phase</span><span class="o">.</span><span class="n">VALIDATION_END_BEST_EPOCH</span><span class="p">,</span>
        <span class="p">]:</span>
            <span class="n">context_methods</span> <span class="o">=</span> <span class="n">ContextSgMethods</span><span class="p">(</span>
                <span class="n">get_net</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">get_net</span><span class="p">,</span>
                <span class="n">set_net</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">set_net</span><span class="p">,</span>
                <span class="n">set_ckpt_best_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">set_ckpt_best_name</span><span class="p">,</span>
                <span class="n">reset_best_metric</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_reset_best_metric</span><span class="p">,</span>
                <span class="n">validate_epoch</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_validate_epoch</span><span class="p">,</span>
                <span class="n">set_ema</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">set_ema</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">context_methods</span> <span class="o">=</span> <span class="n">ContextSgMethods</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">context_methods</span>

    <span class="k">def</span> <span class="nf">_init_loss_logging_names</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss_logging_items</span><span class="p">):</span>
        <span class="n">criterion_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="n">component_names</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">,</span> <span class="s2">&quot;component_names&quot;</span><span class="p">):</span>
            <span class="n">component_names</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="o">.</span><span class="n">component_names</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">loss_logging_items</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">component_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;loss_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">loss_logging_items</span><span class="p">))]</span>

        <span class="k">if</span> <span class="n">component_names</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss_logging_items_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">criterion_name</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span> <span class="o">+</span> <span class="n">component_name</span> <span class="k">for</span> <span class="n">component_name</span> <span class="ow">in</span> <span class="n">component_names</span><span class="p">]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">metric_to_watch</span> <span class="ow">in</span> <span class="n">component_names</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">metric_to_watch</span> <span class="o">=</span> <span class="n">criterion_name</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">metric_to_watch</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss_logging_items_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">criterion_name</span><span class="p">]</span></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, SuperGradients team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>
