<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Training package &mdash; SuperGradients 1.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/graphviz.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="What is SuperGradients?" href="user_guide.html" />
    <link rel="prev" title="Common package" href="super_gradients.common.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> SuperGradients
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Welcome To SuperGradients</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="welcome.html#supergradients">SuperGradients</a></li>
</ul>
<p class="caption"><span class="caption-text">Technical Documentation</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="super_gradients.common.html">Common package</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Training package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#module-super_gradients.training">super_gradients.training module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#super-gradients-training-datasets-module">super_gradients.training.datasets module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#super-gradients-training-exceptions-module">super_gradients.training.exceptions module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-super_gradients.training.legacy">super_gradients.training.legacy module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-super_gradients.training.losses">super_gradients.training.losses_models module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-super_gradients.training.metrics">super_gradients.training.metrics module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-super_gradients.training.models">super_gradients.training.models module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-super_gradients.training.sg_model">super_gradients.training.sg_model module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-super_gradients.training.utils">super_gradients.training.utils module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-contents">Module contents</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="user_guide.html">What is SuperGradients?</a></li>
<li class="toctree-l1"><a class="reference internal" href="user_guide.html#introducing-the-supergradients-library">Introducing the SuperGradients library</a></li>
<li class="toctree-l1"><a class="reference internal" href="user_guide.html#installation">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="user_guide.html#integrating-your-training-code-complete-walkthrough">Integrating Your Training Code - Complete Walkthrough</a></li>
<li class="toctree-l1"><a class="reference internal" href="user_guide.html#training-parameters">Training Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="user_guide.html#logs-and-checkpoints">Logs and Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="user_guide.html#dataset-parameters">Dataset Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="user_guide.html#network-architectures">Network Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="user_guide.html#pretrained-models">Pretrained Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="user_guide.html#how-to-reproduce-our-training-recipes">How To Reproduce Our Training Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="user_guide.html#professional-tools-integration">Professional Tools Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="user_guide.html#supergradients-faq">SuperGradients FAQ</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">SuperGradients</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Training package</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/super_gradients.training.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="training-package">
<h1>Training package<a class="headerlink" href="#training-package" title="Permalink to this headline"></a></h1>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
</tbody>
</table>
<section id="module-super_gradients.training">
<span id="super-gradients-training-module"></span><h2>super_gradients.training module<a class="headerlink" href="#module-super_gradients.training" title="Permalink to this headline"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.DataAugmentation">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.</span></span><span class="sig-name descname"><span class="pre">DataAugmentation</span></span><a class="reference internal" href="_modules/super_gradients/training/datasets/data_augmentation.html#DataAugmentation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.DataAugmentation" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.DataAugmentation.to_tensor">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">to_tensor</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/data_augmentation.html#DataAugmentation.to_tensor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.DataAugmentation.to_tensor" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.DataAugmentation.normalize">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">normalize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mean</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">std</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/data_augmentation.html#DataAugmentation.normalize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.DataAugmentation.normalize" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.DataAugmentation.cutout">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">cutout</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mask_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cutout_inside</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask_color</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(0,</span> <span class="pre">0,</span> <span class="pre">0)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/data_augmentation.html#DataAugmentation.cutout"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.DataAugmentation.cutout" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.DetectionDataSet">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.</span></span><span class="sig-name descname"><span class="pre">DetectionDataSet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">root</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">list_file</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">img_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">416</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">augment</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset_hyper_params</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">dict</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_labels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_images</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_loading_method</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'default'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">collate_fn</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_extension</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'.txt'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels_offset</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_inclusion_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">all_classes_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/detection_datasets/detection_dataset.html#DetectionDataSet"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.DetectionDataSet" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Generic</span></code>[<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.utils.data.dataset.T_co</span></code>]</p>
<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.DetectionDataSet.mixup">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">mixup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">im</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">im2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels2</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/detection_datasets/detection_dataset.html#DetectionDataSet.mixup"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.DetectionDataSet.mixup" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.DetectionDataSet.sample_post_process">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">sample_post_process</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/detection_datasets/detection_dataset.html#DetectionDataSet.sample_post_process"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.DetectionDataSet.sample_post_process" title="Permalink to this definition"></a></dt>
<dd><dl class="simple">
<dt>sample_post_process - Normalizes and orders the image to be 3 x img_size x img_size</dt><dd><dl class="field-list simple">
<dt class="field-odd">param image</dt>
<dd class="field-odd"><p></p></dd>
<dt class="field-even">return</dt>
<dd class="field-even"><p></p></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.DetectionDataSet.sample_loader">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">sample_loader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_path</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/detection_datasets/detection_dataset.html#DetectionDataSet.sample_loader"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.DetectionDataSet.sample_loader" title="Permalink to this definition"></a></dt>
<dd><dl class="simple">
<dt>sample_loader - Loads a coco dataset image from path</dt><dd><dl class="field-list simple">
<dt class="field-odd">param sample_path</dt>
<dd class="field-odd"><p></p></dd>
<dt class="field-even">return</dt>
<dd class="field-even"><p></p></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.DetectionDataSet.sample_transform">
<span class="sig-name descname"><span class="pre">sample_transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/detection_datasets/detection_dataset.html#DetectionDataSet.sample_transform"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.DetectionDataSet.sample_transform" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>image</strong> – </p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.DetectionDataSet.target_loader">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">target_loader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target_path</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_inclusion_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">all_classes_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/detection_datasets/detection_dataset.html#DetectionDataSet.target_loader"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.DetectionDataSet.target_loader" title="Permalink to this definition"></a></dt>
<dd><dl class="simple">
<dt>coco_target_loader</dt><dd><p>&#64;param target_path: str, path to target.
&#64;param all_classes_list: list(str) containing all the class names or None when subclassing is disabled.
&#64;param class_inclusion_list: list(str) containing the subclass names or None when subclassing is disabled.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.DetectionDataSet.target_transform">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">target_transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ratio</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">w</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/detection_datasets/detection_dataset.html#DetectionDataSet.target_transform"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.DetectionDataSet.target_transform" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>target</strong> – </p></li>
<li><p><strong>ratio</strong> – </p></li>
<li><p><strong>w</strong> – </p></li>
<li><p><strong>h</strong> – </p></li>
<li><p><strong>pad</strong> – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.DetectionDataSet.exif_size">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">exif_size</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">img</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/detection_datasets/detection_dataset.html#DetectionDataSet.exif_size"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.DetectionDataSet.exif_size" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>img</strong> – </p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.DetectionDataSet.augment_hsv">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">augment_hsv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">img</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hgain</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sgain</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vgain</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/detection_datasets/detection_dataset.html#DetectionDataSet.augment_hsv"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.DetectionDataSet.augment_hsv" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>img</strong> – <dl class="field-list simple">
<dt class="field-odd">param hgain</dt>
<dd class="field-odd"><p></p></dd>
<dt class="field-even">param sgain</dt>
<dd class="field-even"><p></p></dd>
<dt class="field-odd">param vgain</dt>
<dd class="field-odd"><p></p></dd>
</dl>
</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.DetectionDataSet.letterbox">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">letterbox</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">img</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">new_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(416,</span> <span class="pre">416)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">color</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(128,</span> <span class="pre">128,</span> <span class="pre">128)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">auto</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scaleFill</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scaleup</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">interp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">tuple</span><a class="reference internal" href="_modules/super_gradients/training/datasets/detection_datasets/detection_dataset.html#DetectionDataSet.letterbox"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.DetectionDataSet.letterbox" title="Permalink to this definition"></a></dt>
<dd><p>letterbox - Resizes image to a 32-pixel-multiple rectangle
:param img:
:param new_shape:
:param color:
:param auto:
:param scaleFill:
:param scaleup:
:param interp:
:return:</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.DetectionDataSet.random_perspective">
<span class="sig-name descname"><span class="pre">random_perspective</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">img</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">targets</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">degrees</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">translate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shear</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">border</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">perspective</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/detection_datasets/detection_dataset.html#DetectionDataSet.random_perspective"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.DetectionDataSet.random_perspective" title="Permalink to this definition"></a></dt>
<dd><p>random images and labels using a perspective transform</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.DetectionDataSet.box_candidates">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">box_candidates</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">box1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">box2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wh_thr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ar_thr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">area_thr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/detection_datasets/detection_dataset.html#DetectionDataSet.box_candidates"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.DetectionDataSet.box_candidates" title="Permalink to this definition"></a></dt>
<dd><dl class="simple">
<dt>compute candidate boxes</dt><dd><dl class="field-list simple">
<dt class="field-odd">param box1</dt>
<dd class="field-odd"><p>before augment</p>
</dd>
<dt class="field-even">param box2</dt>
<dd class="field-even"><p>after augment</p>
</dd>
<dt class="field-odd">param wh_thr</dt>
<dd class="field-odd"><p>wh_thr (pixels)</p>
</dd>
<dt class="field-even">param ar_thr</dt>
<dd class="field-even"><p>aspect_ratio_thr</p>
</dd>
<dt class="field-odd">param area_thr</dt>
<dd class="field-odd"><p>area_ratio</p>
</dd>
</dl>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.DetectionDataSet.load_mosaic">
<span class="sig-name descname"><span class="pre">load_mosaic</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">index</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/detection_datasets/detection_dataset.html#DetectionDataSet.load_mosaic"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.DetectionDataSet.load_mosaic" title="Permalink to this definition"></a></dt>
<dd><dl class="simple">
<dt>load_mosaic - Load images in mosaic format to improve noise handling while training</dt><dd><dl class="field-list simple">
<dt class="field-odd">param index</dt>
<dd class="field-odd"><p></p></dd>
<dt class="field-even">return</dt>
<dd class="field-even"><p></p></dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.TestDatasetInterface">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.</span></span><span class="sig-name descname"><span class="pre">TestDatasetInterface</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trainset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/dataset_interfaces/dataset_interface.html#TestDatasetInterface"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.TestDatasetInterface" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="super_gradients.training.datasets.dataset_interfaces.html#super_gradients.training.datasets.dataset_interfaces.dataset_interface.DatasetInterface" title="super_gradients.training.datasets.dataset_interfaces.dataset_interface.DatasetInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">super_gradients.training.datasets.dataset_interfaces.dataset_interface.DatasetInterface</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.TestDatasetInterface.get_data_loaders">
<span class="sig-name descname"><span class="pre">get_data_loaders</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size_factor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_workers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distributed_sampler</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/dataset_interfaces/dataset_interface.html#TestDatasetInterface.get_data_loaders"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.TestDatasetInterface.get_data_loaders" title="Permalink to this definition"></a></dt>
<dd><p>Get self.train_loader, self.test_loader, self.classes.</p>
<p>If the data loaders haven’t been initialized yet, build them first.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>kwargs</strong> – kwargs are passed to build_data_loaders.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.SgModel">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.</span></span><span class="sig-name descname"><span class="pre">SgModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">experiment_name:</span> <span class="pre">str</span></em>, <em class="sig-param"><span class="pre">device:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">None</span></em>, <em class="sig-param"><span class="pre">multi_gpu:</span> <span class="pre">Union[super_gradients.training.sg_model.sg_model.MultiGPUMode</span></em>, <em class="sig-param"><span class="pre">str]</span> <span class="pre">=</span> <span class="pre">&lt;MultiGPUMode.OFF:</span> <span class="pre">'Off'&gt;</span></em>, <em class="sig-param"><span class="pre">model_checkpoints_location:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'local'</span></em>, <em class="sig-param"><span class="pre">overwrite_local_checkpoint:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">True</span></em>, <em class="sig-param"><span class="pre">ckpt_name:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'ckpt_latest.pth'</span></em>, <em class="sig-param"><span class="pre">post_prediction_callback:</span> <span class="pre">Optional[super_gradients.training.utils.detection_utils.DetectionPostPredictionCallback]</span> <span class="pre">=</span> <span class="pre">None</span></em>, <em class="sig-param"><span class="pre">ckpt_root_dir=None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.SgModel" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>SuperGradient Model - Base Class for Sg Models</p>
<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.SgModel.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">max_epochs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_epoch</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.train"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.SgModel.train" title="Permalink to this definition"></a></dt>
<dd><p>the main function used for the training, h.p. updating, logging etc.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.SgModel.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">idx</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.SgModel.predict" title="Permalink to this definition"></a></dt>
<dd><p>returns the predictions and label of the current inputs</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">test(epoch</span> <span class="pre">:</span> <span class="pre">int,</span> <span class="pre">idx</span> <span class="pre">:</span> <span class="pre">int,</span> <span class="pre">save</span> <span class="pre">:</span> <span class="pre">bool):</span></span></dt>
<dd><p>returns the test loss, accuracy and runtime</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.SgModel.connect_dataset_interface">
<span class="sig-name descname"><span class="pre">connect_dataset_interface</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset_interface</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference internal" href="super_gradients.training.datasets.dataset_interfaces.html#super_gradients.training.datasets.dataset_interfaces.dataset_interface.DatasetInterface" title="super_gradients.training.datasets.dataset_interfaces.dataset_interface.DatasetInterface"><span class="pre">super_gradients.training.datasets.dataset_interfaces.dataset_interface.DatasetInterface</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">data_loader_num_workers</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">8</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.connect_dataset_interface"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.SgModel.connect_dataset_interface" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset_interface</strong> – DatasetInterface object</p></li>
<li><p><strong>data_loader_num_workers</strong> – The number of threads to initialize the Data Loaders with
The dataset to be connected</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.SgModel.build_model">
<span class="sig-name descname"><span class="pre">build_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">architecture</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">arch_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.build_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.SgModel.build_model" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>architecture</strong> – Defines the network’s architecture from models/ALL_ARCHITECTURES</p></li>
<li><p><strong>arch_params</strong> – Architecture H.P. e.g.: block, num_blocks, num_classes, etc.</p></li>
<li><p><strong>checkpoint_params</strong> – <p>Dictionary like object with the following key:values:</p>
<p>load_checkpoint:            Load a pre-trained checkpoint
strict_load:                See StrictLoad class documentation for details.
source_ckpt_folder_name:    folder name to load the checkpoint from (self.experiment_name if none is given)
load_weights_only:          loads only the weight from the checkpoint and zeroize the training params
load_backbone:              loads the provided checkpoint to self.net.backbone instead of self.net
external_checkpoint_path:   The path to the external checkpoint to be loaded. Can be absolute or relative</p>
<blockquote>
<div><p>(ie: path/to/checkpoint.pth). If provided, will automatically attempt to
load the checkpoint even if the load_checkpoint flag is not provided.</p>
</div></blockquote>
</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.SgModel.backward_step">
<span class="sig-name descname"><span class="pre">backward_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">context</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference internal" href="super_gradients.training.utils.html#super_gradients.training.utils.callbacks.PhaseContext" title="super_gradients.training.utils.callbacks.PhaseContext"><span class="pre">super_gradients.training.utils.callbacks.PhaseContext</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.backward_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.SgModel.backward_step" title="Permalink to this definition"></a></dt>
<dd><p>Run backprop on the loss and perform a step
:param loss: The value computed by the loss function
:param optimizer: An object that can perform a gradient step and zeroize model gradient
:param epoch: number of epoch the training is on
:param batch_idx: number of iteration inside the current epoch
:param context: current phase context
:return:</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.SgModel.save_checkpoint">
<span class="sig-name descname"><span class="pre">save_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_results_tuple</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">tuple</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">context</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="super_gradients.training.utils.html#super_gradients.training.utils.callbacks.PhaseContext" title="super_gradients.training.utils.callbacks.PhaseContext"><span class="pre">super_gradients.training.utils.callbacks.PhaseContext</span></a><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.save_checkpoint"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.SgModel.save_checkpoint" title="Permalink to this definition"></a></dt>
<dd><p>Save the current state dict as latest (always), best (if metric was improved), epoch# (if determined in training
params)</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id0">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">training_params</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">dict</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">{}</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.train"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#id0" title="Permalink to this definition"></a></dt>
<dd><p>train - Trains the Model</p>
<dl>
<dt>IMPORTANT NOTE: Additional batch parameters can be added as a third item (optional) if a tuple is returned by</dt><dd><p>the data loaders, as dictionary. The phase context will hold the additional items, under an attribute with
the same name as the key in this dictionary. Then such items can be accessed through phase callbacks.</p>
<blockquote>
<div><dl class="field-list">
<dt class="field-odd">param training_params</dt>
<dd class="field-odd"><ul>
<li><p><cite>max_epochs</cite> : int</p>
<blockquote>
<div><p>Number of epochs to run training.</p>
</div></blockquote>
</li>
<li><p><cite>lr_updates</cite> : list(int)</p>
<blockquote>
<div><p>List of fixed epoch numbers to perform learning rate updates when <cite>lr_mode=’step’</cite>.</p>
</div></blockquote>
</li>
<li><p><cite>lr_decay_factor</cite> : float</p>
<blockquote>
<div><p>Decay factor to apply to the learning rate at each update when <cite>lr_mode=’step’</cite>.</p>
</div></blockquote>
</li>
<li><p><cite>lr_mode</cite> : str</p>
<blockquote>
<div><p>Learning rate scheduling policy, one of [‘step’,’poly’,’cosine’,’function’]. ‘step’ refers to
constant updates at epoch numbers passed through <cite>lr_updates</cite>. ‘cosine’ refers to Cosine Anealing
policy as mentioned in <a class="reference external" href="https://arxiv.org/abs/1608.03983">https://arxiv.org/abs/1608.03983</a>. ‘poly’ refers to polynomial decrease i.e
in each epoch iteration <cite>self.lr = self.initial_lr * pow((1.0 - (current_iter / max_iter)),
0.9)</cite> ‘function’ refers to user defined learning rate scheduling function, that is passed through
<cite>lr_schedule_function</cite>.</p>
</div></blockquote>
</li>
<li><p><cite>lr_schedule_function</cite> : Union[callable,None]</p>
<blockquote>
<div><p>Learning rate scheduling function to be used when <cite>lr_mode</cite> is ‘function’.</p>
</div></blockquote>
</li>
<li><p><cite>lr_warmup_epochs</cite> : int (default=0)</p>
<blockquote>
<div><p>Number of epochs for learning rate warm up - see <a class="reference external" href="https://arxiv.org/pdf/1706.02677.pdf">https://arxiv.org/pdf/1706.02677.pdf</a> (Section 2.2).</p>
</div></blockquote>
</li>
<li><dl class="simple">
<dt><cite>cosine_final_lr_ratio</cite><span class="classifier">float (default=0.01)</span></dt><dd><dl class="simple">
<dt>Final learning rate ratio (only relevant when <a href="#id1"><span class="problematic" id="id2">`</span></a>lr_mode`=’cosine’). The cosine starts from initial_lr and reaches</dt><dd><p>initial_lr * cosine_final_lr_ratio in last epoch</p>
</dd>
</dl>
</dd>
</dl>
</li>
<li><p><cite>inital_lr</cite> : float</p>
<blockquote>
<div><p>Initial learning rate.</p>
</div></blockquote>
</li>
<li><p><cite>loss</cite> : Union[nn.module, str]</p>
<blockquote>
<div><p>Loss function for training.
One of SuperGradient’s built in options:</p>
<blockquote>
<div><p>“cross_entropy”: LabelSmoothingCrossEntropyLoss,
“mse”: MSELoss,
“r_squared_loss”: RSquaredLoss,
“detection_loss”: YoLoV3DetectionLoss,
“shelfnet_ohem_loss”: ShelfNetOHEMLoss,
“shelfnet_se_loss”: ShelfNetSemanticEncodingLoss,
“yolo_v5_loss”: YoLoV5DetectionLoss,
“ssd_loss”: SSDLoss,</p>
</div></blockquote>
<p>or user defined nn.module loss function.</p>
<p>IMPORTANT: forward(…) should return a (loss, loss_items) tuple where loss is the tensor used
for backprop (i.e what your original loss function returns), and loss_items should be a tensor of
shape (n_items), of values computed during the forward pass which we desire to log over the
entire epoch. For example- the loss itself should always be logged. Another example is a scenario
where the computed loss is the sum of a few components we would like to log- these entries in
loss_items).</p>
<p>When training, set the loss_logging_items_names parameter in train_params to be a list of
strings, of length n_items who’s ith element is the name of the ith entry in loss_items. Then
each item will be logged, rendered on tensorboard and “watched” (i.e saving model checkpoints
according to it).</p>
<p>Since running logs will save the loss_items in some internal state, it is recommended that
loss_items are detached from their computational graph for memory efficiency.</p>
</div></blockquote>
</li>
<li><p><cite>optimizer</cite> : Union[str, torch.optim.Optimizer]</p>
<blockquote>
<div><p>Optimization algorithm. One of [‘Adam’,’SGD’,’RMSProp’] corresponding to the torch.optim
optimzers implementations, or any object that implements torch.optim.Optimizer.</p>
</div></blockquote>
</li>
<li><p><cite>criterion_params</cite> : dict</p>
<blockquote>
<div><p>Loss function parameters.</p>
</div></blockquote>
</li>
<li><dl>
<dt><cite>optimizer_params</cite><span class="classifier">dict</span></dt><dd><p>When <cite>optimizer</cite> is one of [‘Adam’,’SGD’,’RMSProp’], it will be initialized with optimizer_params.</p>
<p>(see <a class="reference external" href="https://pytorch.org/docs/stable/optim.html">https://pytorch.org/docs/stable/optim.html</a> for the full list of
parameters for each optimizer).</p>
</dd>
</dl>
</li>
<li><p><cite>train_metrics_list</cite> : list(torchmetrics.Metric)</p>
<blockquote>
<div><p>Metrics to log during training. For more information on torchmetrics see
<a class="reference external" href="https://torchmetrics.rtfd.io/en/latest/">https://torchmetrics.rtfd.io/en/latest/</a>.</p>
</div></blockquote>
</li>
<li><p><cite>valid_metrics_list</cite> : list(torchmetrics.Metric)</p>
<blockquote>
<div><p>Metrics to log during validation/testing. For more information on torchmetrics see
<a class="reference external" href="https://torchmetrics.rtfd.io/en/latest/">https://torchmetrics.rtfd.io/en/latest/</a>.</p>
</div></blockquote>
</li>
<li><p><cite>loss_logging_items_names</cite> : list(str)</p>
<blockquote>
<div><p>The list of names/titles for the outputs returned from the loss functions forward pass (reminder-
the loss function should return the tuple (loss, loss_items)). These names will be used for
logging their values.</p>
</div></blockquote>
</li>
<li><p><cite>metric_to_watch</cite> : str (default=”Accuracy”)</p>
<blockquote>
<div><p>will be the metric which the model checkpoint will be saved according to, and can be set to any
of the following:</p>
<blockquote>
<div><p>a metric name (str) of one of the metric objects from the valid_metrics_list</p>
<p>a “metric_name” if some metric in valid_metrics_list has an attribute component_names which
is a list referring to the names of each entry in the output metric (torch tensor of size n)</p>
<p>one of “loss_logging_items_names” i.e which will correspond to an item returned during the
loss function’s forward pass.</p>
</div></blockquote>
<p>At the end of each epoch, if a new best metric_to_watch value is achieved, the models checkpoint
is saved in YOUR_PYTHON_PATH/checkpoints/ckpt_best.pth</p>
</div></blockquote>
</li>
<li><p><cite>greater_metric_to_watch_is_better</cite> : bool</p>
<blockquote>
<div><dl class="simple">
<dt>When choosing a model’s checkpoint to be saved, the best achieved model is the one that maximizes the</dt><dd><p>metric_to_watch when this parameter is set to True, and a one that minimizes it otherwise.</p>
</dd>
</dl>
</div></blockquote>
</li>
<li><p><cite>ema</cite> : bool (default=False)</p>
<blockquote>
<div><p>Whether to use Model Exponential Moving Average (see
<a class="reference external" href="https://github.com/rwightman/pytorch-image-models">https://github.com/rwightman/pytorch-image-models</a> ema implementation)</p>
</div></blockquote>
</li>
<li><p><cite>batch_accumulate</cite> : int (default=1)</p>
<blockquote>
<div><p>Number of batches to accumulate before every backward pass.</p>
</div></blockquote>
</li>
<li><p><cite>ema_params</cite> : dict</p>
<blockquote>
<div><p>Parameters for the ema model.</p>
</div></blockquote>
</li>
<li><p><cite>zero_weight_decay_on_bias_and_bn</cite> : bool (default=False)</p>
<blockquote>
<div><p>Whether to apply weight decay on batch normalization parameters or not (ignored when the passed
optimizer has already been initialized).</p>
</div></blockquote>
</li>
<li><p><cite>load_opt_params</cite> : bool (default=True)</p>
<blockquote>
<div><p>Whether to load the optimizers parameters as well when loading a model’s checkpoint.</p>
</div></blockquote>
</li>
<li><p><cite>run_validation_freq</cite> : int (default=1)</p>
<blockquote>
<div><dl class="simple">
<dt>The frequency in which validation is performed during training (i.e the validation is ran every</dt><dd><p><cite>run_validation_freq</cite> epochs.</p>
</dd>
</dl>
</div></blockquote>
</li>
<li><p><cite>save_model</cite> : bool (default=True)</p>
<blockquote>
<div><p>Whether to save the model checkpoints.</p>
</div></blockquote>
</li>
<li><p><cite>silent_mode</cite> : bool</p>
<blockquote>
<div><p>Silents the print outs.</p>
</div></blockquote>
</li>
<li><p><cite>mixed_precision</cite> : bool</p>
<blockquote>
<div><p>Whether to use mixed precision or not.</p>
</div></blockquote>
</li>
<li><p><cite>save_ckpt_epoch_list</cite> : list(int) (default=[])</p>
<blockquote>
<div><p>List of fixed epoch indices the user wishes to save checkpoints in.</p>
</div></blockquote>
</li>
<li><p><cite>average_best_models</cite> : bool (default=False)</p>
<blockquote>
<div><p>If set, a snapshot dictionary file and the average model will be saved / updated at every epoch
and evaluated only when training is completed. The snapshot file will only be deleted upon
completing the training. The snapshot dict will be managed on cpu.</p>
</div></blockquote>
</li>
<li><p><cite>precise_bn</cite> : bool (default=False)</p>
<blockquote>
<div><p>Whether to use precise_bn calculation during the training.</p>
</div></blockquote>
</li>
<li><p><cite>precise_bn_batch_size</cite> : int (default=None)</p>
<blockquote>
<div><p>The effective batch size we want to calculate the batchnorm on. For example, if we are training a model
on 8 gpus, with a batch of 128 on each gpu, a good rule of thumb would be to give it 8192
(ie: effective_batch_size * num_gpus = batch_per_gpu * num_gpus * num_gpus).
If precise_bn_batch_size is not provided in the training_params, the latter heuristic will be taken.</p>
</div></blockquote>
</li>
<li><p><cite>seed</cite> : int (default=42)</p>
<blockquote>
<div><p>Random seed to be set for torch, numpy, and random. When using DDP each process will have it’s seed
set to seed + rank.</p>
</div></blockquote>
</li>
<li><p><cite>log_installed_packages</cite> : bool (default=False)</p>
<blockquote>
<div><dl class="simple">
<dt>When set, the list of all installed packages (and their versions) will be written to the tensorboard</dt><dd><p>and logfile (useful when trying to reproduce results).</p>
</dd>
</dl>
</div></blockquote>
</li>
<li><p><cite>dataset_statistics</cite> : bool (default=False)</p>
<blockquote>
<div><p>Enable a statistic analysis of the dataset. If set to True the dataset will be analyzed and a report
will be added to the tensorboard along with some sample images from the dataset. Currently only
detection datasets are supported for analysis.</p>
</div></blockquote>
</li>
<li><p><cite>save_full_train_log</cite> : bool (default=False)</p>
<blockquote>
<div><dl class="simple">
<dt>When set, a full log (of all super_gradients modules, including uncaught exceptions from any other</dt><dd><p>module) of the training will be saved in the checkpoint directory under full_train_log.log</p>
</dd>
</dl>
</div></blockquote>
</li>
<li><p><cite>sg_logger</cite> : Union[AbstractSGLogger, str] (defauls=base_sg_logger)</p>
<blockquote>
<div><p>Define the SGLogger object for this training process. The SGLogger handles all disk writes, logs, TensorBoard, remote logging
and remote storage. By overriding the default base_sg_logger, you can change the storage location, support external monitoring and logging
or support remote storage.</p>
</div></blockquote>
</li>
<li><p><cite>sg_logger_params</cite> : dict</p>
<p>SGLogger parameters</p>
</li>
<li><p><cite>clip_grad_norm</cite> : float</p>
<p>Defines a maximal L2 norm of the gradients. Values which exceed the given value will be clipped</p>
</li>
</ul>
</dd>
</dl>
</div></blockquote>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id3">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">targets</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">half</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">move_outputs_to_cpu</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#id3" title="Permalink to this definition"></a></dt>
<dd><p>A fast predictor for a batch of inputs
:param inputs: torch.tensor or numpy.array</p>
<blockquote>
<div><p>a batch of inputs</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>targets</strong> – torch.tensor()
corresponding labels - if non are given - accuracy will not be computed</p></li>
<li><p><strong>verbose</strong> – bool
print the results to screen</p></li>
<li><p><strong>normalize</strong> – bool
If true, normalizes the tensor according to the dataloader’s normalization values</p></li>
<li><p><strong>half</strong> – Performs half precision evaluation</p></li>
<li><p><strong>move_outputs_to_cpu</strong> – Moves the results from the GPU to the CPU</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>outputs, acc, net_time, gross_time
networks predictions, accuracy calculation, forward pass net time, function gross time</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.SgModel.compute_model_runtime">
<span class="sig-name descname"><span class="pre">compute_model_runtime</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dims</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">tuple</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_sizes</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">tuple</span><span class="p"><span class="pre">,</span> </span><span class="pre">list</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">(1,</span> <span class="pre">8,</span> <span class="pre">16,</span> <span class="pre">32,</span> <span class="pre">64)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.compute_model_runtime"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.SgModel.compute_model_runtime" title="Permalink to this definition"></a></dt>
<dd><p>Compute the “atomic” inference time and throughput.
Atomic refers to calculating the forward pass independently, discarding effects such as data augmentation,
data upload to device, multi-gpu distribution etc.
:param input_dims: tuple</p>
<blockquote>
<div><p>shape of a basic input to the network (without the first index) e.g. (3, 224, 224)
if None uses an input from the test loader</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch_sizes</strong> – int or list
Batch sizes for latency calculation</p></li>
<li><p><strong>verbose</strong> – bool
Prints results to screen</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>log: dict
Latency and throughput for each tested batch size</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.SgModel.get_arch_params">
<span class="sig-name descname"><span class="pre">get_arch_params</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.get_arch_params"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.SgModel.get_arch_params" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.SgModel.get_structure">
<span class="sig-name descname"><span class="pre">get_structure</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.get_structure"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.SgModel.get_structure" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.SgModel.get_architecture">
<span class="sig-name descname"><span class="pre">get_architecture</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.get_architecture"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.SgModel.get_architecture" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.SgModel.set_experiment_name">
<span class="sig-name descname"><span class="pre">set_experiment_name</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">experiment_name</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.set_experiment_name"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.SgModel.set_experiment_name" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.SgModel.re_build_model">
<span class="sig-name descname"><span class="pre">re_build_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">arch_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.re_build_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.SgModel.re_build_model" title="Permalink to this definition"></a></dt>
<dd><dl class="simple">
<dt>arch_params<span class="classifier">dict</span></dt><dd><p>Architecture H.P. e.g.: block, num_blocks, num_classes, etc.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.SgModel.update_architecture">
<span class="sig-name descname"><span class="pre">update_architecture</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">structure</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.update_architecture"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.SgModel.update_architecture" title="Permalink to this definition"></a></dt>
<dd><dl class="simple">
<dt>architecture<span class="classifier">str</span></dt><dd><p>Defines the network’s architecture according to the options in models/all_architectures</p>
</dd>
<dt>load_checkpoint<span class="classifier">bool</span></dt><dd><p>Loads a checkpoint according to experiment_name</p>
</dd>
<dt>arch_params<span class="classifier">dict</span></dt><dd><p>Architecture H.P. e.g.: block, num_blocks, num_classes, etc.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.SgModel.get_module">
<span class="sig-name descname"><span class="pre">get_module</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.get_module"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.SgModel.get_module" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.SgModel.set_module">
<span class="sig-name descname"><span class="pre">set_module</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.set_module"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.SgModel.set_module" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.SgModel.test">
<span class="sig-name descname"><span class="pre">test</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">test_loader</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.utils.data.dataloader.DataLoader</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.loss._Loss</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">silent_mode</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_metrics_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_logging_items_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metrics_progress_verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_phase_callbacks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_ema_net</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">tuple</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.test"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.SgModel.test" title="Permalink to this definition"></a></dt>
<dd><p>Evaluates the model on given dataloader and metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>test_loader</strong> – dataloader to perform test on.</p></li>
<li><p><strong>test_metrics_list</strong> – (list(torchmetrics.Metric)) metrics list for evaluation.</p></li>
<li><p><strong>silent_mode</strong> – (bool) controls verbosity</p></li>
<li><p><strong>metrics_progress_verbose</strong> – (bool) controls the verbosity of metrics progress (default=False). Slows down the program.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>:param use_ema_net (bool) whether to perform test on self.ema_model.ema (when self.ema_model.ema exists,</dt><dd><p>otherwise self.net will be tested) (default=True)</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>results tuple (tuple) containing the loss items and metric values.</p>
</dd>
</dl>
<dl class="simple">
<dt>All of the above args will override SgModel’s corresponding attribute when not equal to None. Then evaluation</dt><dd><p>is ran on self.test_loader with self.test_metrics.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.SgModel.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_loader</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.utils.data.dataloader.DataLoader</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metrics</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torchmetrics.collections.MetricCollection</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluation_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference internal" href="super_gradients.training.sg_model.html#super_gradients.training.sg_model.sg_model.EvaluationType" title="super_gradients.training.sg_model.sg_model.EvaluationType"><span class="pre">super_gradients.training.sg_model.sg_model.EvaluationType</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">silent_mode</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metrics_progress_verbose</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.evaluate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.SgModel.evaluate" title="Permalink to this definition"></a></dt>
<dd><p>Evaluates the model on given dataloader and metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data_loader</strong> – dataloader to perform evaluataion on</p></li>
<li><p><strong>metrics</strong> – (MetricCollection) metrics for evaluation</p></li>
<li><p><strong>evaluation_type</strong> – (EvaluationType) controls which phase callbacks will be used (for example, on batch end,
when evaluation_type=EvaluationType.VALIDATION the Phase.VALIDATION_BATCH_END callbacks will be triggered)</p></li>
<li><p><strong>epoch</strong> – (int) epoch idx</p></li>
<li><p><strong>silent_mode</strong> – (bool) controls verbosity</p></li>
<li><p><strong>metrics_progress_verbose</strong> – (bool) controls the verbosity of metrics progress (default=False).
Slows down the program significantly.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>results tuple (tuple) containing the loss items and metric values.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.SgModel.instantiate_net">
<span class="sig-name descname"><span class="pre">instantiate_net</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">architecture</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">,</span> </span><span class="pre">type</span><span class="p"><span class="pre">,</span> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">arch_params</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_params</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">tuple</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.instantiate_net"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.SgModel.instantiate_net" title="Permalink to this definition"></a></dt>
<dd><dl class="simple">
<dt>Instantiates nn.Module according to architecture and arch_params, and handles pretrained weights and the required</dt><dd><p>module manipulation (i.e head replacement).</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>architecture</strong> – String, torch.nn.Module or uninstantiated SgModule class describing the netowrks architecture.</p></li>
<li><p><strong>arch_params</strong> – Architecture’s parameters passed to networks c’tor.</p></li>
<li><p><strong>checkpoint_params</strong> – checkpoint loading related parameters dictionary with ‘pretrained_weights’ key,
s.t it’s value is a string describing the dataset of the pretrained weights (for example “imagenent”).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>instantiated netowrk i.e torch.nn.Module, architecture_class (will be none when architecture is not str)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.MultiGPUMode">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.</span></span><span class="sig-name descname"><span class="pre">MultiGPUMode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#MultiGPUMode"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.MultiGPUMode" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">enum.Enum</span></code></p>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">OFF</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">-</span> <span class="pre">Single</span> <span class="pre">GPU</span> <span class="pre">Mode</span> <span class="pre">/</span> <span class="pre">CPU</span> <span class="pre">Mode</span></span></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">DATA_PARALLEL</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">-</span> <span class="pre">Multiple</span> <span class="pre">GPUs,</span> <span class="pre">Synchronous</span></span></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">DISTRIBUTED_DATA_PARALLEL</span> <span class="pre">-</span> <span class="pre">Multiple</span> <span class="pre">GPUs,</span> <span class="pre">Asynchronous</span></span></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="super_gradients.training.MultiGPUMode.OFF">
<span class="sig-name descname"><span class="pre">OFF</span></span><em class="property"> <span class="pre">=</span> <span class="pre">'Off'</span></em><a class="headerlink" href="#super_gradients.training.MultiGPUMode.OFF" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="super_gradients.training.MultiGPUMode.DATA_PARALLEL">
<span class="sig-name descname"><span class="pre">DATA_PARALLEL</span></span><em class="property"> <span class="pre">=</span> <span class="pre">'DP'</span></em><a class="headerlink" href="#super_gradients.training.MultiGPUMode.DATA_PARALLEL" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="super_gradients.training.MultiGPUMode.DISTRIBUTED_DATA_PARALLEL">
<span class="sig-name descname"><span class="pre">DISTRIBUTED_DATA_PARALLEL</span></span><em class="property"> <span class="pre">=</span> <span class="pre">'DDP'</span></em><a class="headerlink" href="#super_gradients.training.MultiGPUMode.DISTRIBUTED_DATA_PARALLEL" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="super_gradients.training.MultiGPUMode.AUTO">
<span class="sig-name descname"><span class="pre">AUTO</span></span><em class="property"> <span class="pre">=</span> <span class="pre">'AUTO'</span></em><a class="headerlink" href="#super_gradients.training.MultiGPUMode.AUTO" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.SegmentationTestDatasetInterface">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.</span></span><span class="sig-name descname"><span class="pre">SegmentationTestDatasetInterface</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">image_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/dataset_interfaces/dataset_interface.html#SegmentationTestDatasetInterface"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.SegmentationTestDatasetInterface" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="super_gradients.training.datasets.dataset_interfaces.html#super_gradients.training.datasets.dataset_interfaces.dataset_interface.TestDatasetInterface" title="super_gradients.training.datasets.dataset_interfaces.dataset_interface.TestDatasetInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">super_gradients.training.datasets.dataset_interfaces.dataset_interface.TestDatasetInterface</span></code></a></p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.DetectionTestDatasetInterface">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.</span></span><span class="sig-name descname"><span class="pre">DetectionTestDatasetInterface</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">image_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">320</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/dataset_interfaces/dataset_interface.html#DetectionTestDatasetInterface"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.DetectionTestDatasetInterface" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="super_gradients.training.datasets.dataset_interfaces.html#super_gradients.training.datasets.dataset_interfaces.dataset_interface.TestDatasetInterface" title="super_gradients.training.datasets.dataset_interfaces.dataset_interface.TestDatasetInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">super_gradients.training.datasets.dataset_interfaces.dataset_interface.TestDatasetInterface</span></code></a></p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.ClassificationTestDatasetInterface">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.</span></span><span class="sig-name descname"><span class="pre">ClassificationTestDatasetInterface</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">image_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/dataset_interfaces/dataset_interface.html#ClassificationTestDatasetInterface"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.ClassificationTestDatasetInterface" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="super_gradients.training.datasets.dataset_interfaces.html#super_gradients.training.datasets.dataset_interfaces.dataset_interface.TestDatasetInterface" title="super_gradients.training.datasets.dataset_interfaces.dataset_interface.TestDatasetInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">super_gradients.training.datasets.dataset_interfaces.dataset_interface.TestDatasetInterface</span></code></a></p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.StrictLoad">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.</span></span><span class="sig-name descname"><span class="pre">StrictLoad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#StrictLoad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.StrictLoad" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">enum.Enum</span></code></p>
<dl>
<dt>Wrapper for adding more functionality to torch’s strict_load parameter in load_state_dict().</dt><dd><dl>
<dt>Attributes:</dt><dd><p>OFF              - Native torch “strict_load = off” behaviour. See nn.Module.load_state_dict() documentation for more details.
ON               - Native torch “strict_load = on” behaviour. See nn.Module.load_state_dict() documentation for more details.
NO_KEY_MATCHING  - Allows the usage of SuperGradient’s adapt_checkpoint function, which loads a checkpoint by matching each</p>
<blockquote>
<div><p>layer’s shapes (and bypasses the strict matching of the names of each layer (ie: disregards the state_dict key matching)).</p>
</div></blockquote>
</dd>
</dl>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="super_gradients.training.StrictLoad.OFF">
<span class="sig-name descname"><span class="pre">OFF</span></span><em class="property"> <span class="pre">=</span> <span class="pre">False</span></em><a class="headerlink" href="#super_gradients.training.StrictLoad.OFF" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="super_gradients.training.StrictLoad.ON">
<span class="sig-name descname"><span class="pre">ON</span></span><em class="property"> <span class="pre">=</span> <span class="pre">True</span></em><a class="headerlink" href="#super_gradients.training.StrictLoad.ON" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="super_gradients.training.StrictLoad.NO_KEY_MATCHING">
<span class="sig-name descname"><span class="pre">NO_KEY_MATCHING</span></span><em class="property"> <span class="pre">=</span> <span class="pre">'no_key_matching'</span></em><a class="headerlink" href="#super_gradients.training.StrictLoad.NO_KEY_MATCHING" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="super-gradients-training-datasets-module">
<h2>super_gradients.training.datasets module<a class="headerlink" href="#super-gradients-training-datasets-module" title="Permalink to this headline"></a></h2>
<span class="target" id="module-super_gradients.training.datasets"></span><dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.datasets.DataAugmentation">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.datasets.</span></span><span class="sig-name descname"><span class="pre">DataAugmentation</span></span><a class="reference internal" href="_modules/super_gradients/training/datasets/data_augmentation.html#DataAugmentation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.DataAugmentation" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.datasets.DataAugmentation.to_tensor">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">to_tensor</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/data_augmentation.html#DataAugmentation.to_tensor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.DataAugmentation.to_tensor" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.datasets.DataAugmentation.normalize">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">normalize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mean</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">std</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/data_augmentation.html#DataAugmentation.normalize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.DataAugmentation.normalize" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.datasets.DataAugmentation.cutout">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">cutout</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mask_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cutout_inside</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask_color</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(0,</span> <span class="pre">0,</span> <span class="pre">0)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/data_augmentation.html#DataAugmentation.cutout"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.DataAugmentation.cutout" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.datasets.ListDataset">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.datasets.</span></span><span class="sig-name descname"><span class="pre">ListDataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">root</span></em>, <em class="sig-param"><span class="pre">file</span></em>, <em class="sig-param"><span class="pre">sample_loader:</span> <span class="pre">Callable</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">default_loader&gt;</span></em>, <em class="sig-param"><span class="pre">target_loader:</span> <span class="pre">Optional[Callable]</span> <span class="pre">=</span> <span class="pre">None</span></em>, <em class="sig-param"><span class="pre">collate_fn:</span> <span class="pre">Optional[Callable]</span> <span class="pre">=</span> <span class="pre">None</span></em>, <em class="sig-param"><span class="pre">sample_extensions:</span> <span class="pre">tuple</span> <span class="pre">=</span> <span class="pre">('.jpg'</span></em>, <em class="sig-param"><span class="pre">'.jpeg'</span></em>, <em class="sig-param"><span class="pre">'.png'</span></em>, <em class="sig-param"><span class="pre">'.ppm'</span></em>, <em class="sig-param"><span class="pre">'.bmp'</span></em>, <em class="sig-param"><span class="pre">'.pgm'</span></em>, <em class="sig-param"><span class="pre">'.tif'</span></em>, <em class="sig-param"><span class="pre">'.tiff'</span></em>, <em class="sig-param"><span class="pre">'.webp')</span></em>, <em class="sig-param"><span class="pre">sample_transform:</span> <span class="pre">Optional[Callable]</span> <span class="pre">=</span> <span class="pre">None</span></em>, <em class="sig-param"><span class="pre">target_transform:</span> <span class="pre">Optional[Callable]</span> <span class="pre">=</span> <span class="pre">None</span></em>, <em class="sig-param"><span class="pre">target_extension='.npy'</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/sg_dataset.html#ListDataset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.ListDataset" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Generic</span></code>[<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.utils.data.dataset.T_co</span></code>]</p>
<dl>
<dt>ListDataset - A PyTorch Vision Data Set extension that receives a file with FULL PATH to each of the samples.</dt><dd><p>Then, the assumption is that for every sample, there is a * matching target * in the same
path but with a different extension, i.e:</p>
<blockquote>
<div><dl class="simple">
<dt>for the samples paths:  (That appear in the list file)</dt><dd><p>/root/dataset/class_x/sample1.png
/root/dataset/class_y/sample123.png</p>
</dd>
<dt>the matching labels paths:  (That DO NOT appear in the list file)</dt><dd><p>/root/dataset/class_x/sample1.ext
/root/dataset/class_y/sample123.ext</p>
</dd>
</dl>
</div></blockquote>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.datasets.DirectoryDataSet">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.datasets.</span></span><span class="sig-name descname"><span class="pre">DirectoryDataSet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">root:</span> <span class="pre">str</span></em>, <em class="sig-param"><span class="pre">samples_sub_directory:</span> <span class="pre">str</span></em>, <em class="sig-param"><span class="pre">targets_sub_directory:</span> <span class="pre">str</span></em>, <em class="sig-param"><span class="pre">target_extension:</span> <span class="pre">str</span></em>, <em class="sig-param"><span class="pre">sample_loader:</span> <span class="pre">Callable</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">default_loader&gt;</span></em>, <em class="sig-param"><span class="pre">target_loader:</span> <span class="pre">Optional[Callable]</span> <span class="pre">=</span> <span class="pre">None</span></em>, <em class="sig-param"><span class="pre">collate_fn:</span> <span class="pre">Optional[Callable]</span> <span class="pre">=</span> <span class="pre">None</span></em>, <em class="sig-param"><span class="pre">sample_extensions:</span> <span class="pre">tuple</span> <span class="pre">=</span> <span class="pre">('.jpg'</span></em>, <em class="sig-param"><span class="pre">'.jpeg'</span></em>, <em class="sig-param"><span class="pre">'.png'</span></em>, <em class="sig-param"><span class="pre">'.ppm'</span></em>, <em class="sig-param"><span class="pre">'.bmp'</span></em>, <em class="sig-param"><span class="pre">'.pgm'</span></em>, <em class="sig-param"><span class="pre">'.tif'</span></em>, <em class="sig-param"><span class="pre">'.tiff'</span></em>, <em class="sig-param"><span class="pre">'.webp')</span></em>, <em class="sig-param"><span class="pre">sample_transform:</span> <span class="pre">Optional[Callable]</span> <span class="pre">=</span> <span class="pre">None</span></em>, <em class="sig-param"><span class="pre">target_transform:</span> <span class="pre">Optional[Callable]</span> <span class="pre">=</span> <span class="pre">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/sg_dataset.html#DirectoryDataSet"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.DirectoryDataSet" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Generic</span></code>[<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.utils.data.dataset.T_co</span></code>]</p>
<dl class="simple">
<dt>DirectoryDataSet - A PyTorch Vision Data Set extension that receives a root Dir and two separate sub directories:</dt><dd><ul class="simple">
<li><p>Sub-Directory for Samples</p></li>
<li><p>Sub-Directory for Targets</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.datasets.DetectionDataSet">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.datasets.</span></span><span class="sig-name descname"><span class="pre">DetectionDataSet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">root</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">list_file</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">img_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">416</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">augment</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset_hyper_params</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">dict</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_labels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_images</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_loading_method</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'default'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">collate_fn</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_extension</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'.txt'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels_offset</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_inclusion_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">all_classes_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/detection_datasets/detection_dataset.html#DetectionDataSet"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.DetectionDataSet" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Generic</span></code>[<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.utils.data.dataset.T_co</span></code>]</p>
<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.datasets.DetectionDataSet.mixup">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">mixup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">im</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">im2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels2</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/detection_datasets/detection_dataset.html#DetectionDataSet.mixup"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.DetectionDataSet.mixup" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.datasets.DetectionDataSet.sample_post_process">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">sample_post_process</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/detection_datasets/detection_dataset.html#DetectionDataSet.sample_post_process"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.DetectionDataSet.sample_post_process" title="Permalink to this definition"></a></dt>
<dd><dl class="simple">
<dt>sample_post_process - Normalizes and orders the image to be 3 x img_size x img_size</dt><dd><dl class="field-list simple">
<dt class="field-odd">param image</dt>
<dd class="field-odd"><p></p></dd>
<dt class="field-even">return</dt>
<dd class="field-even"><p></p></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.datasets.DetectionDataSet.sample_loader">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">sample_loader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_path</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/detection_datasets/detection_dataset.html#DetectionDataSet.sample_loader"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.DetectionDataSet.sample_loader" title="Permalink to this definition"></a></dt>
<dd><dl class="simple">
<dt>sample_loader - Loads a coco dataset image from path</dt><dd><dl class="field-list simple">
<dt class="field-odd">param sample_path</dt>
<dd class="field-odd"><p></p></dd>
<dt class="field-even">return</dt>
<dd class="field-even"><p></p></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.datasets.DetectionDataSet.sample_transform">
<span class="sig-name descname"><span class="pre">sample_transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/detection_datasets/detection_dataset.html#DetectionDataSet.sample_transform"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.DetectionDataSet.sample_transform" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>image</strong> – </p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.datasets.DetectionDataSet.target_loader">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">target_loader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target_path</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_inclusion_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">all_classes_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/detection_datasets/detection_dataset.html#DetectionDataSet.target_loader"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.DetectionDataSet.target_loader" title="Permalink to this definition"></a></dt>
<dd><dl class="simple">
<dt>coco_target_loader</dt><dd><p>&#64;param target_path: str, path to target.
&#64;param all_classes_list: list(str) containing all the class names or None when subclassing is disabled.
&#64;param class_inclusion_list: list(str) containing the subclass names or None when subclassing is disabled.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.datasets.DetectionDataSet.target_transform">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">target_transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ratio</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">w</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/detection_datasets/detection_dataset.html#DetectionDataSet.target_transform"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.DetectionDataSet.target_transform" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>target</strong> – </p></li>
<li><p><strong>ratio</strong> – </p></li>
<li><p><strong>w</strong> – </p></li>
<li><p><strong>h</strong> – </p></li>
<li><p><strong>pad</strong> – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.datasets.DetectionDataSet.exif_size">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">exif_size</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">img</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/detection_datasets/detection_dataset.html#DetectionDataSet.exif_size"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.DetectionDataSet.exif_size" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>img</strong> – </p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.datasets.DetectionDataSet.augment_hsv">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">augment_hsv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">img</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hgain</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sgain</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vgain</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/detection_datasets/detection_dataset.html#DetectionDataSet.augment_hsv"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.DetectionDataSet.augment_hsv" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>img</strong> – <dl class="field-list simple">
<dt class="field-odd">param hgain</dt>
<dd class="field-odd"><p></p></dd>
<dt class="field-even">param sgain</dt>
<dd class="field-even"><p></p></dd>
<dt class="field-odd">param vgain</dt>
<dd class="field-odd"><p></p></dd>
</dl>
</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.datasets.DetectionDataSet.letterbox">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">letterbox</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">img</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">new_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(416,</span> <span class="pre">416)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">color</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(128,</span> <span class="pre">128,</span> <span class="pre">128)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">auto</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scaleFill</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scaleup</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">interp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">tuple</span><a class="reference internal" href="_modules/super_gradients/training/datasets/detection_datasets/detection_dataset.html#DetectionDataSet.letterbox"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.DetectionDataSet.letterbox" title="Permalink to this definition"></a></dt>
<dd><p>letterbox - Resizes image to a 32-pixel-multiple rectangle
:param img:
:param new_shape:
:param color:
:param auto:
:param scaleFill:
:param scaleup:
:param interp:
:return:</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.datasets.DetectionDataSet.random_perspective">
<span class="sig-name descname"><span class="pre">random_perspective</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">img</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">targets</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">degrees</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">translate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shear</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">border</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">perspective</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/detection_datasets/detection_dataset.html#DetectionDataSet.random_perspective"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.DetectionDataSet.random_perspective" title="Permalink to this definition"></a></dt>
<dd><p>random images and labels using a perspective transform</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.datasets.DetectionDataSet.box_candidates">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">box_candidates</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">box1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">box2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wh_thr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ar_thr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">area_thr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/detection_datasets/detection_dataset.html#DetectionDataSet.box_candidates"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.DetectionDataSet.box_candidates" title="Permalink to this definition"></a></dt>
<dd><dl class="simple">
<dt>compute candidate boxes</dt><dd><dl class="field-list simple">
<dt class="field-odd">param box1</dt>
<dd class="field-odd"><p>before augment</p>
</dd>
<dt class="field-even">param box2</dt>
<dd class="field-even"><p>after augment</p>
</dd>
<dt class="field-odd">param wh_thr</dt>
<dd class="field-odd"><p>wh_thr (pixels)</p>
</dd>
<dt class="field-even">param ar_thr</dt>
<dd class="field-even"><p>aspect_ratio_thr</p>
</dd>
<dt class="field-odd">param area_thr</dt>
<dd class="field-odd"><p>area_ratio</p>
</dd>
</dl>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.datasets.DetectionDataSet.load_mosaic">
<span class="sig-name descname"><span class="pre">load_mosaic</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">index</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/detection_datasets/detection_dataset.html#DetectionDataSet.load_mosaic"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.DetectionDataSet.load_mosaic" title="Permalink to this definition"></a></dt>
<dd><dl class="simple">
<dt>load_mosaic - Load images in mosaic format to improve noise handling while training</dt><dd><dl class="field-list simple">
<dt class="field-odd">param index</dt>
<dd class="field-odd"><p></p></dd>
<dt class="field-even">return</dt>
<dd class="field-even"><p></p></dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.datasets.COCODetectionDataSet">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.datasets.</span></span><span class="sig-name descname"><span class="pre">COCODetectionDataSet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/detection_datasets/coco_detection.html#COCODetectionDataSet"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.COCODetectionDataSet" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Generic</span></code>[<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.utils.data.dataset.T_co</span></code>]</p>
<p>COCODetectionDataSet - Detection Data Set Class COCO Data Set</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.datasets.SegmentationDataSet">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.datasets.</span></span><span class="sig-name descname"><span class="pre">SegmentationDataSet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">root</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">list_file</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">samples_sub_directory</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">targets_sub_directory</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">img_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">608</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">crop_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">augment</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset_hyper_params</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">dict</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_labels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_images</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_loader</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Callable</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_loader</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Callable</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">collate_fn</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Callable</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_extension</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'.png'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">image_mask_transforms</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torchvision.transforms.transforms.Compose</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">image_mask_transforms_aug</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torchvision.transforms.transforms.Compose</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/segmentation_datasets/segmentation_dataset.html#SegmentationDataSet"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.SegmentationDataSet" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Generic</span></code>[<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.utils.data.dataset.T_co</span></code>]</p>
<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.datasets.SegmentationDataSet.sample_loader">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">sample_loader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_path</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">&lt;module</span> <span class="pre">‘PIL.Image’</span> <span class="pre">from</span> <span class="pre">‘/Users/oferbaratz/PycharmProjects/SG/venv/lib/python3.8/site-packages/PIL/Image.py’&gt;</span><a class="reference internal" href="_modules/super_gradients/training/datasets/segmentation_datasets/segmentation_dataset.html#SegmentationDataSet.sample_loader"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.SegmentationDataSet.sample_loader" title="Permalink to this definition"></a></dt>
<dd><dl class="simple">
<dt>sample_loader - Loads a dataset image from path using PIL</dt><dd><dl class="field-list simple">
<dt class="field-odd">param sample_path</dt>
<dd class="field-odd"><p>The path to the sample image</p>
</dd>
<dt class="field-even">return</dt>
<dd class="field-even"><p>The loaded Image</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.datasets.SegmentationDataSet.sample_transform">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">sample_transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/segmentation_datasets/segmentation_dataset.html#SegmentationDataSet.sample_transform"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.SegmentationDataSet.sample_transform" title="Permalink to this definition"></a></dt>
<dd><p>sample_transform - Transforms the sample image</p>
<blockquote>
<div><dl class="field-list simple">
<dt class="field-odd">param image</dt>
<dd class="field-odd"><p>The input image to transform</p>
</dd>
<dt class="field-even">return</dt>
<dd class="field-even"><p>The transformed image</p>
</dd>
</dl>
</div></blockquote>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.datasets.SegmentationDataSet.target_loader">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">target_loader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target_path</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">&lt;module</span> <span class="pre">‘PIL.Image’</span> <span class="pre">from</span> <span class="pre">‘/Users/oferbaratz/PycharmProjects/SG/venv/lib/python3.8/site-packages/PIL/Image.py’&gt;</span><a class="reference internal" href="_modules/super_gradients/training/datasets/segmentation_datasets/segmentation_dataset.html#SegmentationDataSet.target_loader"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.SegmentationDataSet.target_loader" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>target_path</strong> – The path to the sample image</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The loaded Image</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.datasets.SegmentationDataSet.target_transform">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">target_transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/segmentation_datasets/segmentation_dataset.html#SegmentationDataSet.target_transform"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.SegmentationDataSet.target_transform" title="Permalink to this definition"></a></dt>
<dd><p>target_transform - Transforms the sample image</p>
<blockquote>
<div><dl class="field-list simple">
<dt class="field-odd">param target</dt>
<dd class="field-odd"><p>The target mask to transform</p>
</dd>
<dt class="field-even">return</dt>
<dd class="field-even"><p>The transformed target mask</p>
</dd>
</dl>
</div></blockquote>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.datasets.PascalVOC2012SegmentationDataSet">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.datasets.</span></span><span class="sig-name descname"><span class="pre">PascalVOC2012SegmentationDataSet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_suffix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_suffix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/segmentation_datasets/pascal_voc_segmentation.html#PascalVOC2012SegmentationDataSet"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.PascalVOC2012SegmentationDataSet" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Generic</span></code>[<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.utils.data.dataset.T_co</span></code>]</p>
<p>PascalVOC2012SegmentationDataSet - Segmentation Data Set Class for Pascal VOC 2012 Data Set</p>
<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.datasets.PascalVOC2012SegmentationDataSet.decode_segmentation_mask">
<span class="sig-name descname"><span class="pre">decode_segmentation_mask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">label_mask</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/segmentation_datasets/pascal_voc_segmentation.html#PascalVOC2012SegmentationDataSet.decode_segmentation_mask"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.PascalVOC2012SegmentationDataSet.decode_segmentation_mask" title="Permalink to this definition"></a></dt>
<dd><dl class="simple">
<dt>decode_segmentation_mask - Decodes the colors for the Segmentation Mask</dt><dd><dl class="field-list simple">
<dt class="field-odd">param</dt>
<dd class="field-odd"><p>label_mask:  an (M,N) array of integer values denoting
the class label at each spatial location.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.datasets.PascalAUG2012SegmentationDataSet">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.datasets.</span></span><span class="sig-name descname"><span class="pre">PascalAUG2012SegmentationDataSet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/segmentation_datasets/pascal_aug_segmentation.html#PascalAUG2012SegmentationDataSet"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.PascalAUG2012SegmentationDataSet" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Generic</span></code>[<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.utils.data.dataset.T_co</span></code>]</p>
<p>PascalAUG2012SegmentationDataSet - Segmentation Data Set Class for Pascal AUG 2012 Data Set</p>
<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.datasets.PascalAUG2012SegmentationDataSet.target_loader">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">target_loader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target_path</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">&lt;module</span> <span class="pre">‘PIL.Image’</span> <span class="pre">from</span> <span class="pre">‘/Users/oferbaratz/PycharmProjects/SG/venv/lib/python3.8/site-packages/PIL/Image.py’&gt;</span><a class="reference internal" href="_modules/super_gradients/training/datasets/segmentation_datasets/pascal_aug_segmentation.html#PascalAUG2012SegmentationDataSet.target_loader"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.PascalAUG2012SegmentationDataSet.target_loader" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>target_path</strong> – The path to the target data</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The loaded target</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.datasets.CoCoSegmentationDataSet">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.datasets.</span></span><span class="sig-name descname"><span class="pre">CoCoSegmentationDataSet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset_classes_inclusion_tuples_list</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">list</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/segmentation_datasets/coco_segmentation.html#CoCoSegmentationDataSet"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.CoCoSegmentationDataSet" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Generic</span></code>[<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.utils.data.dataset.T_co</span></code>]</p>
<p>CoCoSegmentationDataSet - Segmentation Data Set Class for COCO 2017 Segmentation Data Set</p>
<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.datasets.CoCoSegmentationDataSet.target_loader">
<span class="sig-name descname"><span class="pre">target_loader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mask_metadata_tuple</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">&lt;module</span> <span class="pre">‘PIL.Image’</span> <span class="pre">from</span> <span class="pre">‘/Users/oferbaratz/PycharmProjects/SG/venv/lib/python3.8/site-packages/PIL/Image.py’&gt;</span><a class="reference internal" href="_modules/super_gradients/training/datasets/segmentation_datasets/coco_segmentation.html#CoCoSegmentationDataSet.target_loader"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.CoCoSegmentationDataSet.target_loader" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>mask_metadata_tuple</strong> – A tuple of (coco_image_id, original_image_height, original_image_width)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The mask image created from the array</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.datasets.TestDatasetInterface">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.datasets.</span></span><span class="sig-name descname"><span class="pre">TestDatasetInterface</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trainset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/dataset_interfaces/dataset_interface.html#TestDatasetInterface"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.TestDatasetInterface" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="super_gradients.training.datasets.dataset_interfaces.html#super_gradients.training.datasets.dataset_interfaces.dataset_interface.DatasetInterface" title="super_gradients.training.datasets.dataset_interfaces.dataset_interface.DatasetInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">super_gradients.training.datasets.dataset_interfaces.dataset_interface.DatasetInterface</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.datasets.TestDatasetInterface.get_data_loaders">
<span class="sig-name descname"><span class="pre">get_data_loaders</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size_factor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_workers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distributed_sampler</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/dataset_interfaces/dataset_interface.html#TestDatasetInterface.get_data_loaders"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.TestDatasetInterface.get_data_loaders" title="Permalink to this definition"></a></dt>
<dd><p>Get self.train_loader, self.test_loader, self.classes.</p>
<p>If the data loaders haven’t been initialized yet, build them first.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>kwargs</strong> – kwargs are passed to build_data_loaders.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.datasets.DatasetInterface">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.datasets.</span></span><span class="sig-name descname"><span class="pre">DatasetInterface</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_loader</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_loader</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_loader</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/dataset_interfaces/dataset_interface.html#DatasetInterface"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.DatasetInterface" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>DatasetInterface - This class manages all of the “communiation” the Model has with the Data Sets</p>
<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.datasets.DatasetInterface.download_from_cloud">
<span class="sig-name descname"><span class="pre">download_from_cloud</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/dataset_interfaces/dataset_interface.html#DatasetInterface.download_from_cloud"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.DatasetInterface.download_from_cloud" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.datasets.DatasetInterface.build_data_loaders">
<span class="sig-name descname"><span class="pre">build_data_loaders</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size_factor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_workers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distributed_sampler</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/dataset_interfaces/dataset_interface.html#DatasetInterface.build_data_loaders"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.DatasetInterface.build_data_loaders" title="Permalink to this definition"></a></dt>
<dd><p>define train, val (and optionally test) loaders. The method deals separately with distributed training and standard
(non distributed, or parallel training). In the case of distributed training we need to rely on distributed
samplers.
:param batch_size_factor: int - factor to multiply the batch size (usually for multi gpu)
:param num_workers: int - number of workers (parallel processes) for dataloaders
:param train_batch_size: int - batch size for train loader, if None will be taken from dataset_params
:param val_batch_size: int - batch size for val loader, if None will be taken from dataset_params
:param distributed_sampler: boolean flag for distributed training mode
:return: train_loader, val_loader, classes: list of classes</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.datasets.DatasetInterface.get_data_loaders">
<span class="sig-name descname"><span class="pre">get_data_loaders</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/dataset_interfaces/dataset_interface.html#DatasetInterface.get_data_loaders"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.DatasetInterface.get_data_loaders" title="Permalink to this definition"></a></dt>
<dd><p>Get self.train_loader, self.test_loader, self.classes.</p>
<p>If the data loaders haven’t been initialized yet, build them first.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>kwargs</strong> – kwargs are passed to build_data_loaders.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.datasets.DatasetInterface.get_val_sample">
<span class="sig-name descname"><span class="pre">get_val_sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/dataset_interfaces/dataset_interface.html#DatasetInterface.get_val_sample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.DatasetInterface.get_val_sample" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.datasets.DatasetInterface.get_dataset_params">
<span class="sig-name descname"><span class="pre">get_dataset_params</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/dataset_interfaces/dataset_interface.html#DatasetInterface.get_dataset_params"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.DatasetInterface.get_dataset_params" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.datasets.DatasetInterface.print_dataset_details">
<span class="sig-name descname"><span class="pre">print_dataset_details</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/dataset_interfaces/dataset_interface.html#DatasetInterface.print_dataset_details"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.DatasetInterface.print_dataset_details" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.datasets.Cifar10DatasetInterface">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.datasets.</span></span><span class="sig-name descname"><span class="pre">Cifar10DatasetInterface</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/dataset_interfaces/dataset_interface.html#Cifar10DatasetInterface"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.Cifar10DatasetInterface" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="super_gradients.training.datasets.dataset_interfaces.html#super_gradients.training.datasets.dataset_interfaces.dataset_interface.LibraryDatasetInterface" title="super_gradients.training.datasets.dataset_interfaces.dataset_interface.LibraryDatasetInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">super_gradients.training.datasets.dataset_interfaces.dataset_interface.LibraryDatasetInterface</span></code></a></p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.datasets.CoCoSegmentationDatasetInterface">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.datasets.</span></span><span class="sig-name descname"><span class="pre">CoCoSegmentationDatasetInterface</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_labels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_images</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset_classes_inclusion_tuples_list</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">list</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/dataset_interfaces/dataset_interface.html#CoCoSegmentationDatasetInterface"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.CoCoSegmentationDatasetInterface" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="super_gradients.training.datasets.dataset_interfaces.html#super_gradients.training.datasets.dataset_interfaces.dataset_interface.CoCoDataSetInterfaceBase" title="super_gradients.training.datasets.dataset_interfaces.dataset_interface.CoCoDataSetInterfaceBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">super_gradients.training.datasets.dataset_interfaces.dataset_interface.CoCoDataSetInterfaceBase</span></code></a></p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.datasets.CoCoDetectionDatasetInterface">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.datasets.</span></span><span class="sig-name descname"><span class="pre">CoCoDetectionDatasetInterface</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_images</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_list_file</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train2017.txt'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_list_file</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'val2017.txt'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/dataset_interfaces/dataset_interface.html#CoCoDetectionDatasetInterface"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.CoCoDetectionDatasetInterface" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="super_gradients.training.datasets.dataset_interfaces.html#super_gradients.training.datasets.dataset_interfaces.dataset_interface.CoCoDataSetInterfaceBase" title="super_gradients.training.datasets.dataset_interfaces.dataset_interface.CoCoDataSetInterfaceBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">super_gradients.training.datasets.dataset_interfaces.dataset_interface.CoCoDataSetInterfaceBase</span></code></a></p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.datasets.CoCo2014DetectionDatasetInterface">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.datasets.</span></span><span class="sig-name descname"><span class="pre">CoCo2014DetectionDatasetInterface</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_images</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_list_file</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train2014.txt'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_list_file</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'val2014.txt'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/dataset_interfaces/dataset_interface.html#CoCo2014DetectionDatasetInterface"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.CoCo2014DetectionDatasetInterface" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="super_gradients.training.datasets.dataset_interfaces.html#super_gradients.training.datasets.dataset_interfaces.dataset_interface.CoCoDetectionDatasetInterface" title="super_gradients.training.datasets.dataset_interfaces.dataset_interface.CoCoDetectionDatasetInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">super_gradients.training.datasets.dataset_interfaces.dataset_interface.CoCoDetectionDatasetInterface</span></code></a></p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.datasets.PascalVOC2012SegmentationDataSetInterface">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.datasets.</span></span><span class="sig-name descname"><span class="pre">PascalVOC2012SegmentationDataSetInterface</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_images</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/dataset_interfaces/dataset_interface.html#PascalVOC2012SegmentationDataSetInterface"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.PascalVOC2012SegmentationDataSetInterface" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="super_gradients.training.datasets.dataset_interfaces.html#super_gradients.training.datasets.dataset_interfaces.dataset_interface.DatasetInterface" title="super_gradients.training.datasets.dataset_interfaces.dataset_interface.DatasetInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">super_gradients.training.datasets.dataset_interfaces.dataset_interface.DatasetInterface</span></code></a></p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.datasets.PascalAUG2012SegmentationDataSetInterface">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.datasets.</span></span><span class="sig-name descname"><span class="pre">PascalAUG2012SegmentationDataSetInterface</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_images</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/dataset_interfaces/dataset_interface.html#PascalAUG2012SegmentationDataSetInterface"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.PascalAUG2012SegmentationDataSetInterface" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="super_gradients.training.datasets.dataset_interfaces.html#super_gradients.training.datasets.dataset_interfaces.dataset_interface.DatasetInterface" title="super_gradients.training.datasets.dataset_interfaces.dataset_interface.DatasetInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">super_gradients.training.datasets.dataset_interfaces.dataset_interface.DatasetInterface</span></code></a></p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.datasets.TestYoloDetectionDatasetInterface">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.datasets.</span></span><span class="sig-name descname"><span class="pre">TestYoloDetectionDatasetInterface</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(3,</span> <span class="pre">32,</span> <span class="pre">32)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/dataset_interfaces/dataset_interface.html#TestYoloDetectionDatasetInterface"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.TestYoloDetectionDatasetInterface" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="super_gradients.training.datasets.dataset_interfaces.html#super_gradients.training.datasets.dataset_interfaces.dataset_interface.DatasetInterface" title="super_gradients.training.datasets.dataset_interfaces.dataset_interface.DatasetInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">super_gradients.training.datasets.dataset_interfaces.dataset_interface.DatasetInterface</span></code></a></p>
<p>note: the output size is (batch_size, 6) in the test while in real training
the size of axis 0 can vary (the number of bounding boxes)</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.datasets.DetectionTestDatasetInterface">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.datasets.</span></span><span class="sig-name descname"><span class="pre">DetectionTestDatasetInterface</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">image_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">320</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/dataset_interfaces/dataset_interface.html#DetectionTestDatasetInterface"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.DetectionTestDatasetInterface" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="super_gradients.training.datasets.dataset_interfaces.html#super_gradients.training.datasets.dataset_interfaces.dataset_interface.TestDatasetInterface" title="super_gradients.training.datasets.dataset_interfaces.dataset_interface.TestDatasetInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">super_gradients.training.datasets.dataset_interfaces.dataset_interface.TestDatasetInterface</span></code></a></p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.datasets.ClassificationTestDatasetInterface">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.datasets.</span></span><span class="sig-name descname"><span class="pre">ClassificationTestDatasetInterface</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">image_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/dataset_interfaces/dataset_interface.html#ClassificationTestDatasetInterface"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.ClassificationTestDatasetInterface" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="super_gradients.training.datasets.dataset_interfaces.html#super_gradients.training.datasets.dataset_interfaces.dataset_interface.TestDatasetInterface" title="super_gradients.training.datasets.dataset_interfaces.dataset_interface.TestDatasetInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">super_gradients.training.datasets.dataset_interfaces.dataset_interface.TestDatasetInterface</span></code></a></p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.datasets.SegmentationTestDatasetInterface">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.datasets.</span></span><span class="sig-name descname"><span class="pre">SegmentationTestDatasetInterface</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">image_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/dataset_interfaces/dataset_interface.html#SegmentationTestDatasetInterface"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.SegmentationTestDatasetInterface" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="super_gradients.training.datasets.dataset_interfaces.html#super_gradients.training.datasets.dataset_interfaces.dataset_interface.TestDatasetInterface" title="super_gradients.training.datasets.dataset_interfaces.dataset_interface.TestDatasetInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">super_gradients.training.datasets.dataset_interfaces.dataset_interface.TestDatasetInterface</span></code></a></p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.datasets.ImageNetDatasetInterface">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.datasets.</span></span><span class="sig-name descname"><span class="pre">ImageNetDatasetInterface</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data_dir</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'/data/Imagenet'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/datasets/dataset_interfaces/dataset_interface.html#ImageNetDatasetInterface"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.datasets.ImageNetDatasetInterface" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="super_gradients.training.datasets.dataset_interfaces.html#super_gradients.training.datasets.dataset_interfaces.dataset_interface.DatasetInterface" title="super_gradients.training.datasets.dataset_interfaces.dataset_interface.DatasetInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">super_gradients.training.datasets.dataset_interfaces.dataset_interface.DatasetInterface</span></code></a></p>
</dd></dl>

</section>
<section id="super-gradients-training-exceptions-module">
<h2>super_gradients.training.exceptions module<a class="headerlink" href="#super-gradients-training-exceptions-module" title="Permalink to this headline"></a></h2>
<span class="target" id="module-super_gradients.training.exceptions"></span></section>
<section id="module-super_gradients.training.legacy">
<span id="super-gradients-training-legacy-module"></span><h2>super_gradients.training.legacy module<a class="headerlink" href="#module-super_gradients.training.legacy" title="Permalink to this headline"></a></h2>
</section>
<section id="module-super_gradients.training.losses">
<span id="super-gradients-training-losses-models-module"></span><h2>super_gradients.training.losses_models module<a class="headerlink" href="#module-super_gradients.training.losses" title="Permalink to this headline"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.losses.FocalLoss">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.losses.</span></span><span class="sig-name descname"><span class="pre">FocalLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss_fcn</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.nn.modules.loss.BCEWithLogitsLoss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.25</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/losses/focal_loss.html#FocalLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.losses.FocalLoss" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.loss._Loss</span></code></p>
<p>Wraps focal loss around existing loss_fcn(), i.e. criteria = FocalLoss(nn.BCEWithLogitsLoss(), gamma=1.5)</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="super_gradients.training.losses.FocalLoss.reduction">
<span class="sig-name descname"><span class="pre">reduction</span></span><em class="property"><span class="pre">:</span> <span class="pre">str</span></em><a class="headerlink" href="#super_gradients.training.losses.FocalLoss.reduction" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.losses.FocalLoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">true</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/losses/focal_loss.html#FocalLoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.losses.FocalLoss.forward" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.losses.LabelSmoothingCrossEntropyLoss">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.losses.</span></span><span class="sig-name descname"><span class="pre">LabelSmoothingCrossEntropyLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_index</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">smooth_eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">smooth_dist</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">from_logits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/losses/label_smoothing_cross_entropy_loss.html#LabelSmoothingCrossEntropyLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.losses.LabelSmoothingCrossEntropyLoss" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.loss.CrossEntropyLoss</span></code></p>
<p>CrossEntropyLoss - with ability to recieve distrbution as targets, and optional label smoothing</p>
<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.losses.LabelSmoothingCrossEntropyLoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">smooth_dist</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/losses/label_smoothing_cross_entropy_loss.html#LabelSmoothingCrossEntropyLoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.losses.LabelSmoothingCrossEntropyLoss.forward" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="super_gradients.training.losses.LabelSmoothingCrossEntropyLoss.ignore_index">
<span class="sig-name descname"><span class="pre">ignore_index</span></span><em class="property"><span class="pre">:</span> <span class="pre">int</span></em><a class="headerlink" href="#super_gradients.training.losses.LabelSmoothingCrossEntropyLoss.ignore_index" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="super_gradients.training.losses.LabelSmoothingCrossEntropyLoss.label_smoothing">
<span class="sig-name descname"><span class="pre">label_smoothing</span></span><em class="property"><span class="pre">:</span> <span class="pre">float</span></em><a class="headerlink" href="#super_gradients.training.losses.LabelSmoothingCrossEntropyLoss.label_smoothing" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.losses.ShelfNetOHEMLoss">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.losses.</span></span><span class="sig-name descname"><span class="pre">ShelfNetOHEMLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.7</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mining_percent</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_lb</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">255</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/losses/shelfnet_ohem_loss.html#ShelfNetOHEMLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.losses.ShelfNetOHEMLoss" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="super_gradients.training.losses.html#super_gradients.training.losses.ohem_ce_loss.OhemCELoss" title="super_gradients.training.losses.ohem_ce_loss.OhemCELoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">super_gradients.training.losses.ohem_ce_loss.OhemCELoss</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.losses.ShelfNetOHEMLoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">predictions_list</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">list</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">targets</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/losses/shelfnet_ohem_loss.html#ShelfNetOHEMLoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.losses.ShelfNetOHEMLoss.forward" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="super_gradients.training.losses.ShelfNetOHEMLoss.reduction">
<span class="sig-name descname"><span class="pre">reduction</span></span><em class="property"><span class="pre">:</span> <span class="pre">str</span></em><a class="headerlink" href="#super_gradients.training.losses.ShelfNetOHEMLoss.reduction" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.losses.ShelfNetSemanticEncodingLoss">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.losses.</span></span><span class="sig-name descname"><span class="pre">ShelfNetSemanticEncodingLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">se_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nclass</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">21</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">aux_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_index</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/losses/shelfnet_semantic_encoding_loss.html#ShelfNetSemanticEncodingLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.losses.ShelfNetSemanticEncodingLoss" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.loss.CrossEntropyLoss</span></code></p>
<p>2D Cross Entropy Loss with Auxilary Loss</p>
<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.losses.ShelfNetSemanticEncodingLoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logits</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/losses/shelfnet_semantic_encoding_loss.html#ShelfNetSemanticEncodingLoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.losses.ShelfNetSemanticEncodingLoss.forward" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="super_gradients.training.losses.ShelfNetSemanticEncodingLoss.ignore_index">
<span class="sig-name descname"><span class="pre">ignore_index</span></span><em class="property"><span class="pre">:</span> <span class="pre">int</span></em><a class="headerlink" href="#super_gradients.training.losses.ShelfNetSemanticEncodingLoss.ignore_index" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="super_gradients.training.losses.ShelfNetSemanticEncodingLoss.label_smoothing">
<span class="sig-name descname"><span class="pre">label_smoothing</span></span><em class="property"><span class="pre">:</span> <span class="pre">float</span></em><a class="headerlink" href="#super_gradients.training.losses.ShelfNetSemanticEncodingLoss.label_smoothing" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.losses.YoLoV3DetectionLoss">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.losses.</span></span><span class="sig-name descname"><span class="pre">YoLoV3DetectionLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cls_pw</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">obj_pw</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">giou</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">3.54</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">obj</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">64.3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cls</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">37.4</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/losses/yolo_v3_loss.html#YoLoV3DetectionLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.losses.YoLoV3DetectionLoss" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.loss._Loss</span></code></p>
<p>YoLoV3DetectionLoss - Loss Class for Object Detection</p>
<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.losses.YoLoV3DetectionLoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">targets</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/losses/yolo_v3_loss.html#YoLoV3DetectionLoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.losses.YoLoV3DetectionLoss.forward" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="super_gradients.training.losses.YoLoV3DetectionLoss.reduction">
<span class="sig-name descname"><span class="pre">reduction</span></span><em class="property"><span class="pre">:</span> <span class="pre">str</span></em><a class="headerlink" href="#super_gradients.training.losses.YoLoV3DetectionLoss.reduction" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.losses.YoLoV5DetectionLoss">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.losses.</span></span><span class="sig-name descname"><span class="pre">YoLoV5DetectionLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">anchors</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference internal" href="super_gradients.training.utils.html#super_gradients.training.utils.detection_utils.Anchors" title="super_gradients.training.utils.detection_utils.Anchors"><span class="pre">super_gradients.training.utils.detection_utils.Anchors</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">cls_pos_weight</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">obj_pos_weight</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">obj_loss_gain</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">box_loss_gain</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cls_loss_gain</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">focal_loss_gamma</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cls_objectness_weights</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">anchor_threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/losses/yolo_v5_loss.html#YoLoV5DetectionLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.losses.YoLoV5DetectionLoss" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.loss._Loss</span></code></p>
<p>Calculate YOLO V5 loss:
L = L_objectivness + L_boxes + L_classification</p>
<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.losses.YoLoV5DetectionLoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">targets</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/losses/yolo_v5_loss.html#YoLoV5DetectionLoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.losses.YoLoV5DetectionLoss.forward" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.losses.YoLoV5DetectionLoss.build_targets">
<span class="sig-name descname"><span class="pre">build_targets</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">predictions</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">targets</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><a class="reference internal" href="_modules/super_gradients/training/losses/yolo_v5_loss.html#YoLoV5DetectionLoss.build_targets"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.losses.YoLoV5DetectionLoss.build_targets" title="Permalink to this definition"></a></dt>
<dd><dl>
<dt>Assign targets to anchors to use in L_boxes &amp; L_classification calculation:</dt><dd><ul class="simple">
<li><p>each target can be assigned to a few anchors,</p></li>
</ul>
<p>all anchors that are within [1/self.anchor_threshold, self.anchor_threshold] times target size range
* each anchor can be assigned to a few targets</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>predictions</strong> – Yolo predictions</p></li>
<li><p><strong>targets</strong> – ground truth targets</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>each of 4 outputs contains one element for each Yolo output,
correspondences are raveled over the whole batch and all anchors:</p>
<blockquote>
<div><ul class="simple">
<li><p>classes of the targets;</p></li>
<li><p>boxes of the targets;</p></li>
<li><p>image id in a batch, anchor id, grid y, grid x coordinates;</p></li>
<li><p>anchor sizes.</p></li>
</ul>
</div></blockquote>
<p>All the above can be indexed in parallel to get the selected correspondences</p>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.losses.YoLoV5DetectionLoss.compute_loss">
<span class="sig-name descname"><span class="pre">compute_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">predictions</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">targets</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">giou_loss_ratio</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><a class="reference internal" href="_modules/super_gradients/training/losses/yolo_v5_loss.html#YoLoV5DetectionLoss.compute_loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.losses.YoLoV5DetectionLoss.compute_loss" title="Permalink to this definition"></a></dt>
<dd><p>L = L_objectivness + L_boxes + L_classification
where:</p>
<blockquote>
<div><ul class="simple">
<li><p>L_boxes and L_classification are calculated only between anchors and targets that suit them;</p></li>
<li><p>L_objectivness is calculated on all anchors.</p></li>
</ul>
</div></blockquote>
<dl>
<dt>L_classification:</dt><dd><p>for anchors that have suitable ground truths in their grid locations add BCEs
to force max probability for each GT class in a multi-label way
Coef: self.cls_loss_gain</p>
</dd>
<dt>L_boxes:</dt><dd><p>for anchors that have suitable ground truths in their grid locations
add (1 - IoU), IoU between a predicted box and each GT box, force maximum IoU
Coef: self.box_loss_gain</p>
</dd>
<dt>L_objectness:</dt><dd><p>for each anchor add BCE to force a prediction of (1 - giou_loss_ratio) + giou_loss_ratio * IoU,
IoU between a predicted box and random GT in it
Coef: self.obj_loss_gain, loss from each YOLO grid is additionally multiplied by balance = [4.0, 1.0, 0.4]</p>
<blockquote>
<div><p>to balance different contributions coming from different numbers of grid cells</p>
</div></blockquote>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>predictions</strong> – output from all Yolo levels, each of shape
[Batch x Num_Anchors x GridSizeY x GridSizeX x (4 + 1 + Num_classes)]</p></li>
<li><p><strong>targets</strong> – [Num_targets x (4 + 2)], values on dim 1 are: image id in a batch, class, box x y w h</p></li>
<li><p><strong>giou_loss_ratio</strong> – a coef in L_objectness defining what should be predicted as objecness
in a call with a target: can be a value in [IoU, 1] range</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>loss, all losses separately in a detached tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="super_gradients.training.losses.YoLoV5DetectionLoss.reduction">
<span class="sig-name descname"><span class="pre">reduction</span></span><em class="property"><span class="pre">:</span> <span class="pre">str</span></em><a class="headerlink" href="#super_gradients.training.losses.YoLoV5DetectionLoss.reduction" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.losses.RSquaredLoss">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.losses.</span></span><span class="sig-name descname"><span class="pre">RSquaredLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size_average</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'mean'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/losses/r_squared_loss.html#RSquaredLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.losses.RSquaredLoss" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.loss._Loss</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.losses.RSquaredLoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/losses/r_squared_loss.html#RSquaredLoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.losses.RSquaredLoss.forward" title="Permalink to this definition"></a></dt>
<dd><p>Computes the R-squared for the output and target values
:param output: Tensor / Numpy / List</p>
<blockquote>
<div><p>The prediction</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>target</strong> – Tensor / Numpy / List
The corresponding lables</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="super_gradients.training.losses.RSquaredLoss.reduction">
<span class="sig-name descname"><span class="pre">reduction</span></span><em class="property"><span class="pre">:</span> <span class="pre">str</span></em><a class="headerlink" href="#super_gradients.training.losses.RSquaredLoss.reduction" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.losses.SSDLoss">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.losses.</span></span><span class="sig-name descname"><span class="pre">SSDLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dboxes</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference internal" href="super_gradients.training.utils.html#super_gradients.training.utils.ssd_utils.DefaultBoxes" title="super_gradients.training.utils.ssd_utils.DefaultBoxes"><span class="pre">super_gradients.training.utils.ssd_utils.DefaultBoxes</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/losses/ssd_loss.html#SSDLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.losses.SSDLoss" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.loss._Loss</span></code></p>
<p>Implements the loss as the sum of the followings:
1. Confidence Loss: All labels, with hard negative mining
2. Localization Loss: Only on positive labels</p>
<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.losses.SSDLoss.match_dboxes">
<span class="sig-name descname"><span class="pre">match_dboxes</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">targets</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/losses/ssd_loss.html#SSDLoss.match_dboxes"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.losses.SSDLoss.match_dboxes" title="Permalink to this definition"></a></dt>
<dd><p>convert ground truth boxes into a tensor with the same size as dboxes. each gt bbox is matched to every
destination box which overlaps it over 0.5 (IoU). so some gt bboxes can be duplicated to a few destination boxes
:param targets: a tensor containing the boxes for a single image. shape [num_boxes, 5] (x,y,w,h,label)
:return: two tensors</p>
<blockquote>
<div><p>boxes - shape of dboxes [4, num_dboxes] (x,y,w,h)
labels - sahpe [num_dboxes]</p>
</div></blockquote>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.losses.SSDLoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">predictions</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">targets</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/losses/ssd_loss.html#SSDLoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.losses.SSDLoss.forward" title="Permalink to this definition"></a></dt>
<dd><dl class="simple">
<dt>Compute the loss</dt><dd><p>:param predictions - predictions tensor coming from the network. shape [N, num_classes+4, num_dboxes]
were the first four items are (x,y,w,h) and the rest are class confidence
:param targets - targets for the batch. [num targets, 6] (index in batch, label, x,y,w,h)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="super_gradients.training.losses.SSDLoss.reduction">
<span class="sig-name descname"><span class="pre">reduction</span></span><em class="property"><span class="pre">:</span> <span class="pre">str</span></em><a class="headerlink" href="#super_gradients.training.losses.SSDLoss.reduction" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.losses.BCEDiceLoss">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.losses.</span></span><span class="sig-name descname"><span class="pre">BCEDiceLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss_weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[0.5,</span> <span class="pre">0.5]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/losses/bce_dice_loss.html#BCEDiceLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.losses.BCEDiceLoss" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Binary Cross Entropy + Dice Loss</p>
<p>Weighted average of BCE and Dice loss</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="super_gradients.training.losses.BCEDiceLoss.loss_weights">
<span class="sig-name descname"><span class="pre">loss_weights</span></span><a class="headerlink" href="#super_gradients.training.losses.BCEDiceLoss.loss_weights" title="Permalink to this definition"></a></dt>
<dd><p>list of size 2 s.t loss_weights[0], loss_weights[1] are the weights for BCE, Dice</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">respectively.</span></span></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.losses.BCEDiceLoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">torch.Tensor</span><a class="reference internal" href="_modules/super_gradients/training/losses/bce_dice_loss.html#BCEDiceLoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.losses.BCEDiceLoss.forward" title="Permalink to this definition"></a></dt>
<dd><p>&#64;param input: Network’s raw output shaped (N,1,H,W)
&#64;param target: Ground truth shaped (N,H,W)</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="super_gradients.training.losses.BCEDiceLoss.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#super_gradients.training.losses.BCEDiceLoss.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-super_gradients.training.metrics">
<span id="super-gradients-training-metrics-module"></span><h2>super_gradients.training.metrics module<a class="headerlink" href="#module-super_gradients.training.metrics" title="Permalink to this headline"></a></h2>
</section>
<section id="module-super_gradients.training.models">
<span id="super-gradients-training-models-module"></span><h2>super_gradients.training.models module<a class="headerlink" href="#module-super_gradients.training.models" title="Permalink to this headline"></a></h2>
</section>
<section id="module-super_gradients.training.sg_model">
<span id="super-gradients-training-sg-model-module"></span><h2>super_gradients.training.sg_model module<a class="headerlink" href="#module-super_gradients.training.sg_model" title="Permalink to this headline"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.sg_model.SgModel">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.sg_model.</span></span><span class="sig-name descname"><span class="pre">SgModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">experiment_name:</span> <span class="pre">str</span></em>, <em class="sig-param"><span class="pre">device:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">None</span></em>, <em class="sig-param"><span class="pre">multi_gpu:</span> <span class="pre">Union[super_gradients.training.sg_model.sg_model.MultiGPUMode</span></em>, <em class="sig-param"><span class="pre">str]</span> <span class="pre">=</span> <span class="pre">&lt;MultiGPUMode.OFF:</span> <span class="pre">'Off'&gt;</span></em>, <em class="sig-param"><span class="pre">model_checkpoints_location:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'local'</span></em>, <em class="sig-param"><span class="pre">overwrite_local_checkpoint:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">True</span></em>, <em class="sig-param"><span class="pre">ckpt_name:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'ckpt_latest.pth'</span></em>, <em class="sig-param"><span class="pre">post_prediction_callback:</span> <span class="pre">Optional[super_gradients.training.utils.detection_utils.DetectionPostPredictionCallback]</span> <span class="pre">=</span> <span class="pre">None</span></em>, <em class="sig-param"><span class="pre">ckpt_root_dir=None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.sg_model.SgModel" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>SuperGradient Model - Base Class for Sg Models</p>
<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.sg_model.SgModel.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">max_epochs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_epoch</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.train"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.sg_model.SgModel.train" title="Permalink to this definition"></a></dt>
<dd><p>the main function used for the training, h.p. updating, logging etc.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.sg_model.SgModel.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">idx</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.sg_model.SgModel.predict" title="Permalink to this definition"></a></dt>
<dd><p>returns the predictions and label of the current inputs</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">test(epoch</span> <span class="pre">:</span> <span class="pre">int,</span> <span class="pre">idx</span> <span class="pre">:</span> <span class="pre">int,</span> <span class="pre">save</span> <span class="pre">:</span> <span class="pre">bool):</span></span></dt>
<dd><p>returns the test loss, accuracy and runtime</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.sg_model.SgModel.connect_dataset_interface">
<span class="sig-name descname"><span class="pre">connect_dataset_interface</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset_interface</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference internal" href="super_gradients.training.datasets.dataset_interfaces.html#super_gradients.training.datasets.dataset_interfaces.dataset_interface.DatasetInterface" title="super_gradients.training.datasets.dataset_interfaces.dataset_interface.DatasetInterface"><span class="pre">super_gradients.training.datasets.dataset_interfaces.dataset_interface.DatasetInterface</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">data_loader_num_workers</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">8</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.connect_dataset_interface"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.sg_model.SgModel.connect_dataset_interface" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset_interface</strong> – DatasetInterface object</p></li>
<li><p><strong>data_loader_num_workers</strong> – The number of threads to initialize the Data Loaders with
The dataset to be connected</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.sg_model.SgModel.build_model">
<span class="sig-name descname"><span class="pre">build_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">architecture</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">arch_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.build_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.sg_model.SgModel.build_model" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>architecture</strong> – Defines the network’s architecture from models/ALL_ARCHITECTURES</p></li>
<li><p><strong>arch_params</strong> – Architecture H.P. e.g.: block, num_blocks, num_classes, etc.</p></li>
<li><p><strong>checkpoint_params</strong> – <p>Dictionary like object with the following key:values:</p>
<p>load_checkpoint:            Load a pre-trained checkpoint
strict_load:                See StrictLoad class documentation for details.
source_ckpt_folder_name:    folder name to load the checkpoint from (self.experiment_name if none is given)
load_weights_only:          loads only the weight from the checkpoint and zeroize the training params
load_backbone:              loads the provided checkpoint to self.net.backbone instead of self.net
external_checkpoint_path:   The path to the external checkpoint to be loaded. Can be absolute or relative</p>
<blockquote>
<div><p>(ie: path/to/checkpoint.pth). If provided, will automatically attempt to
load the checkpoint even if the load_checkpoint flag is not provided.</p>
</div></blockquote>
</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.sg_model.SgModel.backward_step">
<span class="sig-name descname"><span class="pre">backward_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">context</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference internal" href="super_gradients.training.utils.html#super_gradients.training.utils.callbacks.PhaseContext" title="super_gradients.training.utils.callbacks.PhaseContext"><span class="pre">super_gradients.training.utils.callbacks.PhaseContext</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.backward_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.sg_model.SgModel.backward_step" title="Permalink to this definition"></a></dt>
<dd><p>Run backprop on the loss and perform a step
:param loss: The value computed by the loss function
:param optimizer: An object that can perform a gradient step and zeroize model gradient
:param epoch: number of epoch the training is on
:param batch_idx: number of iteration inside the current epoch
:param context: current phase context
:return:</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.sg_model.SgModel.save_checkpoint">
<span class="sig-name descname"><span class="pre">save_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_results_tuple</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">tuple</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">context</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="super_gradients.training.utils.html#super_gradients.training.utils.callbacks.PhaseContext" title="super_gradients.training.utils.callbacks.PhaseContext"><span class="pre">super_gradients.training.utils.callbacks.PhaseContext</span></a><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.save_checkpoint"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.sg_model.SgModel.save_checkpoint" title="Permalink to this definition"></a></dt>
<dd><p>Save the current state dict as latest (always), best (if metric was improved), epoch# (if determined in training
params)</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id4">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">training_params</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">dict</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">{}</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.train"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#id4" title="Permalink to this definition"></a></dt>
<dd><p>train - Trains the Model</p>
<dl>
<dt>IMPORTANT NOTE: Additional batch parameters can be added as a third item (optional) if a tuple is returned by</dt><dd><p>the data loaders, as dictionary. The phase context will hold the additional items, under an attribute with
the same name as the key in this dictionary. Then such items can be accessed through phase callbacks.</p>
<blockquote>
<div><dl class="field-list">
<dt class="field-odd">param training_params</dt>
<dd class="field-odd"><ul>
<li><p><cite>max_epochs</cite> : int</p>
<blockquote>
<div><p>Number of epochs to run training.</p>
</div></blockquote>
</li>
<li><p><cite>lr_updates</cite> : list(int)</p>
<blockquote>
<div><p>List of fixed epoch numbers to perform learning rate updates when <cite>lr_mode=’step’</cite>.</p>
</div></blockquote>
</li>
<li><p><cite>lr_decay_factor</cite> : float</p>
<blockquote>
<div><p>Decay factor to apply to the learning rate at each update when <cite>lr_mode=’step’</cite>.</p>
</div></blockquote>
</li>
<li><p><cite>lr_mode</cite> : str</p>
<blockquote>
<div><p>Learning rate scheduling policy, one of [‘step’,’poly’,’cosine’,’function’]. ‘step’ refers to
constant updates at epoch numbers passed through <cite>lr_updates</cite>. ‘cosine’ refers to Cosine Anealing
policy as mentioned in <a class="reference external" href="https://arxiv.org/abs/1608.03983">https://arxiv.org/abs/1608.03983</a>. ‘poly’ refers to polynomial decrease i.e
in each epoch iteration <cite>self.lr = self.initial_lr * pow((1.0 - (current_iter / max_iter)),
0.9)</cite> ‘function’ refers to user defined learning rate scheduling function, that is passed through
<cite>lr_schedule_function</cite>.</p>
</div></blockquote>
</li>
<li><p><cite>lr_schedule_function</cite> : Union[callable,None]</p>
<blockquote>
<div><p>Learning rate scheduling function to be used when <cite>lr_mode</cite> is ‘function’.</p>
</div></blockquote>
</li>
<li><p><cite>lr_warmup_epochs</cite> : int (default=0)</p>
<blockquote>
<div><p>Number of epochs for learning rate warm up - see <a class="reference external" href="https://arxiv.org/pdf/1706.02677.pdf">https://arxiv.org/pdf/1706.02677.pdf</a> (Section 2.2).</p>
</div></blockquote>
</li>
<li><dl class="simple">
<dt><cite>cosine_final_lr_ratio</cite><span class="classifier">float (default=0.01)</span></dt><dd><dl class="simple">
<dt>Final learning rate ratio (only relevant when <a href="#id5"><span class="problematic" id="id6">`</span></a>lr_mode`=’cosine’). The cosine starts from initial_lr and reaches</dt><dd><p>initial_lr * cosine_final_lr_ratio in last epoch</p>
</dd>
</dl>
</dd>
</dl>
</li>
<li><p><cite>inital_lr</cite> : float</p>
<blockquote>
<div><p>Initial learning rate.</p>
</div></blockquote>
</li>
<li><p><cite>loss</cite> : Union[nn.module, str]</p>
<blockquote>
<div><p>Loss function for training.
One of SuperGradient’s built in options:</p>
<blockquote>
<div><p>“cross_entropy”: LabelSmoothingCrossEntropyLoss,
“mse”: MSELoss,
“r_squared_loss”: RSquaredLoss,
“detection_loss”: YoLoV3DetectionLoss,
“shelfnet_ohem_loss”: ShelfNetOHEMLoss,
“shelfnet_se_loss”: ShelfNetSemanticEncodingLoss,
“yolo_v5_loss”: YoLoV5DetectionLoss,
“ssd_loss”: SSDLoss,</p>
</div></blockquote>
<p>or user defined nn.module loss function.</p>
<p>IMPORTANT: forward(…) should return a (loss, loss_items) tuple where loss is the tensor used
for backprop (i.e what your original loss function returns), and loss_items should be a tensor of
shape (n_items), of values computed during the forward pass which we desire to log over the
entire epoch. For example- the loss itself should always be logged. Another example is a scenario
where the computed loss is the sum of a few components we would like to log- these entries in
loss_items).</p>
<p>When training, set the loss_logging_items_names parameter in train_params to be a list of
strings, of length n_items who’s ith element is the name of the ith entry in loss_items. Then
each item will be logged, rendered on tensorboard and “watched” (i.e saving model checkpoints
according to it).</p>
<p>Since running logs will save the loss_items in some internal state, it is recommended that
loss_items are detached from their computational graph for memory efficiency.</p>
</div></blockquote>
</li>
<li><p><cite>optimizer</cite> : Union[str, torch.optim.Optimizer]</p>
<blockquote>
<div><p>Optimization algorithm. One of [‘Adam’,’SGD’,’RMSProp’] corresponding to the torch.optim
optimzers implementations, or any object that implements torch.optim.Optimizer.</p>
</div></blockquote>
</li>
<li><p><cite>criterion_params</cite> : dict</p>
<blockquote>
<div><p>Loss function parameters.</p>
</div></blockquote>
</li>
<li><dl>
<dt><cite>optimizer_params</cite><span class="classifier">dict</span></dt><dd><p>When <cite>optimizer</cite> is one of [‘Adam’,’SGD’,’RMSProp’], it will be initialized with optimizer_params.</p>
<p>(see <a class="reference external" href="https://pytorch.org/docs/stable/optim.html">https://pytorch.org/docs/stable/optim.html</a> for the full list of
parameters for each optimizer).</p>
</dd>
</dl>
</li>
<li><p><cite>train_metrics_list</cite> : list(torchmetrics.Metric)</p>
<blockquote>
<div><p>Metrics to log during training. For more information on torchmetrics see
<a class="reference external" href="https://torchmetrics.rtfd.io/en/latest/">https://torchmetrics.rtfd.io/en/latest/</a>.</p>
</div></blockquote>
</li>
<li><p><cite>valid_metrics_list</cite> : list(torchmetrics.Metric)</p>
<blockquote>
<div><p>Metrics to log during validation/testing. For more information on torchmetrics see
<a class="reference external" href="https://torchmetrics.rtfd.io/en/latest/">https://torchmetrics.rtfd.io/en/latest/</a>.</p>
</div></blockquote>
</li>
<li><p><cite>loss_logging_items_names</cite> : list(str)</p>
<blockquote>
<div><p>The list of names/titles for the outputs returned from the loss functions forward pass (reminder-
the loss function should return the tuple (loss, loss_items)). These names will be used for
logging their values.</p>
</div></blockquote>
</li>
<li><p><cite>metric_to_watch</cite> : str (default=”Accuracy”)</p>
<blockquote>
<div><p>will be the metric which the model checkpoint will be saved according to, and can be set to any
of the following:</p>
<blockquote>
<div><p>a metric name (str) of one of the metric objects from the valid_metrics_list</p>
<p>a “metric_name” if some metric in valid_metrics_list has an attribute component_names which
is a list referring to the names of each entry in the output metric (torch tensor of size n)</p>
<p>one of “loss_logging_items_names” i.e which will correspond to an item returned during the
loss function’s forward pass.</p>
</div></blockquote>
<p>At the end of each epoch, if a new best metric_to_watch value is achieved, the models checkpoint
is saved in YOUR_PYTHON_PATH/checkpoints/ckpt_best.pth</p>
</div></blockquote>
</li>
<li><p><cite>greater_metric_to_watch_is_better</cite> : bool</p>
<blockquote>
<div><dl class="simple">
<dt>When choosing a model’s checkpoint to be saved, the best achieved model is the one that maximizes the</dt><dd><p>metric_to_watch when this parameter is set to True, and a one that minimizes it otherwise.</p>
</dd>
</dl>
</div></blockquote>
</li>
<li><p><cite>ema</cite> : bool (default=False)</p>
<blockquote>
<div><p>Whether to use Model Exponential Moving Average (see
<a class="reference external" href="https://github.com/rwightman/pytorch-image-models">https://github.com/rwightman/pytorch-image-models</a> ema implementation)</p>
</div></blockquote>
</li>
<li><p><cite>batch_accumulate</cite> : int (default=1)</p>
<blockquote>
<div><p>Number of batches to accumulate before every backward pass.</p>
</div></blockquote>
</li>
<li><p><cite>ema_params</cite> : dict</p>
<blockquote>
<div><p>Parameters for the ema model.</p>
</div></blockquote>
</li>
<li><p><cite>zero_weight_decay_on_bias_and_bn</cite> : bool (default=False)</p>
<blockquote>
<div><p>Whether to apply weight decay on batch normalization parameters or not (ignored when the passed
optimizer has already been initialized).</p>
</div></blockquote>
</li>
<li><p><cite>load_opt_params</cite> : bool (default=True)</p>
<blockquote>
<div><p>Whether to load the optimizers parameters as well when loading a model’s checkpoint.</p>
</div></blockquote>
</li>
<li><p><cite>run_validation_freq</cite> : int (default=1)</p>
<blockquote>
<div><dl class="simple">
<dt>The frequency in which validation is performed during training (i.e the validation is ran every</dt><dd><p><cite>run_validation_freq</cite> epochs.</p>
</dd>
</dl>
</div></blockquote>
</li>
<li><p><cite>save_model</cite> : bool (default=True)</p>
<blockquote>
<div><p>Whether to save the model checkpoints.</p>
</div></blockquote>
</li>
<li><p><cite>silent_mode</cite> : bool</p>
<blockquote>
<div><p>Silents the print outs.</p>
</div></blockquote>
</li>
<li><p><cite>mixed_precision</cite> : bool</p>
<blockquote>
<div><p>Whether to use mixed precision or not.</p>
</div></blockquote>
</li>
<li><p><cite>save_ckpt_epoch_list</cite> : list(int) (default=[])</p>
<blockquote>
<div><p>List of fixed epoch indices the user wishes to save checkpoints in.</p>
</div></blockquote>
</li>
<li><p><cite>average_best_models</cite> : bool (default=False)</p>
<blockquote>
<div><p>If set, a snapshot dictionary file and the average model will be saved / updated at every epoch
and evaluated only when training is completed. The snapshot file will only be deleted upon
completing the training. The snapshot dict will be managed on cpu.</p>
</div></blockquote>
</li>
<li><p><cite>precise_bn</cite> : bool (default=False)</p>
<blockquote>
<div><p>Whether to use precise_bn calculation during the training.</p>
</div></blockquote>
</li>
<li><p><cite>precise_bn_batch_size</cite> : int (default=None)</p>
<blockquote>
<div><p>The effective batch size we want to calculate the batchnorm on. For example, if we are training a model
on 8 gpus, with a batch of 128 on each gpu, a good rule of thumb would be to give it 8192
(ie: effective_batch_size * num_gpus = batch_per_gpu * num_gpus * num_gpus).
If precise_bn_batch_size is not provided in the training_params, the latter heuristic will be taken.</p>
</div></blockquote>
</li>
<li><p><cite>seed</cite> : int (default=42)</p>
<blockquote>
<div><p>Random seed to be set for torch, numpy, and random. When using DDP each process will have it’s seed
set to seed + rank.</p>
</div></blockquote>
</li>
<li><p><cite>log_installed_packages</cite> : bool (default=False)</p>
<blockquote>
<div><dl class="simple">
<dt>When set, the list of all installed packages (and their versions) will be written to the tensorboard</dt><dd><p>and logfile (useful when trying to reproduce results).</p>
</dd>
</dl>
</div></blockquote>
</li>
<li><p><cite>dataset_statistics</cite> : bool (default=False)</p>
<blockquote>
<div><p>Enable a statistic analysis of the dataset. If set to True the dataset will be analyzed and a report
will be added to the tensorboard along with some sample images from the dataset. Currently only
detection datasets are supported for analysis.</p>
</div></blockquote>
</li>
<li><p><cite>save_full_train_log</cite> : bool (default=False)</p>
<blockquote>
<div><dl class="simple">
<dt>When set, a full log (of all super_gradients modules, including uncaught exceptions from any other</dt><dd><p>module) of the training will be saved in the checkpoint directory under full_train_log.log</p>
</dd>
</dl>
</div></blockquote>
</li>
<li><p><cite>sg_logger</cite> : Union[AbstractSGLogger, str] (defauls=base_sg_logger)</p>
<blockquote>
<div><p>Define the SGLogger object for this training process. The SGLogger handles all disk writes, logs, TensorBoard, remote logging
and remote storage. By overriding the default base_sg_logger, you can change the storage location, support external monitoring and logging
or support remote storage.</p>
</div></blockquote>
</li>
<li><p><cite>sg_logger_params</cite> : dict</p>
<p>SGLogger parameters</p>
</li>
<li><p><cite>clip_grad_norm</cite> : float</p>
<p>Defines a maximal L2 norm of the gradients. Values which exceed the given value will be clipped</p>
</li>
</ul>
</dd>
</dl>
</div></blockquote>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id7">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">targets</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">half</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">move_outputs_to_cpu</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#id7" title="Permalink to this definition"></a></dt>
<dd><p>A fast predictor for a batch of inputs
:param inputs: torch.tensor or numpy.array</p>
<blockquote>
<div><p>a batch of inputs</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>targets</strong> – torch.tensor()
corresponding labels - if non are given - accuracy will not be computed</p></li>
<li><p><strong>verbose</strong> – bool
print the results to screen</p></li>
<li><p><strong>normalize</strong> – bool
If true, normalizes the tensor according to the dataloader’s normalization values</p></li>
<li><p><strong>half</strong> – Performs half precision evaluation</p></li>
<li><p><strong>move_outputs_to_cpu</strong> – Moves the results from the GPU to the CPU</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>outputs, acc, net_time, gross_time
networks predictions, accuracy calculation, forward pass net time, function gross time</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.sg_model.SgModel.compute_model_runtime">
<span class="sig-name descname"><span class="pre">compute_model_runtime</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dims</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">tuple</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_sizes</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">tuple</span><span class="p"><span class="pre">,</span> </span><span class="pre">list</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">(1,</span> <span class="pre">8,</span> <span class="pre">16,</span> <span class="pre">32,</span> <span class="pre">64)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.compute_model_runtime"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.sg_model.SgModel.compute_model_runtime" title="Permalink to this definition"></a></dt>
<dd><p>Compute the “atomic” inference time and throughput.
Atomic refers to calculating the forward pass independently, discarding effects such as data augmentation,
data upload to device, multi-gpu distribution etc.
:param input_dims: tuple</p>
<blockquote>
<div><p>shape of a basic input to the network (without the first index) e.g. (3, 224, 224)
if None uses an input from the test loader</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch_sizes</strong> – int or list
Batch sizes for latency calculation</p></li>
<li><p><strong>verbose</strong> – bool
Prints results to screen</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>log: dict
Latency and throughput for each tested batch size</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.sg_model.SgModel.get_arch_params">
<span class="sig-name descname"><span class="pre">get_arch_params</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.get_arch_params"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.sg_model.SgModel.get_arch_params" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.sg_model.SgModel.get_structure">
<span class="sig-name descname"><span class="pre">get_structure</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.get_structure"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.sg_model.SgModel.get_structure" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.sg_model.SgModel.get_architecture">
<span class="sig-name descname"><span class="pre">get_architecture</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.get_architecture"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.sg_model.SgModel.get_architecture" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.sg_model.SgModel.set_experiment_name">
<span class="sig-name descname"><span class="pre">set_experiment_name</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">experiment_name</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.set_experiment_name"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.sg_model.SgModel.set_experiment_name" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.sg_model.SgModel.re_build_model">
<span class="sig-name descname"><span class="pre">re_build_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">arch_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.re_build_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.sg_model.SgModel.re_build_model" title="Permalink to this definition"></a></dt>
<dd><dl class="simple">
<dt>arch_params<span class="classifier">dict</span></dt><dd><p>Architecture H.P. e.g.: block, num_blocks, num_classes, etc.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.sg_model.SgModel.update_architecture">
<span class="sig-name descname"><span class="pre">update_architecture</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">structure</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.update_architecture"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.sg_model.SgModel.update_architecture" title="Permalink to this definition"></a></dt>
<dd><dl class="simple">
<dt>architecture<span class="classifier">str</span></dt><dd><p>Defines the network’s architecture according to the options in models/all_architectures</p>
</dd>
<dt>load_checkpoint<span class="classifier">bool</span></dt><dd><p>Loads a checkpoint according to experiment_name</p>
</dd>
<dt>arch_params<span class="classifier">dict</span></dt><dd><p>Architecture H.P. e.g.: block, num_blocks, num_classes, etc.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.sg_model.SgModel.get_module">
<span class="sig-name descname"><span class="pre">get_module</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.get_module"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.sg_model.SgModel.get_module" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.sg_model.SgModel.set_module">
<span class="sig-name descname"><span class="pre">set_module</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.set_module"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.sg_model.SgModel.set_module" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.sg_model.SgModel.test">
<span class="sig-name descname"><span class="pre">test</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">test_loader</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.utils.data.dataloader.DataLoader</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.loss._Loss</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">silent_mode</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_metrics_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_logging_items_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metrics_progress_verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_phase_callbacks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_ema_net</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">tuple</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.test"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.sg_model.SgModel.test" title="Permalink to this definition"></a></dt>
<dd><p>Evaluates the model on given dataloader and metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>test_loader</strong> – dataloader to perform test on.</p></li>
<li><p><strong>test_metrics_list</strong> – (list(torchmetrics.Metric)) metrics list for evaluation.</p></li>
<li><p><strong>silent_mode</strong> – (bool) controls verbosity</p></li>
<li><p><strong>metrics_progress_verbose</strong> – (bool) controls the verbosity of metrics progress (default=False). Slows down the program.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>:param use_ema_net (bool) whether to perform test on self.ema_model.ema (when self.ema_model.ema exists,</dt><dd><p>otherwise self.net will be tested) (default=True)</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>results tuple (tuple) containing the loss items and metric values.</p>
</dd>
</dl>
<dl class="simple">
<dt>All of the above args will override SgModel’s corresponding attribute when not equal to None. Then evaluation</dt><dd><p>is ran on self.test_loader with self.test_metrics.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.sg_model.SgModel.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_loader</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.utils.data.dataloader.DataLoader</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metrics</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torchmetrics.collections.MetricCollection</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluation_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference internal" href="super_gradients.training.sg_model.html#super_gradients.training.sg_model.sg_model.EvaluationType" title="super_gradients.training.sg_model.sg_model.EvaluationType"><span class="pre">super_gradients.training.sg_model.sg_model.EvaluationType</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">silent_mode</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metrics_progress_verbose</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.evaluate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.sg_model.SgModel.evaluate" title="Permalink to this definition"></a></dt>
<dd><p>Evaluates the model on given dataloader and metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data_loader</strong> – dataloader to perform evaluataion on</p></li>
<li><p><strong>metrics</strong> – (MetricCollection) metrics for evaluation</p></li>
<li><p><strong>evaluation_type</strong> – (EvaluationType) controls which phase callbacks will be used (for example, on batch end,
when evaluation_type=EvaluationType.VALIDATION the Phase.VALIDATION_BATCH_END callbacks will be triggered)</p></li>
<li><p><strong>epoch</strong> – (int) epoch idx</p></li>
<li><p><strong>silent_mode</strong> – (bool) controls verbosity</p></li>
<li><p><strong>metrics_progress_verbose</strong> – (bool) controls the verbosity of metrics progress (default=False).
Slows down the program significantly.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>results tuple (tuple) containing the loss items and metric values.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.sg_model.SgModel.instantiate_net">
<span class="sig-name descname"><span class="pre">instantiate_net</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">architecture</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">,</span> </span><span class="pre">type</span><span class="p"><span class="pre">,</span> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">arch_params</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_params</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">tuple</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#SgModel.instantiate_net"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.sg_model.SgModel.instantiate_net" title="Permalink to this definition"></a></dt>
<dd><dl class="simple">
<dt>Instantiates nn.Module according to architecture and arch_params, and handles pretrained weights and the required</dt><dd><p>module manipulation (i.e head replacement).</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>architecture</strong> – String, torch.nn.Module or uninstantiated SgModule class describing the netowrks architecture.</p></li>
<li><p><strong>arch_params</strong> – Architecture’s parameters passed to networks c’tor.</p></li>
<li><p><strong>checkpoint_params</strong> – checkpoint loading related parameters dictionary with ‘pretrained_weights’ key,
s.t it’s value is a string describing the dataset of the pretrained weights (for example “imagenent”).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>instantiated netowrk i.e torch.nn.Module, architecture_class (will be none when architecture is not str)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.sg_model.MultiGPUMode">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.sg_model.</span></span><span class="sig-name descname"><span class="pre">MultiGPUMode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#MultiGPUMode"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.sg_model.MultiGPUMode" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">enum.Enum</span></code></p>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">OFF</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">-</span> <span class="pre">Single</span> <span class="pre">GPU</span> <span class="pre">Mode</span> <span class="pre">/</span> <span class="pre">CPU</span> <span class="pre">Mode</span></span></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">DATA_PARALLEL</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">-</span> <span class="pre">Multiple</span> <span class="pre">GPUs,</span> <span class="pre">Synchronous</span></span></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">DISTRIBUTED_DATA_PARALLEL</span> <span class="pre">-</span> <span class="pre">Multiple</span> <span class="pre">GPUs,</span> <span class="pre">Asynchronous</span></span></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="super_gradients.training.sg_model.MultiGPUMode.OFF">
<span class="sig-name descname"><span class="pre">OFF</span></span><em class="property"> <span class="pre">=</span> <span class="pre">'Off'</span></em><a class="headerlink" href="#super_gradients.training.sg_model.MultiGPUMode.OFF" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="super_gradients.training.sg_model.MultiGPUMode.DATA_PARALLEL">
<span class="sig-name descname"><span class="pre">DATA_PARALLEL</span></span><em class="property"> <span class="pre">=</span> <span class="pre">'DP'</span></em><a class="headerlink" href="#super_gradients.training.sg_model.MultiGPUMode.DATA_PARALLEL" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="super_gradients.training.sg_model.MultiGPUMode.DISTRIBUTED_DATA_PARALLEL">
<span class="sig-name descname"><span class="pre">DISTRIBUTED_DATA_PARALLEL</span></span><em class="property"> <span class="pre">=</span> <span class="pre">'DDP'</span></em><a class="headerlink" href="#super_gradients.training.sg_model.MultiGPUMode.DISTRIBUTED_DATA_PARALLEL" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="super_gradients.training.sg_model.MultiGPUMode.AUTO">
<span class="sig-name descname"><span class="pre">AUTO</span></span><em class="property"> <span class="pre">=</span> <span class="pre">'AUTO'</span></em><a class="headerlink" href="#super_gradients.training.sg_model.MultiGPUMode.AUTO" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.sg_model.StrictLoad">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.sg_model.</span></span><span class="sig-name descname"><span class="pre">StrictLoad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/sg_model/sg_model.html#StrictLoad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.sg_model.StrictLoad" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">enum.Enum</span></code></p>
<dl>
<dt>Wrapper for adding more functionality to torch’s strict_load parameter in load_state_dict().</dt><dd><dl>
<dt>Attributes:</dt><dd><p>OFF              - Native torch “strict_load = off” behaviour. See nn.Module.load_state_dict() documentation for more details.
ON               - Native torch “strict_load = on” behaviour. See nn.Module.load_state_dict() documentation for more details.
NO_KEY_MATCHING  - Allows the usage of SuperGradient’s adapt_checkpoint function, which loads a checkpoint by matching each</p>
<blockquote>
<div><p>layer’s shapes (and bypasses the strict matching of the names of each layer (ie: disregards the state_dict key matching)).</p>
</div></blockquote>
</dd>
</dl>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="super_gradients.training.sg_model.StrictLoad.OFF">
<span class="sig-name descname"><span class="pre">OFF</span></span><em class="property"> <span class="pre">=</span> <span class="pre">False</span></em><a class="headerlink" href="#super_gradients.training.sg_model.StrictLoad.OFF" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="super_gradients.training.sg_model.StrictLoad.ON">
<span class="sig-name descname"><span class="pre">ON</span></span><em class="property"> <span class="pre">=</span> <span class="pre">True</span></em><a class="headerlink" href="#super_gradients.training.sg_model.StrictLoad.ON" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="super_gradients.training.sg_model.StrictLoad.NO_KEY_MATCHING">
<span class="sig-name descname"><span class="pre">NO_KEY_MATCHING</span></span><em class="property"> <span class="pre">=</span> <span class="pre">'no_key_matching'</span></em><a class="headerlink" href="#super_gradients.training.sg_model.StrictLoad.NO_KEY_MATCHING" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-super_gradients.training.utils">
<span id="super-gradients-training-utils-module"></span><h2>super_gradients.training.utils module<a class="headerlink" href="#module-super_gradients.training.utils" title="Permalink to this headline"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.utils.Timer">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.utils.</span></span><span class="sig-name descname"><span class="pre">Timer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/utils/utils.html#Timer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.utils.Timer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>A class to measure time handling both GPU &amp; CPU processes
Returns time in milliseconds</p>
<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.utils.Timer.start">
<span class="sig-name descname"><span class="pre">start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/utils/utils.html#Timer.start"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.utils.Timer.start" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.utils.Timer.stop">
<span class="sig-name descname"><span class="pre">stop</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/utils/utils.html#Timer.stop"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.utils.Timer.stop" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.utils.HpmStruct">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.utils.</span></span><span class="sig-name descname"><span class="pre">HpmStruct</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">entries</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/utils/utils.html#HpmStruct"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.utils.HpmStruct" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.utils.HpmStruct.set_schema">
<span class="sig-name descname"><span class="pre">set_schema</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">schema</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">dict</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/utils/utils.html#HpmStruct.set_schema"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.utils.HpmStruct.set_schema" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.utils.HpmStruct.override">
<span class="sig-name descname"><span class="pre">override</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">entries</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/utils/utils.html#HpmStruct.override"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.utils.HpmStruct.override" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.utils.HpmStruct.to_dict">
<span class="sig-name descname"><span class="pre">to_dict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/utils/utils.html#HpmStruct.to_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.utils.HpmStruct.to_dict" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.utils.HpmStruct.validate">
<span class="sig-name descname"><span class="pre">validate</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/utils/utils.html#HpmStruct.validate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.utils.HpmStruct.validate" title="Permalink to this definition"></a></dt>
<dd><p>Validate the current dict values according to the provided schema
:raises</p>
<blockquote>
<div><p><cite>AttributeError</cite> if schema was not set
<cite>jsonschema.exceptions.ValidationError</cite> if the instance is invalid
<cite>jsonschema.exceptions.SchemaError</cite> if the schema itselfis invalid</p>
</div></blockquote>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="super_gradients.training.utils.WrappedModel">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">super_gradients.training.utils.</span></span><span class="sig-name descname"><span class="pre">WrappedModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/utils/utils.html#WrappedModel"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.utils.WrappedModel" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="super_gradients.training.utils.WrappedModel.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/utils/utils.html#WrappedModel.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.utils.WrappedModel.forward" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="super_gradients.training.utils.WrappedModel.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#super_gradients.training.utils.WrappedModel.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="super_gradients.training.utils.convert_to_tensor">
<span class="sig-prename descclassname"><span class="pre">super_gradients.training.utils.</span></span><span class="sig-name descname"><span class="pre">convert_to_tensor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">array</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/utils/utils.html#convert_to_tensor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.utils.convert_to_tensor" title="Permalink to this definition"></a></dt>
<dd><p>Converts numpy arrays and lists to Torch tensors before calculation losses
:param array: torch.tensor / Numpy array / List</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="super_gradients.training.utils.get_param">
<span class="sig-prename descclassname"><span class="pre">super_gradients.training.utils.</span></span><span class="sig-name descname"><span class="pre">get_param</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">default_val</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/utils/utils.html#get_param"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.utils.get_param" title="Permalink to this definition"></a></dt>
<dd><p>Retrieves a param from a parameter object/dict. If the parameter does not exist, will return default_val.
In case the default_val is of type dictionary, and a value is found in the params - the function
will return the default value dictionary with internal values overridden by the found value</p>
<p>i.e.
default_opt_params = {‘lr’:0.1, ‘momentum’:0.99, ‘alpha’:0.001}
training_params = {‘optimizer_params’: {‘lr’:0.0001}, ‘batch’: 32 …. }
get_param(training_params, name=’optimizer_params’, default_val=default_opt_params)
will return {‘lr’:0.0001, ‘momentum’:0.99, ‘alpha’:0.001}</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> – an object (typically HpmStruct) or a dict holding the params</p></li>
<li><p><strong>name</strong> – name of the searched parameter</p></li>
<li><p><strong>default_val</strong> – assumed to be the same type as the value searched in the params</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the found value, or default if not found</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="super_gradients.training.utils.tensor_container_to_device">
<span class="sig-prename descclassname"><span class="pre">super_gradients.training.utils.</span></span><span class="sig-name descname"><span class="pre">tensor_container_to_device</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obj</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span> </span><span class="pre">tuple</span><span class="p"><span class="pre">,</span> </span><span class="pre">list</span><span class="p"><span class="pre">,</span> </span><span class="pre">dict</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/utils/utils.html#tensor_container_to_device"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.utils.tensor_container_to_device" title="Permalink to this definition"></a></dt>
<dd><dl>
<dt>recursively send compounded objects to device (sending all tensors to device and maintaining structure)</dt><dd><p>:param obj           the object to send to device (list / tuple / tensor / dict)
:param device:       device to send the tensors to
:param non_blocking: used for DistributedDataParallel
:returns        an object with the same structure (tensors, lists, tuples) with the device pointers (like</p>
<blockquote>
<div><p>the return value of Tensor.to(device)</p>
</div></blockquote>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="super_gradients.training.utils.adapt_state_dict_to_fit_model_layer_names">
<span class="sig-prename descclassname"><span class="pre">super_gradients.training.utils.</span></span><span class="sig-name descname"><span class="pre">adapt_state_dict_to_fit_model_layer_names</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_state_dict</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">source_ckpt</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exclude</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">list</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">[]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/utils/checkpoint_utils.html#adapt_state_dict_to_fit_model_layer_names"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.utils.adapt_state_dict_to_fit_model_layer_names" title="Permalink to this definition"></a></dt>
<dd><p>Given a model state dict and source checkpoints, the method tries to correct the keys in the model_state_dict to fit
the ckpt in order to properly load the weights into the model. If unsuccessful - returns None</p>
<blockquote>
<div><dl class="field-list simple">
<dt class="field-odd">param model_state_dict</dt>
<dd class="field-odd"><p>the model state_dict</p>
</dd>
<dt class="field-even">param source_ckpt</dt>
<dd class="field-even"><p>checkpoint dict</p>
</dd>
</dl>
<p>:exclude                  optional list for excluded layers
:return: renamed checkpoint dict (if possible)</p>
</div></blockquote>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="super_gradients.training.utils.raise_informative_runtime_error">
<span class="sig-prename descclassname"><span class="pre">super_gradients.training.utils.</span></span><span class="sig-name descname"><span class="pre">raise_informative_runtime_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exception_msg</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/utils/checkpoint_utils.html#raise_informative_runtime_error"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.utils.raise_informative_runtime_error" title="Permalink to this definition"></a></dt>
<dd><p>Given a model state dict and source checkpoints, the method calls “adapt_state_dict_to_fit_model_layer_names”
and enhances the exception_msg if loading the checkpoint_dict via the conversion method is possible</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="super_gradients.training.utils.random_seed">
<span class="sig-prename descclassname"><span class="pre">super_gradients.training.utils.</span></span><span class="sig-name descname"><span class="pre">random_seed</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">is_ddp</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/super_gradients/training/utils/utils.html#random_seed"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#super_gradients.training.utils.random_seed" title="Permalink to this definition"></a></dt>
<dd><p>Sets random seed of numpy, torch and random.</p>
<p>When using ddp a seed will be set for each process according to its local rank derived from the device number.
:param is_ddp: bool, will set different random seed for each process when using ddp.
:param device: ‘cuda’,’cpu’, ‘cuda:&lt;device_number&gt;’
:param seed: int, random seed to be set</p>
</dd></dl>

</section>
<section id="module-contents">
<h2>Module contents<a class="headerlink" href="#module-contents" title="Permalink to this headline"></a></h2>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="super_gradients.common.html" class="btn btn-neutral float-left" title="Common package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="user_guide.html" class="btn btn-neutral float-right" title="What is SuperGradients?" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, SuperGradients team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>