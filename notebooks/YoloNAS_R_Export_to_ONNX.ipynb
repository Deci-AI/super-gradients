{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# This tutorial shows how to export YoloNAS-R model to ONNX\n",
    "\n",
    "From this tutorial you will learn:\n",
    "\n",
    "* How to export YoloNAR-R Oriented Bounding Box (OBB) Detection model to ONNX\n",
    "* How to enable FP16 / INT8 quantization and export a model with calibration\n",
    "* How to customize NMS parameters and number of detections per image"
   ],
   "metadata": {
    "collapsed": false,
    "id": "tpvvI6z8G7bK",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Currently supported models\n",
    "\n",
    "- YoloNAS-R\n",
    "\n",
    "### Supported features\n",
    "\n",
    "- Exporting a model with preprocessing (e.g. normalizing/standardizing image according to normalization parameters during training)\n",
    "- Exporting a model with postprocessing (e.g. predictions decoding and NMS) - you obtain the ready-to-consume bounding box outputs\n",
    "- FP16 / INT8 quantization support with calibration\n",
    "- Pre- and post-processing steps can be customized by the user if needed\n",
    "- Customising input image shape and batch size\n",
    "- Customising NMS parameters and number of detections per image\n",
    "- Customising output format (flat or batched)"
   ],
   "metadata": {
    "collapsed": false,
    "id": "ykoGiEtLG7bV",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -qq super_gradients==3.7.1"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Je-_9KmG7bV",
    "outputId": "45c8a021-156e-4c1b-e4ad-db5b4688212c",
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-05-10T13:54:02.852577Z",
     "start_time": "2024-05-10T13:53:50.513676Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Minimalistic export example\n",
    "\n",
    "Let start with the most simple example of exporting a model to ONNX format.\n",
    "We will use YoloNAS-R Small model in this example. All models that suports new export API now expose a `export()` method that can be used to export a model. There is one mandatory argument that should be passed to the `export()` method - the path to the output file. Currently, only `.onnx` format is supported, but we may add support for CoreML and other formats in the future."
   ],
   "metadata": {
    "collapsed": false,
    "id": "h0EWfAetG7bW",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from super_gradients.training.processing.defaults import default_yolo_nas_r_dota_processing_params\n",
    "from super_gradients.common.object_names import Models\n",
    "from super_gradients.training import models\n",
    "\n",
    "model = models.get(Models.YOLO_NAS_R_S,\n",
    "                   checkpoint_path=\"C:/Develop/GitHub/Deci/super-gradients/checkpoints/dota_yolo_nas_r_s/ckpt_best.pth\",\n",
    "                   num_classes=18)\n",
    "model.set_dataset_processing_params(**default_yolo_nas_r_dota_processing_params())\n",
    "\n",
    "export_result = model.export(\"yolo_nas_r_s.onnx\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vVSVkf8oG7bW",
    "outputId": "cc3d33e0-1c11-4b6f-ff95-e2aa5ed594a4",
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-05-10T13:54:19.517560Z",
     "start_time": "2024-05-10T13:54:02.855604Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [1000] at index 0 does not match the shape of the indexed tensor [1, 1000, 5] at index 0",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 10\u001B[0m\n\u001B[0;32m      5\u001B[0m model \u001B[38;5;241m=\u001B[39m models\u001B[38;5;241m.\u001B[39mget(Models\u001B[38;5;241m.\u001B[39mYOLO_NAS_R_S,\n\u001B[0;32m      6\u001B[0m                    checkpoint_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mC:/Develop/GitHub/Deci/super-gradients/checkpoints/dota_yolo_nas_r_s/ckpt_best.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      7\u001B[0m                    num_classes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m18\u001B[39m)\n\u001B[0;32m      8\u001B[0m model\u001B[38;5;241m.\u001B[39mset_dataset_processing_params(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdefault_yolo_nas_r_dota_processing_params())\n\u001B[1;32m---> 10\u001B[0m export_result \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexport\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43myolo_nas_r_s.onnx\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Develop\\GitHub\\Deci\\super-gradients\\src\\super_gradients\\module_interfaces\\exportable_obb_detector.py:461\u001B[0m, in \u001B[0;36mExportableOBBDetectionModel.export\u001B[1;34m(self, output, confidence_threshold, nms_threshold, quantization_mode, selective_quantizer, calibration_loader, calibration_method, calibration_batches, calibration_percentile, preprocessing, postprocessing, postprocessing_kwargs, batch_size, input_image_shape, input_image_channels, input_image_dtype, max_predictions_per_image, onnx_export_kwargs, onnx_simplify, device, output_predictions_format, num_pre_nms_predictions)\u001B[0m\n\u001B[0;32m    458\u001B[0m onnx_export_kwargs \u001B[38;5;241m=\u001B[39m onnx_export_kwargs \u001B[38;5;129;01mor\u001B[39;00m {}\n\u001B[0;32m    459\u001B[0m onnx_input \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandn(input_shape)\u001B[38;5;241m.\u001B[39mto(device\u001B[38;5;241m=\u001B[39mdevice, dtype\u001B[38;5;241m=\u001B[39minput_image_dtype)\n\u001B[1;32m--> 461\u001B[0m \u001B[43mexport_to_onnx\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    462\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcomplete_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    463\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43monnx_input\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    464\u001B[0m \u001B[43m    \u001B[49m\u001B[43monnx_filename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    465\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    466\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    467\u001B[0m \u001B[43m    \u001B[49m\u001B[43monnx_opset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43monnx_export_kwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mopset_version\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    468\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdo_constant_folding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43monnx_export_kwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdo_constant_folding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    469\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdynamic_axes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdynamic_axes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    470\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkeep_initializers_as_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43monnx_export_kwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mkeep_initializers_as_inputs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    471\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43monnx_export_kwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mverbose\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    472\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    474\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m onnx_simplify:\n\u001B[0;32m    475\u001B[0m     model_opt, simplify_successful \u001B[38;5;241m=\u001B[39m onnxsim\u001B[38;5;241m.\u001B[39msimplify(output)\n",
      "File \u001B[1;32m~\\.conda\\envs\\kaggle\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\Develop\\GitHub\\Deci\\super-gradients\\src\\super_gradients\\conversion\\onnx\\export_to_onnx.py:57\u001B[0m, in \u001B[0;36mexport_to_onnx\u001B[1;34m(model, model_input, onnx_filename, input_names, output_names, onnx_opset, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, verbose)\u001B[0m\n\u001B[0;32m     54\u001B[0m         logger\u001B[38;5;241m.\u001B[39mwarning(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel buffer \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is on device \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mp\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m but expected to be on device \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdevice\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     56\u001B[0m \u001B[38;5;66;03m# Sanity check that model works\u001B[39;00m\n\u001B[1;32m---> 57\u001B[0m _ \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_input\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     59\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExporting model to ONNX\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     60\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mONNX input shape: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_input\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m with dtype: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_input\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\.conda\\envs\\kaggle\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\kaggle\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Develop\\GitHub\\Deci\\super-gradients\\src\\super_gradients\\training\\models\\conversion.py:56\u001B[0m, in \u001B[0;36mConvertableCompletePipelineModel.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m---> 56\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpost_process\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpre_process\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\kaggle\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\kaggle\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\kaggle\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    213\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    214\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 215\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\kaggle\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\kaggle\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Develop\\GitHub\\Deci\\super-gradients\\src\\super_gradients\\conversion\\onnx\\obb_nms.py:74\u001B[0m, in \u001B[0;36mOBBNMSAndReturnAsBatchedResult.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m     72\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_size):\n\u001B[0;32m     73\u001B[0m     keep_i \u001B[38;5;241m=\u001B[39m keep[i]\n\u001B[1;32m---> 74\u001B[0m     pred_boxes_i \u001B[38;5;241m=\u001B[39m \u001B[43mpred_boxes\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkeep_i\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m     75\u001B[0m     pred_scores_i \u001B[38;5;241m=\u001B[39m pred_cls_conf[keep_i]\n\u001B[0;32m     76\u001B[0m     pred_classes_i \u001B[38;5;241m=\u001B[39m pred_cls_labels[keep_i]\n",
      "\u001B[1;31mIndexError\u001B[0m: The shape of the mask [1000] at index 0 does not match the shape of the indexed tensor [1, 1000, 5] at index 0"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "A lot of work just happened under the hood:\n",
    "\n",
    "* A model was exported to ONNX format using default batch size of 1 and input image shape that was used during training\n",
    "* A preprocessing and postprocessing steps were attached to ONNX graph\n",
    "* For pre-processing step, the normalization parameters were extracted from the model itself (to be consistent with the image normalization and channel order used during training)\n",
    "* For post-processing step, the NMS parameters were also extracted from the model and NMS module was attached to the graph\n",
    "* ONNX graph was checked and simplified to improve compatibility with ONNX runtimes."
   ],
   "metadata": {
    "collapsed": false,
    "id": "ILppAMFZG7bW",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A returned value of `export()` method is an instance of `ModelExportResult` class.\n",
    "First of all it serves the purpose of storing all the information about the exported model in a single place.\n",
    "It also provides a convenient way to get an example of running the model and getting the output:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "YXl28XcWG7bX",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "export_result"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9JK-VsBHG7bX",
    "outputId": "153dff12-71c7-4086-9001-516b18cb498f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "That's it. You can now use the exported model with any ONNX-compatible runtime or accelerator.\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "_djOsc4FG7bX",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from super_gradients.training.utils.media.image import load_image\n",
    "import onnxruntime\n",
    "\n",
    "image = load_image(\"h:/DOTA/DOTA-v2.0/val/images/P1924.png\")\n",
    "image = cv2.resize(image, (export_result.input_image_shape[1], export_result.input_image_shape[0]))\n",
    "image_bchw = np.transpose(np.expand_dims(image, 0), (0, 3, 1, 2))\n",
    "\n",
    "session = onnxruntime.InferenceSession(export_result.output,\n",
    "                                       providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\n",
    "inputs = [o.name for o in session.get_inputs()]\n",
    "outputs = [o.name for o in session.get_outputs()]\n",
    "result = session.run(outputs, {inputs[0]: image_bchw})\n",
    "\n",
    "result[0].shape, result[1].shape, result[2].shape, result[3].shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FIAKv7tJG7bY",
    "outputId": "0b1e4b54-65b3-4e9f-d258-744d5d8c687b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the next section we unpack the result of prediction and show how to use it."
   ],
   "metadata": {
    "collapsed": false,
    "id": "osAr7VlHG7bY",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Output format for detection models\n",
    "\n",
    "If `preprocessing=True` (default value) then all models will be exported with NMS. If `preprocessing=False` models will be exported without NMS and raw model outputs will be returned. In this case, you will need to apply NMS yourself. This is useful if you want to use a custom NMS implementation that is not ONNX-compatible. In most cases you will want to use default `preprocessing=True`. It is also possible to pass a custom `nn.Module` as a `postprocessing` argument to the `export()` method. This module will be attached to the exported ONNX graph instead of the default NMS module. We encourage users to read the documentation of the `export()` method to learn more about the advanced options.\n",
    "\n",
    "When exporting an object detection model with postprocessing enabled, the prediction format can be one of two:\n",
    "\n",
    "* A \"flat\" format - `DetectionOutputFormatMode.FLAT_FORMAT`\n",
    "* A \"batched\" format - `DetectionOutputFormatMode.BATCH_FORMAT`\n",
    "\n",
    "You can select the desired output format by setting `export(..., output_predictions_format=DetectionOutputFormatMode.BATCH_FORMAT)`.\n",
    "\n",
    "### Flat format\n",
    "\n",
    "A detection results returned as a single tensor of shape `[N, 8]`, where `N` is the number of detected objects in the entire batch. Each row in the tensor represents a single detection result and has the following format:\n",
    "\n",
    "`[batch_index, cx, cy, w, h, r, class score, class index]`\n",
    "\n",
    "When exporting a model with batch size of 1 (default mode) you can ignore the first column as all boxes will belong to the single sample. In case you export model with batch size > 1 you have to iterate over this array like so:\n",
    "\n",
    "```python\n",
    "for sample_index, pred_boxes_cxcywhr, pred_scores, pred_labels, in iterate_over_obb_detection_predictions_in_flat_format(flat_predictions, export_result.batch_size):\n",
    "    # do something with the detection predictions\n",
    "```\n",
    "\n",
    "### Batch format\n",
    "\n",
    "A second supported format is so-called \"batch\". It matches with output format of TensorRT's NMS implementation. The return value in this case is tuple of 4 tensors:\n",
    "\n",
    "* `num_predictions` - [B, 1] - A number of predictions per sample\n",
    "* `pred_boxes` - [B, N, 5] - A coordinates of the predicted boxes in `[СX, СН, W, H, R]` format\n",
    "* `pred_scores` - [B, N] - A scores of the predicted boxes\n",
    "* `pred_classes` - [B, N] - A class indices of the predicted boxes\n",
    "\n",
    "Here `B` corresponds to batch size and `N` is the maximum number of detected objects per image.\n",
    "In order to get the actual number of detections per image you need to iterate over `num_predictions` tensor and get the first element of each row."
   ],
   "metadata": {
    "collapsed": false,
    "id": "SIJd_GA6G7bY",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now when you're familiar with the output formats, let's see how to use them.\n",
    "To start, it's useful to take a look at the values of the predictions with a naked eye:\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "CortPI_PG7bY",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "num_predictions, pred_boxes, pred_scores, pred_classes = result\n",
    "num_predictions"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FPFV7dkCG7bY",
    "outputId": "7ed6df67-268b-4e97-a9ef-a1fedde1c077",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "np.set_printoptions(threshold=50, edgeitems=3)\n",
    "pred_boxes, pred_boxes.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WkyHxBIpG7bY",
    "outputId": "b90d205f-caf7-4660-a147-dd5e2c210db8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "np.set_printoptions(threshold=50, edgeitems=5)\n",
    "pred_scores, pred_scores.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_o932ejFG7bY",
    "outputId": "3ed9f6fe-30b2-41a2-b4c1-95f622cb167b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "np.set_printoptions(threshold=50, edgeitems=10)\n",
    "pred_classes, pred_classes.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fCcrryOMG7bY",
    "outputId": "4f172de2-2844-44e9-abf3-d5b9e1e0de20",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualizing predictions\n",
    "\n",
    "For sake of this tutorial we will use a simple visualization function that is tailored for batch_size=1 only.\n",
    "You can use it as a starting point for your own visualization code."
   ],
   "metadata": {
    "collapsed": false,
    "id": "G6i-d4koG7bY",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from super_gradients.inference import iterate_over_obb_detection_predictions_in_batched_format\n",
    "from super_gradients.training.utils.visualization.utils import generate_color_mapping\n",
    "from super_gradients.training.utils.visualization.obb import OBBVisualization\n",
    "from super_gradients.training.datasets.datasets_conf import DOTA2_DEFAULT_CLASSES_LIST\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DOTA2_CLASS_COLORS = generate_color_mapping(len(DOTA2_DEFAULT_CLASSES_LIST))\n",
    "\n",
    "\n",
    "def show_predictions_from_batch_format(image, predictions):\n",
    "    image = image.copy()\n",
    "\n",
    "    _, pred_boxes, pred_scores, pred_classes = next(iter(iterate_over_obb_detection_predictions_in_batched_format(predictions)))\n",
    "\n",
    "    image = OBBVisualization.draw_obb(\n",
    "        image=image,\n",
    "        rboxes_cxcywhr=pred_boxes,\n",
    "        scores=pred_scores,\n",
    "        labels=pred_classes,\n",
    "        class_names=DOTA2_DEFAULT_CLASSES_LIST,\n",
    "        class_colors=DOTA2_CLASS_COLORS,\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "id": "isa324XWG7bY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "show_predictions_from_batch_format(image, result)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 801
    },
    "id": "6dmXY63DG7bZ",
    "outputId": "9f666453-a488-4f39-d7cc-9d09e520eb9f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Changing the output format\n",
    "\n",
    "You can explicitly specify output format of the predictions by setting the `output_predictions_format` argument of `export()` method. Let's see how it works:\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "u8FelWdNG7bZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from super_gradients.conversion import DetectionOutputFormatMode\n",
    "\n",
    "export_result = model.export(\"yolo_nas_s_r_flat_format.onnx\",\n",
    "                             output_predictions_format=DetectionOutputFormatMode.FLAT_FORMAT)\n",
    "export_result"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cjPRJd-KG7bZ",
    "outputId": "65871a3b-77e0-4a0c-fcf2-607a6f197e88",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we exported a model that produces predictions in `flat` format. Let's run the model like before and see the result:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "3JkQAHrLG7bZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "session = onnxruntime.InferenceSession(export_result.output,\n",
    "                                       providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\n",
    "inputs = [o.name for o in session.get_inputs()]\n",
    "outputs = [o.name for o in session.get_outputs()]\n",
    "result = session.run(outputs, {inputs[0]: image_bchw})\n",
    "result[0].shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tMWsM-kQG7bZ",
    "outputId": "6b9362bb-01ba-45fc-dd10-d2f9770aede9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from super_gradients.inference import iterate_over_obb_detection_predictions_in_flat_format\n",
    "\n",
    "\n",
    "def show_predictions_from_flat_format(image, predictions):\n",
    "    image = image.copy()\n",
    "    _, pred_boxes, pred_scores, pred_classes = next(\n",
    "        iter(iterate_over_obb_detection_predictions_in_flat_format(predictions, batch_size=1)))\n",
    "\n",
    "    image = OBBVisualization.draw_obb(\n",
    "        image=image,\n",
    "        rboxes_cxcywhr=pred_boxes,\n",
    "        scores=pred_scores,\n",
    "        labels=pred_classes,\n",
    "        class_names=DOTA2_DEFAULT_CLASSES_LIST,\n",
    "        class_colors=DOTA2_CLASS_COLORS,\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "id": "HYOrJGwXG7bZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "show_predictions_from_flat_format(image, result)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 801
    },
    "id": "tLPAIW8GG7bZ",
    "outputId": "2408b048-29bd-4cc0-c363-710a3e3691eb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Changing postprocessing settings\n",
    "\n",
    "You can control a number of parameters in the NMS settings as well as maximum number of detections per image before and after NMS step:\n",
    "\n",
    "* IOU threshold for NMS - `nms_iou_threshold`\n",
    "* Score threshold for NMS - `nms_score_threshold`\n",
    "* Maximum number of detections per image before NMS - `max_detections_before_nms`\n",
    "* Maximum number of detections per image after NMS - `max_detections_after_nms`\n",
    "\n",
    "For sake of demonstration, let's export a model that would produce at most one detection per image with confidence threshold above 0.8 and NMS IOU threshold of 0.5. Let's use at most 100 predictions per image before NMS step:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "l_9nllP9G7bZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "export_result = model.export(\n",
    "    \"yolo_nas_s_top_1.onnx\",\n",
    "    confidence_threshold=0.8,\n",
    "    nms_threshold=0.15,\n",
    "    num_pre_nms_predictions=100,\n",
    "    max_predictions_per_image=1,\n",
    "    output_predictions_format=DetectionOutputFormatMode.FLAT_FORMAT\n",
    ")\n",
    "\n",
    "session = onnxruntime.InferenceSession(export_result.output,\n",
    "                                       providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\n",
    "inputs = [o.name for o in session.get_inputs()]\n",
    "outputs = [o.name for o in session.get_outputs()]\n",
    "result = session.run(outputs, {inputs[0]: image_bchw})\n",
    "\n",
    "show_predictions_from_flat_format(image, result)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 801
    },
    "id": "R2M9pdGIG7bZ",
    "outputId": "2baad879-0678-47b1-aacc-3c0a40b248e8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Export of quantized model\n",
    "\n",
    "You can export a model with quantization to FP16 or INT8. To do so, you need to specify the `quantization_mode` argument of `export()` method.\n",
    "\n",
    "Important notes:\n",
    "* Quantization to FP16 requires CUDA / MPS device available and would not work on CPU-only machines.\n",
    "\n",
    "Let's see how it works:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "6tU82QMZG7bZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from super_gradients.conversion.conversion_enums import ExportQuantizationMode\n",
    "\n",
    "export_result = model.export(\n",
    "    \"yolo_nas_r_s_int8.onnx\",\n",
    "    output_predictions_format=DetectionOutputFormatMode.FLAT_FORMAT,\n",
    "    quantization_mode=ExportQuantizationMode.INT8\n",
    ")\n",
    "\n",
    "session = onnxruntime.InferenceSession(export_result.output,\n",
    "                                       providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\n",
    "inputs = [o.name for o in session.get_inputs()]\n",
    "outputs = [o.name for o in session.get_outputs()]\n",
    "result = session.run(outputs, {inputs[0]: image_bchw})\n",
    "\n",
    "show_predictions_from_flat_format(image, result)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 995
    },
    "id": "sa65jEIjG7bZ",
    "outputId": "f7b51a56-5a71-497e-88eb-554eba65eaa2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Advanced INT-8 quantization options\n",
    "\n",
    "When quantizing a model using `quantization_mode==ExportQuantizationMode.INT8` you can pass a DataLoader to export() function to collect correct statistics of activations to prodice a more accurate quantized model.\n",
    "We expect the DataLoader to return either a tuple of tensors or a single tensor. In case a tuple of tensors is returned by data-loader the first element will be used as input image.\n",
    "You can use existing data-loaders from SG here as is.\n",
    "\n",
    "**Important notes**\n",
    "* A `calibration_loader` should use same image normalization parameters that were used during training.\n",
    "\n",
    "In the example below we use a dummy data-loader for sake of showing how to use this feature. You should use your own data-loader here."
   ],
   "metadata": {
    "collapsed": false,
    "id": "NYr7b65NG7ba",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from super_gradients.conversion import ExportQuantizationMode\n",
    "\n",
    "# THIS IS ONLY AN EXAMPLE. YOU SHOULD USE YOUR OWN DATA-LOADER HERE\n",
    "dummy_calibration_dataset = [torch.randn((3, 640, 640), dtype=torch.float32) for _ in range(32)]\n",
    "dummy_calibration_loader = DataLoader(dummy_calibration_dataset, batch_size=8, num_workers=0)\n",
    "# THIS IS ONLY AN EXAMPLE. YOU SHOULD USE YOUR OWN DATA-LOADER HERE\n",
    "\n",
    "export_result = model.export(\n",
    "    \"yolo_nas_r_s_int8_with_calibration.onnx\",\n",
    "    output_predictions_format=DetectionOutputFormatMode.FLAT_FORMAT,\n",
    "    quantization_mode=ExportQuantizationMode.INT8,\n",
    "    calibration_loader=dummy_calibration_loader\n",
    ")\n",
    "\n",
    "session = onnxruntime.InferenceSession(export_result.output,\n",
    "                                       providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\n",
    "inputs = [o.name for o in session.get_inputs()]\n",
    "outputs = [o.name for o in session.get_outputs()]\n",
    "result = session.run(outputs, {inputs[0]: image_bchw})\n",
    "\n",
    "show_predictions_from_flat_format(image, result)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "W-wjSejMG7ba",
    "outputId": "21dd28f1-80c7-4c1a-9bd8-59b3fa254931",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Limitations\n",
    "\n",
    "* Dynamic batch size / input image shape is not supported yet. You can only export a model with a fixed batch size and input image shape.\n",
    "* TensorRT of version 8.6 or higher is required.\n",
    "* Quantization to FP16 requires CUDA / MPS device available."
   ],
   "metadata": {
    "collapsed": false,
    "id": "KZK3oB3EG7ba",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Legacy low-level export API\n",
    "\n",
    "The .export() API is a new high-level API that is recommended for most use-cases.\n",
    "However old low-level API is still available for advanced users:\n",
    "\n",
    "* https://docs.deci.ai/super-gradients/docstring/training/models.html#training.models.conversion.convert_to_onnx\n",
    "* https://docs.deci.ai/super-gradients/docstring/training/models.html#training.models.conversion.convert_to_coreml\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "5sFMdApzG7ba",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "n5Su7rpSG7ba",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
