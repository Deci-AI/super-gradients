# TODO: REPLICATE THE BELOW RESULTS BELOW AND ADD TENSORBOARDS, LOGS, TRAINING TIME ETC.
# Yolo v5 Detection training on CoCo2017 Dataset:
# Yolo v5s train on 320x320 mAP@0.5-0.95 (confidence 0.001, test on 320x320 images) ~28.4
# Yolo v5s train in 640x640 mAP@0.5-0.95 (confidence 0.001, test on 320x320 images) ~29.1

# batch size may need to change depending on model size and GPU (2080Ti, V100)
# The code is optimized for running with a Mini-Batch of 64 examples... So depending on the amount of GPUs,
# you should change the "batch_accumulate" param in the training_params dict to be batch_size * gpu_num * batch_accumulate = 64.

# Instructions:
# Set the PYTHONPATH environment variable: (Replace "YOUR_LOCAL_PATH" with the path to the downloaded repo):
#   export PYTHONPATH="YOUR_LOCAL_PATH"/super_gradients/
# Then for 320x320 image size for training:
#   python -m torch.distributed.launch --nproc_per_node=4 train_from_recipe_example/train_from_recipe.py --config-name=coco2017_yolov5
# And for 640x640 image size for training:
#   python -m torch.distributed.launch --nproc_per_node=4 train_from_recipe_example/train_from_recipe.py --config-name=coco2017_yolov5 dataset_params.train_image_size=640

defaults:
  - training_hyperparams: coco2017_yolov5_train_params
  - dataset_params: coco_detection_dataset_params
  - arch_params: yolov5s_arch_params

dataset_interface:
  _target_: super_gradients.training.datasets.dataset_interfaces.dataset_interface.CoCoDetectionDatasetInterface
  dataset_params: ${dataset_params}

data_loader_num_workers: 8

model_checkpoints_location: local

load_checkpoint: False

architecture: yolo_v5s

experiment_name: ${architecture}_coco2017_320

multi_gpu:
  _target_: super_gradients.training.sg_model.MultiGPUMode
  value: 'DDP'

deci_model:
  _target_: super_gradients.SgModel
  experiment_name: ${experiment_name}
  multi_gpu: ${multi_gpu}

training_hyperparams:
  criterion_params:
    anchors:
      _target_: super_gradients.training.utils.detection_utils.Anchors
      anchors_list: [[10, 13, 16, 30, 33, 23],[30, 61, 62, 45, 59, 119],[116, 90, 156, 198, 373, 326]]
      strides: [8, 16, 32]  # output strides of all yolo outputs
    obj_loss_gain: 1.0    # will be scaled according to train image size
    box_loss_gain: 0.05   # will be scaled according to num output levels
    cls_loss_gain: 0.5    # will be scaled according to num classes and num output levels
