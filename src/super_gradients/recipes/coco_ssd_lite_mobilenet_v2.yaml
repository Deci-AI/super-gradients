# SSD MobileNetV2 Detection training on CoCo2017 Dataset:
# Trained in 320x320 mAP@0.5@0.95 (COCO API, confidence 0.001, IoU threshold 0.6, test on 320x320 images) ~21.5
# Checkpoint path: https://deci-pretrained-models.s3.amazonaws.com/ssd_lite_mobilenet_v2/ckpt_best.pth
# (trained with stride_16_plus_big)
# Hardware: 4 NVIDIA RTX A5000
# Training time: Â±16 hours

# Instructions:
# Set the PYTHONPATH environment variable: (Replace "YOUR_LOCAL_PATH" with the path to the downloaded repo):
#   export PYTHONPATH="YOUR_LOCAL_PATH"/super_gradients/

# Run with:
# python3 -m torch.distributed.launch --nproc_per_node=4 train_from_recipe_example/train_from_recipe.py \
# --config-name=coco_ssd_lite_mobilenet_v2.yaml


# NOTE:
# Anchors will be selected based on validation resolution and anchors_name
# To switch between anchors, set anchors_name to something else defined in the anchors section
# e.g.
# python3 -m torch.distributed.launch --nproc_per_node=4 train_from_recipe_example/train_from_recipe.py \
# --config-name=coco_ssd_lite_mobilenet_v2.yaml anchors_name=stride_16_plus


defaults:
  - training_hyperparams: default_train_params
  - dataset_params: coco_detection_dataset_params
  - arch_params: default_arch_params
  - checkpoint_params: default_checkpoint_params

architecture: ssd_lite_mobilenet_v2
project_name: SSD_Mobile

dataset_params:
  batch_size: 64
  val_batch_size: 64
  val_image_size: 320
  train_image_size: 320

# stride_N_plus is for models where the first skip begins from the feature map with output stride N and higher
# grids of [input_size / N x input_size / N] and smaller
# NOTE: changing anchors to a different stride requires updating output_paths in model anch params
# because feat_size are hardcoded and won't automatically change in a model
anchors:
  256x256:
    stride_16_plus:
      _target_: super_gradients.training.utils.ssd_utils.DefaultBoxes
      fig_size: 256
      feat_size: [ 32, 16, 8, 4, 2, 1 ]
      scales: [ 18, 38, 84, 131, 177, 223, 269 ]
      aspect_ratios: [ [ 2 ], [ 2, 3 ], [ 2, 3 ], [ 2, 3 ], [ 2 ], [ 2 ] ]
      scale_xy: 0.1
      scale_wh: 0.2
    stride_8_plus: [[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]]
  300x300:
    stride_8_plus:
      _target_: super_gradients.training.utils.ssd_utils.DefaultBoxes
      fig_size: 300
      feat_size: [38, 19, 10, 5, 3, 2]
      scales: [21, 45, 99, 153, 207, 261, 315]
      aspect_ratios: [[2], [2, 3], [2, 3], [2, 3], [2], [2]]
      scale_xy: 0.1
      scale_wh: 0.2
    stride_16_plus:
      _target_: super_gradients.training.utils.ssd_utils.DefaultBoxes
      fig_size: 300
      feat_size: [19, 10, 5, 3, 2, 1]
      scales: [21, 45, 99, 153, 207, 261, 315]
      # scales: [60, 105, 150, 195, 240, 285, 330]
      aspect_ratios: [[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]]
      scale_xy: 0.1
      scale_wh: 0.2
  320x320:
    stride_8_plus:
      _target_: super_gradients.training.utils.ssd_utils.DefaultBoxes
      fig_size: 320
      feat_size: [ 40, 20, 10, 5, 3, 2 ]
      scales: [ 22, 48, 106, 163, 221, 278, 336 ]
      aspect_ratios: [ [ 2 ], [ 2, 3 ], [ 2, 3 ], [ 2, 3 ], [ 2 ], [ 2 ] ]
      scale_xy: 0.1
      scale_wh: 0.2
    stride_16_plus:
      _target_: super_gradients.training.utils.ssd_utils.DefaultBoxes
      fig_size: 320
      feat_size: [ 20, 10, 5, 3, 2, 1 ]
      scales: [ 22, 48,  106, 163, 221, 278, 336 ]
      aspect_ratios: [ [ 2, 3 ], [ 2, 3 ], [ 2, 3 ], [ 2, 3 ], [ 2, 3 ], [ 2, 3 ] ]
      scale_xy: 0.1
      scale_wh: 0.2
    stride_16_plus_big:
      _target_: super_gradients.training.utils.ssd_utils.DefaultBoxes
      fig_size: 320
      feat_size: [ 20, 10, 5, 3, 2, 1 ]
      scales: [ 32, 82, 133, 184, 235, 285, 336 ]
      aspect_ratios: [ [ 2, 3 ], [ 2, 3 ], [ 2, 3 ], [ 2, 3 ], [ 2, 3 ], [ 2, 3 ] ]
      scale_xy: 0.1
      scale_wh: 0.2

data_loader_num_workers: 8
model_checkpoints_location: local
experiment_suffix: res${dataset_params.val_image_size}
experiment_name: ${architecture}_coco_${experiment_suffix}

sg_model:
  _target_: super_gradients.SgModel
  experiment_name: ${experiment_name}
  model_checkpoints_location: ${model_checkpoints_location}
  multi_gpu: AUTO

anchors_resolution: ${dataset_params.val_image_size}x${dataset_params.val_image_size}
anchors_name: stride_16_plus_big
dboxes: ${anchors.${anchors_resolution}.${anchors_name}}

arch_params:
  num_classes: 80
  anchors: ${dboxes}

dataset_interface:
  coco2017_detection:
    dataset_params: ${dataset_params}

training_hyperparams:
  ema: True
  anchors_name: ${anchors_name}
  max_epochs: 400
  lr_mode: cosine
  cosine_final_lr_ratio: 0.01
  batch_accumulate: 1
  initial_lr: 0.02
  loss: ssd_loss
  loss_logging_items_names: [ smooth_l1, closs, Loss ]
  criterion_params:
    alpha: 1.0
    dboxes: ${dboxes}

  optimizer: SGD
  optimizer_params:
    momentum: 0.9
    weight_decay: 0.0005
    nesterov: True
  lr_warmup_epochs: 3
  warmup_momentum: 0.8
  warmup_initial_lr: 1e-06
  warmup_bias_lr: 0.1

  valid_metrics_list:
    - _target_: super_gradients.training.metrics.DetectionMetrics
      post_prediction_callback:
        _target_: super_gradients.training.utils.ssd_utils.SSDPostPredictCallback
        conf: 0.001
        iou: 0.6
      num_cls: ${arch_params.num_classes}

  metric_to_watch: 'mAP@0.50:0.95'
  greater_metric_to_watch_is_better: True
