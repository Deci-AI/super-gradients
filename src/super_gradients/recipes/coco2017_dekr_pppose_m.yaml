#  DEKR training example with COCO dataset.
#  Reproduction and refinement of paper: Bottom-Up Human Pose Estimation Via Disentangled Keypoint Regression.
#
#  Recipe runs with batch size = 8 X 8 gpus = 64.
#
#  Instructions:
#   0. Make sure that the data is stored in dataset_params.dataset_dir or add "dataset_params.data_dir=<PATH-TO-DATASET>" at the end of the command below (feel free to check ReadMe)
#   1. Move to the project root (where you will find the ReadMe and src folder)
#   2. Run the command:
#       PP-Pose-M: python src/super_gradients/examples/train_from_recipe_example/train_from_recipe.py --config-name=coco2017_dekr_pppose_m
#  NOTE: Add "checkpoint_params.checkpoint_path=<hrnet-pretrained-path>" to use pretrained backbone (See line 55).
#
#
#  Validation AP (Without multiscale & flip augmentation) - COCO, training time:
#      DEKR-W32:    input-size: [640, 640]     AP: TBD    8 X RTX A5000
#
#
#  Official git repo:
#      https://github.com/HRNet/DEKR
#  Paper:
#      https://arxiv.org/abs/2104.02300
#
#
#  Comments:
#      * Pretrained backbones were used.
#      * Results with Deci code are higher than original implementation, mostly thanks to changes in Detail loss and
#          module, different auxiliary feature maps and different loss weights.

defaults:
  - training_hyperparams: coco2017_dekr_pose_train_params
  - dataset_params: coco_pose_estimation_dekr_dataset_params
  - arch_params: dekr_pppose_m
  - checkpoint_params: default_checkpoint_params
  - _self_

resume: False

architecture: dekr_pp_pose_m

multi_gpu: DDP
num_gpus: 8

experiment_suffix: ""
experiment_name: coco2017_{architecture}${experiment_suffix}

ckpt_root_dir:

train_dataloader: coco2017_pose_train
val_dataloader: coco2017_pose_val

arch_params:
  num_classes: ${dataset_params.num_joints}


dataset_params:
  train_dataloader_params:
    batch_size: 16

  val_dataloader_params:
    batch_size: 16

training_hyperparams:
  resume: ${resume}
#  phase_callbacks: []
#   Note: You can uncomment following block to enable visualization of intermediate results during training.
#   When enabled, these callbacks will save first batch from training & validation to Tensorboard.
#   This is helpful for debugging and doing visual checks whether predictions are reasonable and transforms are
#   working as expected.
#   The only downside is that it tend to bloat Tensorboard logs (Up to ten Gigs for long training regimes).
  phase_callbacks:
    - DEKRVisualizationCallback:
        phase:
          _target_: super_gradients.training.utils.callbacks.callbacks.Phase
          value: TRAIN_BATCH_END
        prefix: "train_"
        mean: [ 0.485, 0.456, 0.406 ]
        std: [ 0.229, 0.224, 0.225 ]
        apply_sigmoid: False

    - DEKRVisualizationCallback:
        phase:
          _target_: super_gradients.training.utils.callbacks.callbacks.Phase
          value: VALIDATION_BATCH_END
        prefix: "val_"
        mean: [ 0.485, 0.456, 0.406 ]
        std: [ 0.229, 0.224, 0.225 ]
        apply_sigmoid: False

# THE FOLLOWING PARAMS ARE DIRECTLY USED BY HYDRA
hydra:
  run:
    # Set the output directory (i.e. where .hydra folder that logs all the input params will be generated)
    dir: ${hydra_output_dir:${ckpt_root_dir}, ${experiment_name}}
